{
  "paragraphs": [
    {
      "text": "%spark.conf\n\nPYSPARK_PYTHON \"python2\"\nspark.pyspark.python \"python2\"\n\nspark.driver.cores       4\nspark.driver.memory      4g\nspark.executor.cores     4\nspark.executor.memory    4g\nspark.executor.instances 4\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-15T16:26:47+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/text"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1597375100057_18651878",
      "id": "paragraph_1597375100057_18651878",
      "dateCreated": "2020-08-14T03:18:20+0000",
      "dateStarted": "2020-08-15T16:26:47+0000",
      "dateFinished": "2020-08-15T16:26:47+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:31151",
      "results": {
        "code": "SUCCESS",
        "msg": []
      }
    },
    {
      "text": "%spark.pyspark\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.endpoint\", \"cumulus.openstack.hpc.cam.ac.uk:6780/\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.path.style.access\", \"true\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.list.version\", \"2\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.bucket.probe\", \"0\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.aws.credentials.provider\",\n    \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\"\n    )",
      "user": "anonymous",
      "dateUpdated": "2020-08-15T16:26:49+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1597375189970_963315047",
      "id": "paragraph_1597375189970_963315047",
      "dateCreated": "2020-08-14T03:19:49+0000",
      "dateStarted": "2020-08-15T16:26:49+0000",
      "dateFinished": "2020-08-15T16:27:13+0000",
      "status": "FINISHED",
      "$$hashKey": "object:31152",
      "results": {
        "code": "SUCCESS",
        "msg": []
      }
    },
    {
      "text": "%spark.pyspark\n\ndf = sqlContext.read.parquet(\n    \"s3a://albert/\"\n    )\n\nprint \"DF count: \", df.count()\nprint \"DF partitions: \", df.rdd.getNumPartitions()",
      "user": "anonymous",
      "dateUpdated": "2020-08-15T16:27:17+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "//4040-spark-tefbxv.local.zeppelin-project.org:8080/jobs/job?id=0",
              "$$hashKey": "object:33203"
            },
            {
              "jobUrl": "//4040-spark-tefbxv.local.zeppelin-project.org:8080/jobs/job?id=1",
              "$$hashKey": "object:33204"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1597418456972_485773191",
      "id": "paragraph_1597418456972_485773191",
      "dateCreated": "2020-08-14T15:20:56+0000",
      "dateStarted": "2020-08-15T16:27:17+0000",
      "dateFinished": "2020-08-15T16:27:30+0000",
      "status": "FINISHED",
      "$$hashKey": "object:31153",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DF count:  621626\nDF partitions:  16\n"
          }
        ]
      }
    },
    {
      "text": "%spark.pyspark\n\ndf = sqlContext.read.parquet(\n    \"s3a://gaia-dr2-parquet-0-32/\"\n    )\n\nprint \"DF count: \", df.count()\nprint \"DF partitions: \", df.rdd.getNumPartitions()",
      "user": "anonymous",
      "dateUpdated": "2020-08-15T16:27:32+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "//4040-spark-tefbxv.local.zeppelin-project.org:8080/jobs/job?id=2",
              "$$hashKey": "object:33281"
            },
            {
              "jobUrl": "//4040-spark-tefbxv.local.zeppelin-project.org:8080/jobs/job?id=3",
              "$$hashKey": "object:33282"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1597375159233_2006400480",
      "id": "paragraph_1597375159233_2006400480",
      "dateCreated": "2020-08-14T03:19:19+0000",
      "dateStarted": "2020-08-15T16:27:32+0000",
      "dateFinished": "2020-08-15T16:27:34+0000",
      "status": "FINISHED",
      "$$hashKey": "object:31154",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DF count:  52659345\nDF partitions:  186\n"
          }
        ]
      }
    },
    {
      "text": "%spark.pyspark\n\ndf = sqlContext.read.parquet(\n    \"s3a://gaia-dr2-parquet-0-16/\"\n    )\n\nprint \"DF count: \", df.count()\nprint \"DF partitions: \", df.rdd.getNumPartitions()",
      "user": "anonymous",
      "dateUpdated": "2020-08-15T16:27:44+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "//4040-spark-tefbxv.local.zeppelin-project.org:8080/jobs/job?id=4",
              "$$hashKey": "object:33356"
            },
            {
              "jobUrl": "//4040-spark-tefbxv.local.zeppelin-project.org:8080/jobs/job?id=5",
              "$$hashKey": "object:33357"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1597375708909_1448755122",
      "id": "paragraph_1597375708909_1448755122",
      "dateCreated": "2020-08-14T03:28:28+0000",
      "dateStarted": "2020-08-15T16:27:44+0000",
      "dateFinished": "2020-08-15T16:27:46+0000",
      "status": "FINISHED",
      "$$hashKey": "object:31155",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DF count:  105699833\nDF partitions:  374\n"
          }
        ]
      }
    },
    {
      "text": "%spark.pyspark\n\ndf = sqlContext.read.parquet(\n    \"s3a://gaia-dr2-parquet-0-8/\"\n    )\n\nprint \"DF count: \", df.count()\nprint \"DF partitions: \", df.rdd.getNumPartitions()",
      "user": "anonymous",
      "dateUpdated": "2020-08-15T16:28:37+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "//4040-spark-tefbxv.local.zeppelin-project.org:8080/jobs/job?id=6",
              "$$hashKey": "object:33444"
            },
            {
              "jobUrl": "//4040-spark-tefbxv.local.zeppelin-project.org:8080/jobs/job?id=7",
              "$$hashKey": "object:33445"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1597413659420_1753327719",
      "id": "paragraph_1597413659420_1753327719",
      "dateCreated": "2020-08-14T14:00:59+0000",
      "dateStarted": "2020-08-15T16:28:37+0000",
      "dateFinished": "2020-08-15T16:28:41+0000",
      "status": "FINISHED",
      "$$hashKey": "object:31156",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DF count:  211537973\nDF partitions:  749\n"
          }
        ]
      }
    },
    {
      "text": "%spark.pyspark\n\ndf = sqlContext.read.parquet(\n    \"s3a://gaia-dr2-parquet-0-4/\"\n    )\n\nprint \"DF count: \", df.count()\nprint \"DF partitions: \", df.rdd.getNumPartitions()",
      "user": "anonymous",
      "dateUpdated": "2020-08-15T16:29:06+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1597413672091_148192074",
      "id": "paragraph_1597413672091_148192074",
      "dateCreated": "2020-08-14T14:01:12+0000",
      "dateStarted": "2020-08-15T16:29:06+0000",
      "dateFinished": "2020-08-15T18:44:27+0000",
      "status": "ERROR",
      "$$hashKey": "object:31157",
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Exception in thread \"dag-scheduler-event-loop\" java.lang.OutOfMemoryError: Java heap space\nException in thread \"Thread-3\" java.lang.OutOfMemoryError: Java heap space\nException in thread \"pool-3-thread-3\" java.lang.OutOfMemoryError: Java heap space\nException in thread \"dispatcher-event-loop-1\" java.lang.OutOfMemoryError: Java heap space\nException in thread \"spark-listener-group-executorManagement\" java.lang.OutOfMemoryError: Java heap space\nException in thread \"spark-listener-group-appStatus\" java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3181)\n\tat java.util.ArrayList.grow(ArrayList.java:265)\n\tat java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:239)\n\tat java.util.ArrayList.ensureCapacityInternal(ArrayList.java:231)\n\tat java.util.ArrayList.add(ArrayList.java:462)\n\tat java.util.concurrent.ConcurrentSkipListMap.toList(ConcurrentSkipListMap.java:2391)\n\tat java.util.concurrent.ConcurrentSkipListMap$KeySet.toArray(ConcurrentSkipListMap.java:2440)\n\tat java.util.ArrayList.<init>(ArrayList.java:178)\n\tat com.codahale.metrics.ExponentiallyDecayingReservoir.rescale(ExponentiallyDecayingReservoir.java:173)\n\tat com.codahale.metrics.ExponentiallyDecayingReservoir.rescaleIfNeeded(ExponentiallyDecayingReservoir.java:122)\n\tat com.codahale.metrics.ExponentiallyDecayingReservoir.update(ExponentiallyDecayingReservoir.java:94)\n\tat com.codahale.metrics.ExponentiallyDecayingReservoir.update(ExponentiallyDecayingReservoir.java:84)\n\tat com.codahale.metrics.Histogram.update(Histogram.java:41)\n\tat com.codahale.metrics.Timer.update(Timer.java:195)\n\tat com.codahale.metrics.Timer.update(Timer.java:90)\n\tat com.codahale.metrics.Timer$Context.stop(Timer.java:37)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:129)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:99)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$Lambda$1365/270712385.apply$mcJ$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2$$Lambda$1364/337459226.apply$mcV$sp(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1319)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n"
          },
          {
            "type": "TEXT",
            "data": "Py4JJavaError: An error occurred while calling o121.parquet.\n: java.lang.OutOfMemoryError: Java heap space\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling o121.parquet.\\n', JavaObject id=o124), <traceback object at 0x7fb760021fc8>)"
          }
        ]
      }
    },
    {
      "text": "%spark.pyspark\ndf = sqlContext.read.parquet(\n    \"s3a://gaia-dr2-parquet-0-2x/\"\n    )\n\nprint \"DF count: \", df.count()\nprint \"DF partitions: \", df.rdd.getNumPartitions()",
      "user": "anonymous",
      "dateUpdated": "2020-08-15T15:32:07+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1597413715354_2110442873",
      "id": "paragraph_1597413715354_2110442873",
      "dateCreated": "2020-08-14T14:01:55+0000",
      "dateStarted": "2020-08-15T15:16:47+0000",
      "dateFinished": "2020-08-15T15:24:15+0000",
      "status": "ERROR",
      "$$hashKey": "object:31158"
    }
  ],
  "name": "Untitled Note 1",
  "id": "2FHNVCRNQ",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0-SNAPSHOT",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {
    "isRunning": false
  },
  "path": "/Untitled Note 1"
}