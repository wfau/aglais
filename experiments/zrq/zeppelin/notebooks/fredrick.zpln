{
  "paragraphs": [
    {
      "text": "%md\n## Welcome to my world\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-11T01:45:58+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596813406501_52047383",
      "id": "paragraph_1596689529045_996230152",
      "dateCreated": "2020-08-07T15:16:46+0000",
      "dateStarted": "2020-08-11T01:45:58+0000",
      "dateFinished": "2020-08-11T01:45:58+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:2046",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Welcome to my world</h2>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%python\nprint (1 + 1)\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-11T01:45:58+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596813406501_476645663",
      "id": "paragraph_1596689843287_146045104",
      "dateCreated": "2020-08-07T15:16:46+0000",
      "dateStarted": "2020-08-11T01:45:58+0000",
      "dateFinished": "2020-08-11T01:45:59+0000",
      "status": "FINISHED",
      "$$hashKey": "object:2047",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "2\n"
          }
        ]
      }
    },
    {
      "text": "%spark.conf\n\nPYSPARK_PYTHON \"python2\"\nspark.pyspark.python \"python2\"\n\nspark.executor.instances 10\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-11T01:45:59+0000",
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596813406501_460964884",
      "id": "paragraph_1596710199844_1808913724",
      "dateCreated": "2020-08-07T15:16:46+0000",
      "dateStarted": "2020-08-11T01:45:59+0000",
      "dateFinished": "2020-08-11T01:45:59+0000",
      "status": "FINISHED",
      "$$hashKey": "object:2048",
      "results": {
        "code": "SUCCESS",
        "msg": []
      }
    },
    {
      "text": "%spark\n\nval NUM_SAMPLES = 1000000\n\nval count = sc.parallelize(1 to NUM_SAMPLES).filter{\n    _ =>\n        val x = math.random\n        val y = math.random\n        x*x + y*y < 1\n    }.count()\n\nprintln(s\"Pi is roughly ${4.0 * count / NUM_SAMPLES}\")",
      "user": "anonymous",
      "dateUpdated": "2020-08-11T01:45:59+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "//4040-spark-xrutte.local.zeppelin-project.org:8080/jobs/job?id=0",
              "$$hashKey": "object:2860"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596819754444_1709209169",
      "id": "paragraph_1596819754444_1709209169",
      "dateCreated": "2020-08-07T17:02:34+0000",
      "dateStarted": "2020-08-11T01:45:59+0000",
      "dateFinished": "2020-08-11T01:46:46+0000",
      "status": "FINISHED",
      "$$hashKey": "object:2049",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Pi is roughly 3.140788\n\u001b[1m\u001b[34mNUM_SAMPLES\u001b[0m: \u001b[1m\u001b[32mInt\u001b[0m = 1000000\n\u001b[1m\u001b[34mcount\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 785197\n"
          }
        ]
      }
    },
    {
      "text": "%spark.pyspark\n\nimport random\nNUM_SAMPLES = 10000000\n\ndef inside(p):\n    x, y = random.random(), random.random()\n    return x*x + y*y < 1\n\ncount = sc.parallelize(\n    range(\n        0,\n        NUM_SAMPLES\n        )\n    ).filter(\n        inside\n        ).count()\n\nguess = 4.0 * count / NUM_SAMPLES\nprint(\"Pi is roughly {}\".format(guess))\n\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-11T01:46:46+0000",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "//4040-spark-xrutte.local.zeppelin-project.org:8080/jobs/job?id=1",
              "$$hashKey": "object:2905"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596813406502_894937877",
      "id": "paragraph_1596689855731_210489457",
      "dateCreated": "2020-08-07T15:16:46+0000",
      "dateStarted": "2020-08-11T01:46:46+0000",
      "dateFinished": "2020-08-11T01:46:50+0000",
      "status": "FINISHED",
      "$$hashKey": "object:2050",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Pi is roughly 3.1414936\n"
          }
        ]
      }
    },
    {
      "text": "%spark.pyspark\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.endpoint\", \"cumulus.openstack.hpc.cam.ac.uk:6780/\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.path.style.access\", \"true\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.list.version\", \"2\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.bucket.probe\", \"0\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.aws.credentials.provider\",\n    \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\"\n    )\n\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-11T01:46:50+0000",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 267.15,
              "optionOpen": false
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596813406502_403380169",
      "id": "paragraph_1596692369388_1831019531",
      "dateCreated": "2020-08-07T15:16:46+0000",
      "dateStarted": "2020-08-11T01:46:50+0000",
      "dateFinished": "2020-08-11T01:46:50+0000",
      "status": "FINISHED",
      "$$hashKey": "object:2051",
      "results": {
        "code": "SUCCESS",
        "msg": []
      }
    },
    {
      "text": "%spark.pyspark\n\ndf = sqlContext.read.parquet(\n    \"s3a://gaia-dr2-parquet/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet\"\n    )\n\nprint \"DF count: \", df.count()\nprint \"DF partitions: \", df.rdd.getNumPartitions()\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-11T01:46:50+0000",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "//4040-spark-xrutte.local.zeppelin-project.org:8080/jobs/job?id=2",
              "$$hashKey": "object:2970"
            },
            {
              "jobUrl": "//4040-spark-xrutte.local.zeppelin-project.org:8080/jobs/job?id=3",
              "$$hashKey": "object:2971"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596813406502_956346881",
      "id": "paragraph_1596735544843_2001843771",
      "dateCreated": "2020-08-07T15:16:46+0000",
      "dateStarted": "2020-08-11T01:46:50+0000",
      "dateFinished": "2020-08-11T01:47:03+0000",
      "status": "FINISHED",
      "$$hashKey": "object:2052",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DF count:  262492\nDF partitions:  10\n"
          }
        ]
      }
    },
    {
      "text": "%spark.pyspark\n\ndf = sqlContext.read.parquet(\n    \"s3a://gaia-dr2-parquet/\"\n    )\n\nprint \"DF count: \", df.count()\nprint \"DF partitions: \", df.rdd.getNumPartitions()\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-11T01:47:03+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596820827213_2133669053",
      "id": "paragraph_1596820827213_2133669053",
      "dateCreated": "2020-08-07T17:20:27+0000",
      "dateStarted": "2020-08-11T01:47:03+0000",
      "dateFinished": "2020-08-11T01:50:31+0000",
      "status": "ERROR",
      "$$hashKey": "object:2053",
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Py4JJavaError: An error occurred while calling o109.parquet.\n: java.io.EOFException: listObjects() on s3a://gaia-dr2-parquet/: com.amazonaws.SdkClientException: Failed to sanitize XML document destined for handler class com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListObjectsV2Handler\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:181)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)\n\tat org.apache.hadoop.fs.s3a.Listing$ObjectListingIterator.next(Listing.java:604)\n\tat org.apache.hadoop.fs.s3a.Listing$FileStatusListingIterator.requestNextBatch(Listing.java:421)\n\tat org.apache.hadoop.fs.s3a.Listing$FileStatusListingIterator.sourceHasNext(Listing.java:373)\n\tat org.apache.hadoop.fs.s3a.Listing$FileStatusListingIterator.hasNext(Listing.java:368)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerListStatus(S3AFileSystem.java:1924)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listStatus$9(S3AFileSystem.java:1882)\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.listStatus(S3AFileSystem.java:1882)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.listLeafFiles(InMemoryFileIndex.scala:322)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.$anonfun$bulkListLeafFiles$1(InMemoryFileIndex.scala:195)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:187)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:135)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:98)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:70)\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:561)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:399)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:737)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.SdkClientException: Failed to sanitize XML document destined for handler class com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListObjectsV2Handler\n\tat com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.sanitizeXmlDocument(XmlResponsesSaxParser.java:224)\n\tat com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.parseListObjectsV2Response(XmlResponsesSaxParser.java:339)\n\tat com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsV2Unmarshaller.unmarshall(Unmarshallers.java:128)\n\tat com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsV2Unmarshaller.unmarshall(Unmarshallers.java:117)\n\tat com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:62)\n\tat com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:31)\n\tat com.amazonaws.http.response.AwsResponseHandlerAdapter.handle(AwsResponseHandlerAdapter.java:69)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleResponse(AmazonHttpClient.java:1726)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleSuccessResponse(AmazonHttpClient.java:1446)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1368)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5062)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5008)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5002)\n\tat com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:941)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$continueListObjects$6(S3AFileSystem.java:1317)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.continueListObjects(S3AFileSystem.java:1306)\n\tat org.apache.hadoop.fs.s3a.Listing$ObjectListingIterator.next(Listing.java:600)\n\t... 38 more\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling o109.parquet.\\n', JavaObject id=o112), <traceback object at 0x7fc5e8de2290>)"
          }
        ]
      }
    },
    {
      "user": "anonymous",
      "dateUpdated": "2020-08-10T16:50:01+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596818468456_367867782",
      "id": "paragraph_1596818468456_367867782",
      "dateCreated": "2020-08-07T16:41:08+0000",
      "status": "FINISHED",
      "$$hashKey": "object:2054"
    }
  ],
  "name": "fredrick",
  "id": "2FFXJRSDR",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0-SNAPSHOT",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {
    "isRunning": false
  },
  "path": "/fredrick"
}