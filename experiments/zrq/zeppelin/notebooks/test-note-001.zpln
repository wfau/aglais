{
  "paragraphs": [
    {
      "text": "%md\n## Welcome to my world\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-06T04:55:40+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Welcome to my world</h2>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596689529045_996230152",
      "id": "paragraph_1596689529045_996230152",
      "dateCreated": "2020-08-06T04:52:09+0000",
      "dateStarted": "2020-08-06T04:55:40+0000",
      "dateFinished": "2020-08-06T04:55:56+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:8647"
    },
    {
      "text": "%python\nprint (1 + 1)\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-07T03:10:18+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "2\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596689843287_146045104",
      "id": "paragraph_1596689843287_146045104",
      "dateCreated": "2020-08-06T04:57:23+0000",
      "dateStarted": "2020-08-07T03:10:18+0000",
      "dateFinished": "2020-08-07T03:10:18+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8648"
    },
    {
      "text": "%spark.conf\n\nPYSPARK_PYTHON \"python2\"\nspark.pyspark.python \"python2\"\n\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-07T03:10:21+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionSupport": false
        },
        "editorMode": "ace/mode/text"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596710199844_1808913724",
      "id": "paragraph_1596710199844_1808913724",
      "dateCreated": "2020-08-06T10:36:39+0000",
      "dateStarted": "2020-08-07T03:10:21+0000",
      "dateFinished": "2020-08-07T03:10:21+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8649"
    },
    {
      "text": "%spark.conf\n\nspark.executor.instances 10\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-07T03:10:24+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionSupport": false
        },
        "editorMode": "ace/mode/text"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596710565268_1361764921",
      "id": "paragraph_1596710565268_1361764921",
      "dateCreated": "2020-08-06T10:42:45+0000",
      "dateStarted": "2020-08-07T03:10:24+0000",
      "dateFinished": "2020-08-07T03:10:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8650"
    },
    {
      "text": "%spark.pyspark\n\nimport random\nNUM_SAMPLES = 10000000\n\ndef inside(p):\n    x, y = random.random(), random.random()\n    return x*x + y*y < 1\n\ncount = sc.parallelize(\n    range(\n        0,\n        NUM_SAMPLES\n        )\n    ).filter(\n        inside\n        ).count()\n\nguess = 4.0 * count / NUM_SAMPLES\nprint(\"Pi is roughly {}\".format(guess))\n\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-07T03:10:41+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Pi is roughly 3.1422036\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "//4040-spark-jyarev.local.zeppelin-project.org:8080/jobs/job?id=3",
              "$$hashKey": "object:12897"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596689855731_210489457",
      "id": "paragraph_1596689855731_210489457",
      "dateCreated": "2020-08-06T04:57:35+0000",
      "dateStarted": "2020-08-07T03:10:41+0000",
      "dateFinished": "2020-08-07T03:10:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8651"
    },
    {
      "text": "%spark.pyspark\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.endpoint\", \"cumulus.openstack.hpc.cam.ac.uk:6780/swift/v1/AUTH_21b4ae3a2ea44bc5a9c14005ed2963af\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.path.style.access\", \"true\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.list.version\", \"2\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.access.key\", \"none\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.secret.key\", \"none\"\n    )\ndf = sqlContext.read.parquet(\n    \"s3a://gaia-dr2-parquet/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet\"\n    )\n\nprint \"DF count: \", df.count()\nprint \"DF partitions: \", df.rdd.getNumPartitions()\n\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-07T03:11:13+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 267.15,
              "optionOpen": false
            }
          }
        },
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "msg": [
          {
            "data": "",
            "type": "TEXT"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "//4040-spark-ymkdpa.local.zeppelin-project.org:8080/jobs/job?id=0",
              "$$hashKey": "object:12985"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596692369388_1831019531",
      "id": "paragraph_1596692369388_1831019531",
      "dateCreated": "2020-08-06T05:39:29+0000",
      "dateStarted": "2020-08-07T03:11:13+0000",
      "dateFinished": "2020-08-07T03:10:57+0000",
      "status": "RUNNING",
      "$$hashKey": "object:8653"
    },
    {
      "text": "%spark.pyspark\n\ndf = sqlContext.read.parquet(\n    \"s3a://gaia-dr2-parquet/\"\n    )\n\nprint \"DF count: \", df.count()\nprint \"DF partitions: \", df.rdd.getNumPartitions()\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-06T17:39:16+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596735544843_2001843771",
      "id": "paragraph_1596735544843_2001843771",
      "dateCreated": "2020-08-06T17:39:04+0000",
      "status": "ERROR",
      "focus": true,
      "$$hashKey": "object:12292",
      "dateFinished": "2020-08-06T17:39:28+0000",
      "dateStarted": "2020-08-06T17:39:16+0000",
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Py4JJavaError: An error occurred while calling o250.parquet.\n: org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a://gaia-dr2-parquet/: com.amazonaws.SdkClientException: Failed to parse XML document with handler class com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListObjectsV2Handler: Failed to parse XML document with handler class com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListObjectsV2Handler\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:189)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2265)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)\n\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1700)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.isDirectory(S3AFileSystem.java:2995)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:737)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.SdkClientException: Failed to parse XML document with handler class com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListObjectsV2Handler\n\tat com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.parseXmlInputStream(XmlResponsesSaxParser.java:162)\n\tat com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.parseListObjectsV2Response(XmlResponsesSaxParser.java:335)\n\tat com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsV2Unmarshaller.unmarshall(Unmarshallers.java:88)\n\tat com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsV2Unmarshaller.unmarshall(Unmarshallers.java:77)\n\tat com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:62)\n\tat com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:31)\n\tat com.amazonaws.http.response.AwsResponseHandlerAdapter.handle(AwsResponseHandlerAdapter.java:70)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleResponse(AmazonHttpClient.java:1555)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1272)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1058)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:743)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:717)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4368)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4315)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4309)\n\tat com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:883)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$5(S3AFileSystem.java:1276)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:1269)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2237)\n\t... 22 more\nCaused by: org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.\n\tat com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:203)\n\tat com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.fatalError(ErrorHandlerWrapper.java:177)\n\tat com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:400)\n\tat com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:327)\n\tat com.sun.org.apache.xerces.internal.impl.XMLScanner.reportFatalError(XMLScanner.java:1472)\n\tat com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$PrologDriver.next(XMLDocumentScannerImpl.java:994)\n\tat com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:602)\n\tat com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:112)\n\tat com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:505)\n\tat com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:842)\n\tat com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:771)\n\tat com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)\n\tat com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1213)\n\tat com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.parseXmlInputStream(XmlResponsesSaxParser.java:148)\n\t... 46 more\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling o250.parquet.\\n', JavaObject id=o253), <traceback object at 0x7f5a9b61b6c8>)"
          }
        ]
      }
    },
    {
      "text": "%spark.pyspark\n\ndf = sqlContext.read.parquet(\n    \"s3a://albert/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet\"\n    )\n\nprint \"DF count: \", df.count()\nprint \"DF partitions: \", df.rdd.getNumPartitions()\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-06T17:45:37+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "//4040-spark-rtkyxn.local.zeppelin-project.org:8080/jobs/job?id=8",
              "$$hashKey": "object:12503"
            },
            {
              "jobUrl": "//4040-spark-rtkyxn.local.zeppelin-project.org:8080/jobs/job?id=9",
              "$$hashKey": "object:12504"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596731985850_28815355",
      "id": "paragraph_1596731985850_28815355",
      "dateCreated": "2020-08-06T16:39:45+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:10953",
      "dateFinished": "2020-08-06T17:45:38+0000",
      "dateStarted": "2020-08-06T17:45:37+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DF count:  262492\nDF partitions:  2\n"
          }
        ]
      }
    },
    {
      "text": "%spark.pyspark\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-06T17:22:58+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596734578926_2088265811",
      "id": "paragraph_1596734578926_2088265811",
      "dateCreated": "2020-08-06T17:22:58+0000",
      "status": "READY",
      "focus": true,
      "$$hashKey": "object:11871"
    }
  ],
  "name": "Untitled Note 1",
  "id": "2FGCSDPW2",
  "defaultInterpreterGroup": "md",
  "version": "0.9.0-SNAPSHOT",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/Untitled Note 1"
}