	#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2019, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#




################################################################
##  Setup Zepellin/Yarn/Spark/Hadoop Cluster                  ##
################################################################


# Read up and try to follow
# https://docs.hpc.cam.ac.uk/cloud-training/#16


#-------------------------------------------------------------------------
# GUI Steps to Setup Networks, Security Groups & Instances
#-------------------------------------------------------------------------


## Step 1:
Create Private Network 


## Step 2:
Create Router, connected to Internet


## Create Security Groups (based on training presentation)

external-bastion: 22 / ICMP
internal-bastion: default
internal-webserver: 22 - internal-bastion
external-webserver: 80
aglais-zeppelin : 8080
aglais-master: *
aglais-worker: *


## Step 3:

Create Bastion Instance (Gateway)
 (external-bastion + internal-bastion)


## Step 4:

Create Master node
 (external-bastion + internal-webserver + aglais-master)

## Step 5:

Create 3 Worker Nodes 
 (external-bastion + internal-webserver + aglais-worker)


## Step 6:

Create Zeppelin Node 
 (external-bastion + internal-webserver + aglais-zeppelin)





stv-aglais-bastion-ip=
stv-aglais-worker-1-ip=
stv-aglais-worker-2-ip=
stv-aglais-worker-3-ip=
stv-aglais-master-ip=
stv-aglais-zeppelin-ip=




#-------------------------------------------------------------------------
# Setup .ssh config locally
#-------------------------------------------------------------------------

~/.ssh/config
..


Host stv-aglais-bastion
    User centos
    IdentityFile ~/.ssh/stv-master
    Hostname${stv-aglais-bastion-ip:?}

Host stv-aglais-master
    User fedora
    IdentityFile ~/.ssh/stv-master
    Hostname${stv-aglais-master-ip:?}
    ProxyCommand ssh -W %h:%p stv-aglais-bastion

Host stv-aglais-worker-1
    User fedora
    IdentityFile ~/.ssh/stv-master
    Hostname ${stv-aglais-worker-1-ip:?}
    ProxyCommand ssh -W %h:%p stv-aglais-bastion

Host stv-aglais-worker-2
    User fedora
    IdentityFile ~/.ssh/stv-master
    Hostname ${stv-aglais-worker-2-ip:?}
    ProxyCommand ssh -W %h:%p stv-aglais-bastion


Host stv-aglais-worker-3
    User fedora
    IdentityFile ~/.ssh/stv-master
    Hostname ${stv-aglais-worker-3-ip:?}
    ProxyCommand ssh -W %h:%p stv-aglais-bastion

Host stv-aglais-zeppelin
    User fedora
    IdentityFile ~/.ssh/stv-master
    Hostname ${stv-aglais-zeppelin-ip:?}
    ProxyCommand ssh -W %h:%p stv-aglais-bastion

..



#-------------------------------------------------------------------------
## Step 5: Install HDFS Cluster
#-------------------------------------------------------------------------

## Fetch Hadoop Binaries on each node & install java


## Run on all nodes

sudo yum install wget
wget http://apache.cs.utah.edu/hadoop/common/current/hadoop-3.1.3.tar.gz
tar -xzf hadoop-3.1.3.tar.gz
mv hadoop-3.1.3 hadoop
sudo yum install java-1.8.0-openjdk






#-------------------------------------------------------------------------
## Setup /etc/hosts file and ~/.ssh/config file
#-------------------------------------------------------------------------

## Copy stv-master from Desktop to each node


sudo su
cat <<EOF >> "/etc/hosts"

127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

	
${stv-aglais-bastion-ip:?}       stv-aglais-master.novalocal
${stv-aglais-bastion-ip:?}       stv-aglais-master
${stv-aglais-worker-1-ip:?}       stv-aglais-worker-1
${stv-aglais-worker-2-ip:?}       stv-aglais-worker-2
${stv-aglais-worker-3-ip:?}       stv-aglais-worker-3
${stv-aglais-zeppelin-ip:?}       stv-aglais-zeppelin

EOF
  
exit


cat > "${HOME:?}/.ssh/config" << EOF

    Host stv-aglais-worker-1
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

    Host stv-aglais-worker-2 
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

    Host stv-aglais-worker-3
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

    Host stv-aglais-master.novalocal
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

    Host stv-aglais-master
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

    Host stv-aglais-zeppelin
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

EOF


sudo chmod 600 ~/.ssh/config
sudo chmod 600 ~/.ssh/stv-master



#-------------------------------------------------------------------------
# Setup Hadoop 
#-------------------------------------------------------------------------

## Setup Hadoop Configurations


cat > "${HOME:?}/hadoop/etc/hadoop/core-site.xml" << EOF

<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>fs.default.name</name>
            <value>hdfs://stv-aglais-master:9000</value>
        </property>
    </configuration>

EOF


cat > "${HOME:?}/hadoop/etc/hadoop/yarn-site.xml" << EOF

<configuration>
    <property>
            <name>yarn.acl.enable</name>
            <value>0</value>
    </property>

    <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>stv-aglais-master</value>
    </property>

    <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
    </property>
<property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>120000</value>
</property>

<property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>120000</value>
</property>

<property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>2000</value>
</property>

<property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
</property>
<property>
   <name>yarn.scheduler.capacity.root.support.user-limit-factor</name>  
   <value>2</value>
</property>
<property>
   <name>yarn.nodemanager.disk-health-checker.min-healthy-disks</name>
   <value>0.0</value>
</property>
<property>
   <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
   <value>100.0</value>
</property>

 <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>stv-aglais-master:8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.address</name>
    <value>stv-aglais-master:8032</value>
  </property>
  <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>stv-aglais-master:8088</value>
  </property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>stv-aglais-master:8031</value>
  </property>
  <property>
    <name>yarn.resourcemanager.admin.address</name>
    <value>stv-aglais-master:8033</value>
  </property>

</configuration>

EOF




cat > "${HOME:?}/hadoop/etc/hadoop/workers" << EOF

stv-aglais-worker-1
stv-aglais-worker-2
stv-aglais-worker-3

EOF




cat > "${HOME:?}/hadoop/etc/hadoop/mapred-site.xml" << EOF

<configuration>
    <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
    </property>
    <property>
            <name>yarn.app.mapreduce.am.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
    <property>
            <name>mapreduce.map.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
    <property>
            <name>mapreduce.reduce.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
<property>
	<name>yarn.app.mapreduce.am.resource.mb</name>
        <value>80000</value>
</property>

<property>
	<name>mapreduce.map.memory.mb</name>
        <value>40000</value>
</property>

<property>
	<name>mapreduce.reduce.memory.mb</name>
        <value>40000</value>
</property>
</configuration>

EOF




cat > "${HOME:?}/hadoop/etc/hadoop/core-site.xml" << EOF

<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>fs.default.name</name>
            <value>hdfs://stv-aglais-master:9000</value>
        </property>
    </configuration>

EOF



#-------------------------------------------------------------------------
## Format HDFS Master Node on each node
#-------------------------------------------------------------------------

sudo mkdir /home/hadoop/
sudo chown -R fedora:root /home/hadoop/
source /home/fedora/hadoop/bin/hdfs namenode -format





#-------------------------------------------------------------------------
## Start and Stop HDFS services
#-------------------------------------------------------------------------

## On Master Node:
start-all.sh


## Create a directory
hdfs dfs -mkdir /hadoop/


## LS directory
hdfs dfs -ls /hadoop/


yarn jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount "/hadoop/books/*" output
## Job Completed Successfully
## Check Results

hdfs dfs -ls output
hdfs dfs -cat output/part-r-00000 | less




#-------------------------------------------------------------------------
# Spark
#-------------------------------------------------------------------------


## Setup Spark on Existing yarn & hdfs cluster


## Download and Install Spark Binaries
## From Node Master 

cd /home/fedora
wget https://www-eu.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz
tar -xvf spark-2.4.4-bin-hadoop2.7.tgz
mv spark-2.4.4-bin-hadoop2.7.tgz spark


## Setup Default Spark Config

mv /home/fedora/spark/conf/spark-defaults.conf.template /home/fedora/spark/conf/spark-defaults.conf

cat <<EOF >> "/home/fedora/spark/conf/spark-defaults.conf"
spark.master                     yarn
spark.driver.memory              80g
spark.yarn.am.memory            80g
spark.executor.memory          80g
spark.eventLog.enabled  true
spark.eventLog.dir hdfs://${stv-aglais-bastion-ip:?}:9000/spark-log
EOF


## Create the log directory in HDFS:
hdfs dfs -mkdir /spark-log


#-------------------------------------------------------------------------
# Setup Environment variables on each node
#-------------------------------------------------------------------------

# For each Worker node & Master node

cat > "${HOME:?}/.profile" << EOF
PATH=/home/fedora/spark/bin:/home/fedora/.local/bin:/home/fedora/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/bin:/sbin:/home/fedora/hadoop/bin:/home/fedora/hadoop/sbin
EOF

cat <<EOF >> "${HOME:?}/.bashrc"
export HADOOP_HOME=/home/fedora/hadoop
export PATH=/home/fedora/.local/bin:/home/fedora/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/bin:/sbin:/home/fedora/hadoop/bin:/home/fedora/hadoop/sbin:/home/fedora/spark/bin
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre/
export HADOOP_CONF_DIR=/home/fedora/hadoop/etc/hadoop
export SPARK_HOME=/home/fedora/spark
export LD_LIBRARY_PATH=/home/fedora/hadoop/lib/native
EOF




#-------------------------------------------------------------------------
# Zeppelin Installation and Setup
#-------------------------------------------------------------------------

ssh stv-aglais-zeppelin

## Fetch Zeppelin Binaries
wget https://www-eu.apache.org/dist/zeppelin/zeppelin-0.8.2/zeppelin-0.8.2-bin-all.tgz
tar -xzvf zeppelin-0.8.2-bin-all.tgz 
mv zeppelin-0.8.2-bin-all zeppelin
rm zeppelin-0.8.2-bin-all.tgz 



## Install java & other libraries
sudo yum install wget
sudo yum install nano
sudo yum install java-1.8.0-openjdk



## Setup Zeppelin Configurations


## Set Users in shiro.ini
cp /home/zeppelin/conf/shiro.ini.template /home/zeppelin/conf/shiro.ini
## We can now setup users & admin in shiro.ini



## Set Zeppelin Configuration
cp /home/zeppelin/conf/zeppelin-site.xml.template /home/zeppelin/conf/zeppelin-site.xml
nano /home/zeppelin/conf/zeppelin-site.xml


## Start Server
/home/zeppelin/bin/zeppelin-daemon.sh start


## Tunnel connection to 8080 and go to
## On Local Machine (Browser)
## Go to: http://localhost:8080/#/



## Spark not working
## Trying many things:
## Opening ports on different nodes
## Setting interpreter master=yarn, master=yarn-client or master=yarn-cluster




## Following changes seem to have worked


## Modify Spark Interpreter, add deploy-mode
nano ~/zeppelin/interpreter/spark/interpreter-setting.json


..

       "spark.submit.deployMode":{
        "envName": "spark submit deploy mode",
        "propertyName": "spark.submit.deployMode",
        "value": "cluster",
        "description": "",
        "type": "string"
      },


..


## Change Spark Interpreter in UI, set master=yarn
## Set deploy-mode to cluster
## !! Not sure if this step is required or not



## Fetch Spark code as previous step & copy config files from Spark master node
## Fetch Hadoop & Yarn config files from previous step, add them to a directory in zeppelin/ & spark/



## Modify ~/zeppelin/conf/zeppelin-env.sh

export SPARK_HOME=/home/fedora/spark
export HADOOP_CONF_DIR=/home/zeppelin/hadoop-conf
export MASTER=yarn-cluster



#-------------------------------------------------------------------------
# Enable Python3 and matplotlib for python3 notebooks
#-------------------------------------------------------------------------
## Update python interpreter, set it to python3

## Install matplotlib
sudo python3 -m pip install -U pip
sudo python3 -m pip install -U matplotlib


