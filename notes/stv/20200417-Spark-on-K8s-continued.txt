#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2019, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#


# ---------------------------------------------
# Experimenting with Spark on Kubernetes
# ---------------------------------------------


## The following are notes run on an existing Spark cluster ontop of kubernetes, notes on how these are deployed can be found in the 20200408-Spark-on-K8s.txt file
## https://github.com/stvoutsin/aglais/blob/4b72625e7aee2e1b901d7e2e4c7a3e066f85e88c/notes/stv/20200408-Spark-on-K8s.txt


# [Master K8s Node]
# Get List of Pods
# ---------------------------------------------

kubectl get pods
NAME                            READY   STATUS    RESTARTS   AGE
spark-master-controller-l96t5   1/1     Running   0          2d19h
spark-worker-controller-4wmtp   1/1     Running   0          2d19h
spark-worker-controller-g8skg   1/1     Running   0          2d19h


# [Master K8s Node]
# Connnect to Pod, and try starting Pyspark
# ---------------------------------------------

kubectl exec -it spark-master-controller-l96t5 /bin/bash

root@spark-master-hostname:/# pyspark

Python 2.7.9 (default, Jun 29 2016, 13:08:31) 
[GCC 4.9.2] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Error: A JNI error has occurred, please check your installation and try again
Exception in thread "main" java.lang.NoClassDefFoundError: org/slf4j/Logger
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
	at java.lang.Class.privateGetMethodRecursive(Class.java:3048)
	at java.lang.Class.getMethod0(Class.java:3018)
	at java.lang.Class.getMethod(Class.java:1784)
	at sun.launcher.LauncherHelper.validateMainClass(LauncherHelper.java:544)
	at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:526)
Caused by: java.lang.ClassNotFoundException: org.slf4j.Logger
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 7 more
Traceback (most recent call last):
  File "/opt/spark/python/pyspark/shell.py", line 38, in <module>
    SparkContext._ensure_initialized()
  File "/opt/spark/python/pyspark/context.py", line 316, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File "/opt/spark/python/pyspark/java_gateway.py", line 46, in launch_gateway
    return _launch_gateway(conf)
  File "/opt/spark/python/pyspark/java_gateway.py", line 108, in _launch_gateway
    raise Exception("Java gateway process exited before sending its port number")
Exception: Java gateway process exited before sending its port number
>>> 




## Getting an Exception ???

## Googling points to the following solution
# https://stackoverflow.com/questions/32547832/error-to-start-pre-built-spark-master-when-slf4j-is-not-installed


export SPARK_DIST_CLASSPATH=$(hadoop classpath)


pyspark
Python 2.7.9 (default, Jun 29 2016, 13:08:31) 
[GCC 4.9.2] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.4.4
      /_/

Using Python version 2.7.9 (default, Jun 29 2016 13:08:31)
SparkSession available as 'spark'.



## Pyspark now works again..



# [Master K8s Node]
# Try running a PySpark job
# ---------------------------------------------

>>> words = 'the quick brown fox jumps over the\
...         lazy dog the quick brown fox jumps over the lazy dog'
>>> seq = words.split()
>>> data = sc.parallelize(seq)
>>> counts = data.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b).collect()
>>> dict(counts)                                                                
{'brown': 2, 'lazy': 2, 'jumps': 2, 'fox': 2, 'dog': 2, 'quick': 2, 'the': 4, 'over': 2}
>>> sc.stop()


## Job Completes Successfully

## To confirm that the job is being distributed we can either:
##   a) Check the logs

     kubectl -n=default logs -f spark-worker-controller-g8skg

     ## .. Shows Standard Spark output

        ..

	04/18 13:38:20 INFO shuffle.ExternalShuffleBlockResolver: Clean up non-shuffle files associated with the finished executor 1
	20/04/18 13:38:20 INFO shuffle.ExternalShuffleBlockResolver: Executor is not registered (appId=app-20200418133739-0008, execId=1)
	20/04/18 13:38:20 INFO shuffle.ExternalShuffleBlockResolver: Application app-20200418133739-0008 removed, cleanupLocalDirs = true
	20/04/18 13:38:20 INFO worker.Worker: Cleaning up local directories for application app-20200418133739-0008
	20/04/18 16:41:21 INFO worker.Worker: Asked to launch executor app-20200418164120-0009/1 for PySparkShell
	20/04/18 16:41:21 INFO spark.SecurityManager: Changing view acls to: root
	20/04/18 16:41:21 INFO spark.SecurityManager: Changing modify acls to: root
	20/04/18 16:41:21 INFO spark.SecurityManager: Changing view acls groups to: 
	20/04/18 16:41:21 INFO spark.SecurityManager: Changing modify acls groups to: 
	20/04/18 16:41:21 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
	20/04/18 16:41:21 INFO worker.ExecutorRunner: Launch command: "/usr/lib/jvm/java-8-openjdk-amd64/bin/java" "-cp" "/opt/spark/conf/:/opt/spark/jars/*:/opt/hadoop-2.8.2/etc/hadoop/:/opt/hadoop-2.8.2/share/hadoop/common/lib/*:/opt/hadoop-2.8.2/share/hadoop/common/*:/opt/hadoop-2.8.2/share/hadoop/hdfs/:/opt/hadoop-2.8.2/share/hadoop/hdfs/lib/*:/opt/hadoop-2.8.2/share/hadoop/hdfs/*:/opt/hadoop-2.8.2/share/hadoop/yarn/lib/*:/opt/hadoop-2.8.2/share/hadoop/yarn/*:/opt/hadoop-2.8.2/share/hadoop/mapreduce/lib/*:/opt/hadoop-2.8.2/share/hadoop/mapreduce/*:/opt/hadoop/contrib/capacity-scheduler/*.jar" "-Xmx1024M" "-Dspark.driver.port=34537" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@spark-master-hostname.spark-master-headless.default.svc.cluster.local:34537" "--executor-id" "1" "--hostname" "10.100.4.127" "--cores" "6" "--app-id" "app-20200418164120-0009" "--worker-url" "spark://Worker@10.100.4.127:40615"
	20/04/18 16:43:25 INFO worker.Worker: Asked to kill executor app-20200418164120-0009/1
	20/04/18 16:43:25 INFO worker.ExecutorRunner: Runner thread for executor app-20200418164120-0009/1 interrupted
	20/04/18 16:43:25 INFO worker.ExecutorRunner: Killing process!
	20/04/18 16:43:25 INFO worker.Worker: Executor app-20200418164120-0009/1 finished with state KILLED exitStatus 143
	20/04/18 16:43:25 INFO shuffle.ExternalShuffleBlockResolver: Clean up non-shuffle files associated with the finished executor 1
	20/04/18 16:43:25 INFO shuffle.ExternalShuffleBlockResolver: Executor is not registered (appId=app-20200418164120-0009, execId=1)
	20/04/18 16:43:25 INFO shuffle.ExternalShuffleBlockResolver: Application app-20200418164120-0009 removed, cleanupLocalDirs = true
	20/04/18 16:43:25 INFO worker.Worker: Cleaning up local directories for application app-20200418164120-0009


     kubectl -n=default logs -f spark-worker-controller-g8skg

        ..

	20/04/18 13:38:20 INFO worker.Worker: Asked to kill executor app-20200418133739-0008/1
	20/04/18 13:38:20 INFO worker.ExecutorRunner: Runner thread for executor app-20200418133739-0008/1 interrupted
	20/04/18 13:38:20 INFO worker.ExecutorRunner: Killing process!
	20/04/18 13:38:20 INFO worker.Worker: Executor app-20200418133739-0008/1 finished with state KILLED exitStatus 143
	20/04/18 13:38:20 INFO shuffle.ExternalShuffleBlockResolver: Clean up non-shuffle files associated with the finished executor 1
	20/04/18 13:38:20 INFO shuffle.ExternalShuffleBlockResolver: Executor is not registered (appId=app-20200418133739-0008, execId=1)
	20/04/18 13:38:20 INFO shuffle.ExternalShuffleBlockResolver: Application app-20200418133739-0008 removed, cleanupLocalDirs = true
	20/04/18 13:38:20 INFO worker.Worker: Cleaning up local directories for application app-20200418133739-0008
	20/04/18 16:41:21 INFO worker.Worker: Asked to launch executor app-20200418164120-0009/1 for PySparkShell
	20/04/18 16:41:21 INFO spark.SecurityManager: Changing view acls to: root
	20/04/18 16:41:21 INFO spark.SecurityManager: Changing modify acls to: root
	20/04/18 16:41:21 INFO spark.SecurityManager: Changing view acls groups to: 
	20/04/18 16:41:21 INFO spark.SecurityManager: Changing modify acls groups to: 
	20/04/18 16:41:21 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
	20/04/18 16:41:21 INFO worker.ExecutorRunner: Launch command: "/usr/lib/jvm/java-8-openjdk-amd64/bin/java" "-cp" "/opt/spark/conf/:/opt/spark/jars/*:/opt/hadoop-2.8.2/etc/hadoop/:/opt/hadoop-2.8.2/share/hadoop/common/lib/*:/opt/hadoop-2.8.2/share/hadoop/common/*:/opt/hadoop-2.8.2/share/hadoop/hdfs/:/opt/hadoop-2.8.2/share/hadoop/hdfs/lib/*:/opt/hadoop-2.8.2/share/hadoop/hdfs/*:/opt/hadoop-2.8.2/share/hadoop/yarn/lib/*:/opt/hadoop-2.8.2/share/hadoop/yarn/*:/opt/hadoop-2.8.2/share/hadoop/mapreduce/lib/*:/opt/hadoop-2.8.2/share/hadoop/mapreduce/*:/opt/hadoop/contrib/capacity-scheduler/*.jar" "-Xmx1024M" "-Dspark.driver.port=34537" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@spark-master-hostname.spark-master-headless.default.svc.cluster.local:34537" "--executor-id" "1" "--hostname" "10.100.4.127" "--cores" "6" "--app-id" "app-20200418164120-0009" "--worker-url" "spark://Worker@10.100.4.127:40615"
	20/04/18 16:43:25 INFO worker.Worker: Asked to kill executor app-20200418164120-0009/1
	20/04/18 16:43:25 INFO worker.ExecutorRunner: Runner thread for executor app-20200418164120-0009/1 interrupted
	20/04/18 16:43:25 INFO worker.ExecutorRunner: Killing process!
	20/04/18 16:43:25 INFO worker.Worker: Executor app-20200418164120-0009/1 finished with state KILLED exitStatus 143
	20/04/18 16:43:25 INFO shuffle.ExternalShuffleBlockResolver: Clean up non-shuffle files associated with the finished executor 1
	20/04/18 16:43:25 INFO shuffle.ExternalShuffleBlockResolver: Executor is not registered (appId=app-20200418164120-0009, execId=1)
	20/04/18 16:43:25 INFO shuffle.ExternalShuffleBlockResolver: Application app-20200418164120-0009 removed, cleanupLocalDirs = true
	20/04/18 16:43:25 INFO worker.Worker: Cleaning up local directories for application app-20200418164120-0009



     ## .. Shows Standard Spark output



## b) Check Spark UI:
## On the master node, get the IP of the spark-master service:

	kubectl get svc spark-master
	NAME           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
	spark-master   ClusterIP   10.254.177.162   <none>        7077/TCP,8080/TCP   3d23h
  
## From localhost,  Tunnel the connection to the IP address of the spark-master Service, 
## 
##  ssh -L '8080:10.254.177.162:8080' k8s-test-1-2zuxnqa5tj2u-master-0

## Check GUI from local browser : http://localhost:8080

## Shows all jobs submitted to the Spark cluster and a detailed breakdown of the spark nodes





# [Master K8s Node]
# Increasing the number of Worker nodes
# -------------------------------------------

kubectl scale rc spark-worker-controller --replicas=4


kubectl get all

NAME                                READY   STATUS    RESTARTS   AGE
pod/spark-master-controller-l96t5   1/1     Running   0          5d2h
pod/spark-worker-controller-29vld   1/1     Running   0          20h
pod/spark-worker-controller-4wmtp   1/1     Running   0          5d2h
pod/spark-worker-controller-g8skg   1/1     Running   0          5d2h
pod/spark-worker-controller-txfcj   1/1     Running   0          20h

NAME                                            DESIRED   CURRENT   READY   AGE
replicationcontroller/spark-master-controller   1         1         1       5d2h
replicationcontroller/spark-worker-controller   4         4         4       5d2h

NAME                            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
service/kubernetes              ClusterIP   10.254.0.1       <none>        443/TCP             6d23h
service/spark-master            ClusterIP   10.254.177.162   <none>        7077/TCP,8080/TCP   5d2h
service/spark-master-headless   ClusterIP   None             <none>        <none>              5d2h

