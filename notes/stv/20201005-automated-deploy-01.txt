#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#


# -----------------------------------------------------
# Create Clouds YAML file
#[user@desktop]

cat > "${HOME}/clouds.yaml" << EOF

clouds:


  gaia-test:
    auth:
      auth_url: https://cumulus.openstack.hpc.cam.ac.uk:5000/v3
      application_credential_id:     '$(secret 'stv-gaia-test.CREDENTIAL_ID')'
      application_credential_secret: '$(secret 'stv-gaia-test.CREDENTIAL_SECRET')'
    region_name: "RegionOne"
    interface: "public"
    identity_api_version: 3
    auth_type: "v3applicationcredential"

  gaia-test-super:
    auth:
      auth_url: https://cumulus.openstack.hpc.cam.ac.uk:5000/v3
      application_credential_id:     '$(secret 'stv-gaia-test.CREDENTIAL_ID')'
      application_credential_secret: '$(secret 'stv-gaia-test.CREDENTIAL_SECRET')'
    region_name: "RegionOne"
    interface: "public"
    identity_api_version: 3
    auth_type: "v3applicationcredential"

EOF



# -----------------------------------------------------
# Create our project config file.
#[user@desktop]

cat > "${HOME:?}/aglais.env" << 'EOF'

AGLAIS_REPO='git@github.com:stvoutsin/aglais.git'
AGLAIS_HOME="${PROJECTS_ROOT:?}/aglais"
AGLAIS_CODE="${AGLAIS_HOME:?}"
AGLAIS_CLOUD=gaia-test
AGLAIS_USER=stv

EOF



# -----------------------------------------------------
# Edit hosts.yml file 
#[user@desktop]

  source "${HOME}/aglais.settings"
  nano ${AGLAIS_CODE:?}/experiments/zrq/ansible/hosts.yml
	..	
	keypair: ''
	...


# -----------------------------------------------------
# Create a container to work with.
# https://podman.readthedocs.io/en/latest/markdown/podman-run.1.html
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler8 \
        --hostname ansibler \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock" \
        --env "clouduser=${AGLAIS_USER:?}" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml" \
        --env "ANSIBLE_CODE=/mnt/ansible" \
        --volume "${AGLAIS_CODE:?}/experiments/zrq/ansible:/mnt/ansible" \
        atolmis/ansible-client:latest \
        bash

	# Success




# -----------------------------------------------------
# Create our Ansible include vars file.
#[root@ansibler]

    cat > /tmp/ansible-vars.yml << EOF
buildtag:  'aglais-$(date '+%Y%m%d')'
cloudname: '${cloudname}'
clouduser: '${clouduser}'
EOF


# -----------------------------------------------------
# Run the scripts from the ansible directory.
#[root@ansibler]

    cd "${ANSIBLE_CODE:?}"



# -----------------------------------------------------
# Run the initial part of our deployment.
#[root@ansibler]
	
    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-01.yml"

PLAY RECAP **************************************************************************************************************************************************************************************************
gateway                    : ok=8    changed=6    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
localhost                  : ok=33   changed=24   unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   
master01                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
master02                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker01                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker02                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
zeppelin                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   




# -----------------------------------------------------
# Run the Hadoop part of our deployment.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-02.yml"

PLAY RECAP **************************************************************************************************************************************************************************************************
localhost                  : ok=29   changed=26   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
master01                   : ok=24   changed=18   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
master02                   : ok=19   changed=13   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker01                   : ok=22   changed=19   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker02                   : ok=22   changed=19   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
zeppelin                   : ok=24   changed=23   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   




# -----------------------------------------------------
# Format the HDFS NameNode on master01.
#[root@ansibler]

    ssh master01 \
        '
        hdfs namenode -format
        '

# -----------------------------------------------------
# Start the HDFS services.
#[root@ansibler]

    ssh master01 \
        '
        start-all.sh
	'	

# -----------------------------------------------------
# Start the HDFS services.
#[root@ansibler]

    ssh master01 \
        '
        hdfs dfsadmin -safemode leave
	'	

# -----------------------------------------------------
# Create our HDFS log directory.
#[root@ansibler]

    ssh master01 \
        '
        hdfs dfs -mkdir /spark-log
        '




# -----------------------------------------------------
# Check the HDFS status.
#[root@ansibler]

    ssh master01 \
        '
        hdfs dfsadmin -report
	'
Configured Capacity: 1099511627776 (1 TB)
Present Capacity: 1095180492800 (1019.97 GB)
DFS Remaining: 1095180484608 (1019.97 GB)
DFS Used: 8192 (8 KB)
DFS Used%: 0.00%
Replicated Blocks:
	Under replicated blocks: 0
	Blocks with corrupt replicas: 0
	Missing blocks: 0
	Missing blocks (with replication factor 1): 0
	Low redundancy blocks with highest priority to recover: 0
	Pending deletion blocks: 0
Erasure Coded Block Groups: 
	Low redundancy block groups: 0
	Block groups with corrupt internal blocks: 0
	Missing block groups: 0
	Low redundancy blocks with highest priority to recover: 0
	Pending deletion blocks: 0

-------------------------------------------------
Live datanodes (2):

Name: 10.10.2.226:9866 (worker01)
Hostname: worker01
Decommission Status : Normal
Configured Capacity: 549755813888 (512 GB)
DFS Used: 4096 (4 KB)
Non DFS Used: 17297408 (16.50 MB)
DFS Remaining: 547590242304 (509.98 GB)
DFS Used%: 0.00%
DFS Remaining%: 99.61%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Mon Oct 05 18:11:13 UTC 2020
Last Block Report: Mon Oct 05 18:09:55 UTC 2020
Num of Blocks: 0


Name: 10.10.2.38:9866 (worker02)
Hostname: worker02
Decommission Status : Normal
Configured Capacity: 549755813888 (512 GB)
DFS Used: 4096 (4 KB)
Non DFS Used: 17297408 (16.50 MB)
DFS Remaining: 547590242304 (509.98 GB)
DFS Used%: 0.00%
DFS Remaining%: 99.61%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Mon Oct 05 18:11:13 UTC 2020
Last Block Report: Mon Oct 05 18:09:55 UTC 2020
Num of Blocks: 0





# -----------------------------------------------------
# Install the Spark binaries.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "20-install-spark.yml"


PLAY RECAP **************************************************************************************************************************************************************************************************
master01                   : ok=3    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
zeppelin                   : ok=3    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0 





# -----------------------------------------------------
# Add the security rules for Spark.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "21-config-spark-security.yml"


PLAY RECAP **************************************************************************************************************************************************************************************************
localhost                  : ok=6    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   



# -----------------------------------------------------
# Create our Spark configuration.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "22-config-spark-master.yml"

	
PLAY RECAP **************************************************************************************************************************************************************************************************
gateway                    : ok=2    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
master01                   : ok=2    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   


# -----------------------------------------------------
# Run the SparkPi example from the Spark install instructtions.
# https://spark.apache.org/docs/3.0.0-preview2/running-on-yarn.html#launching-spark-on-yarn
#[root@ansibler]

    ssh master01 \
        '
        cd "${SPARK_HOME:?}"

        spark-submit \
            --class org.apache.spark.examples.SparkPi \
            --master yarn \
            --deploy-mode cluster \
            --driver-memory 1g \
            --executor-memory 1g \
            --executor-cores 1 \
            examples/jars/spark-examples*.jar \
                10
        '

2020-10-05 18:13:36,713 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: [Mon Oct 05 18:13:36 +0000 2020] Scheduler has assigned a container for AM, waiting for AM container to be launched
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1601921615458
	 final status: UNDEFINED
	 tracking URL: http://master01:8088/proxy/application_1601921402537_0001/
	 user: fedora
2020-10-05 18:13:37,716 INFO yarn.Client: Application report for application_1601921402537_0001 (state: ACCEPTED)
2020-10-05 18:13:38,718 INFO yarn.Client: Application report for application_1601921402537_0001 (state: ACCEPTED)
2020-10-05 18:13:39,721 INFO yarn.Client: Application report for application_1601921402537_0001 (state: ACCEPTED)
2020-10-05 18:13:40,724 INFO yarn.Client: Application report for application_1601921402537_0001 (state: ACCEPTED)
2020-10-05 18:13:41,725 INFO yarn.Client: Application report for application_1601921402537_0001 (state: ACCEPTED)
2020-10-05 18:13:42,728 INFO yarn.Client: Application report for application_1601921402537_0001 (state: RUNNING)
2020-10-05 18:13:42,728 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: worker02
	 ApplicationMaster RPC port: 45747
	 queue: default
	 start time: 1601921615458
	 final status: UNDEFINED
	 tracking URL: http://master01:8088/proxy/application_1601921402537_0001/
	 user: fedora
2020-10-05 18:13:43,730 INFO yarn.Client: Application report for application_1601921402537_0001 (state: RUNNING)
2020-10-05 18:13:44,732 INFO yarn.Client: Application report for application_1601921402537_0001 (state: RUNNING)
2020-10-05 18:13:45,734 INFO yarn.Client: Application report for application_1601921402537_0001 (state: RUNNING)
2020-10-05 18:13:46,736 INFO yarn.Client: Application report for application_1601921402537_0001 (state: RUNNING)
2020-10-05 18:13:47,738 INFO yarn.Client: Application report for application_1601921402537_0001 (state: RUNNING)
2020-10-05 18:13:48,740 INFO yarn.Client: Application report for application_1601921402537_0001 (state: FINISHED)
2020-10-05 18:13:48,740 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: worker02
	 ApplicationMaster RPC port: 45747
	 queue: default
	 start time: 1601921615458
	 final status: SUCCEEDED
	 tracking URL: http://master01:8088/proxy/application_1601921402537_0001/
	 user: fedora
2020-10-05 18:13:48,749 INFO util.ShutdownHookManager: Shutdown hook called
2020-10-05 18:13:48,750 INFO util.ShutdownHookManager: Deleting directory /opt/spark-2.4.7-bin-hadoop2.7/local/spark-132a4162-9ed7-47f3-bf0f-db3dae80c719
2020-10-05 18:13:48,759 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-18b634c6-21a6-4c48-8837-52a43c32b622



# -----------------------------------------------------
# Run the Zeppelin install.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-04.yml"
	
PLAY RECAP **************************************************************************************************************************************************************************************************
localhost                  : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
master01                   : ok=7    changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
master02                   : ok=7    changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker01                   : ok=7    changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker02                   : ok=7    changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
zeppelin                   : ok=12   changed=11   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   


# -----------------------------------------------------
# Start the YARN services.
#[root@ansibler]

    ssh zeppelin \
        '
        /home/fedora/zeppelin-0.8.2-bin-all/bin/zeppelin-daemon.sh start
        '

Log dir doesn't exist, create /home/fedora/zeppelin-0.8.2-bin-all/logs
Pid dir doesn't exist, create /home/fedora/zeppelin-0.8.2-bin-all/run
Zeppelin start                                             [  OK  ]



# -----------------------------------------------------
#  Get IP Address
#[root@ansibler]


    openstack \
        --os-cloud "${cloudname:?}" \
        floating ip list

+--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+
| ID                                   | Floating IP Address | Fixed IP Address | Port                                 | Floating Network                     | Project                          |
+--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+
| 077b110d-f61d-41fd-9073-c1f50821feab | 128.232.227.125     | 10.10.0.134      | f23a41b9-f0f6-414c-b349-3235f8fa3886 | a929e8db-1bf4-4a5f-a80c-fabd39d06a26 | bea28e83e6aa47a8962b59c3b24495fe |
| c93c8f7f-3295-4998-b0c8-08792cec3147 | 128.232.227.182     | 10.10.0.184      | 4aff3944-cee1-4019-83a4-ce21ce2dabc2 | a929e8db-1bf4-4a5f-a80c-fabd39d06a26 | bea28e83e6aa47a8962b59c3b24495fe |
+--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+




# -----------------------------------------------------
# Try some Spark jobs via the Zeppelin GUI.
firefox http://128.232.227.125:8080/#/ &


%spark.pyspark
import random 
NUM_SAMPLES = 2000000000

def inside(p):
    x, y = random.random(), random.random()
    return x*x + y*y < 1

count = sc.parallelize(range(0, NUM_SAMPLES)) \
             .filter(inside).count()
print ("Pi is roughly %f" % (4.0 * count / NUM_SAMPLES))


# Fails with error in Yarn:
	
# Zeppelin Exception:
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:954)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:952)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:952)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2164)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2077)
	at org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1948)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:121)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:989)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

# Yarn Exception
Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from zeppelin:42175 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
at scala.util.Try$.apply(Try.scala:192)
at scala.util.Failure.recover(Try.scala:216)
at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
at scala.concurrent.Promise$class.complete(Promise.scala:55)
at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:157)
at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:157)
at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:206)
at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:243)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from zeppelin:42175 in 120 seconds
... 8 more



## After some investigation this seems to be a network issue
## We need to open up some ports between Zeppelin & Master

## Fix that got this working was to enable all ingress traffic to Zeppelin from masters & workers

## After restarting nodes, we run the cell again:

%spark.pyspark
import random 
NUM_SAMPLES = 2000000000

def inside(p):
    x, y = random.random(), random.random()
    return x*x + y*y < 1

count = sc.parallelize(range(0, NUM_SAMPLES)) \
             .filter(inside).count()
print ("Pi is roughly %f" % (4.0 * count / NUM_SAMPLES))

> Pi is roughly 3.141593



# Fetch one Gaia parquet file and try reading from Zeppelin

sudo yum update
sudo yum install wget
wget https://cumulus.openstack.hpc.cam.ac.uk:6780/swift/v1/AUTH_21b4ae3a2ea44bc5a9c14005ed2963af/gaia-dr2-parquet-6-8/part-00125-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet
hdfs dfs -mkdir /gaia
hdfs dfs -put part-00125-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet  /gaia



# In Zeppelin..

%spark.pyspark
gs_df = sqlContext.read.parquet("/gaia/*.parquet")

# following https://spark.apache.org/docs/latest/sql-data-sources-parquet.html
gs_df.createOrReplaceTempView("gaia_source")
sql_df = spark.sql("SELECT floor(source_id / 562949953421312) AS hpx5, COUNT(*) AS n, AVG(pmra) AS avgPmRa, AVG(pmdec) AS avgPmDec FROM gaia_source GROUP BY hpx5")
sql_df.show()


# Success!
