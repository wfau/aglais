#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

# Some Notes on a brief investigation into AXS
# ---------------------------------------------

# The following was run on a cluster deployed using the following instructions:
# https://github.com/wfau/aglais/blob/master/notes/stv/20200915-automated-deploy-02.txt

# Run on Hadoop Master
# fedora@master
# --------------------

sudo yum install wget
wget https://github.com/astronomy-commons/axs/releases/download/v1.0/axs-distribution.tar.gz
tar -xzvf axs-distribution.tar.gz 
sudo mv /opt/spark-2.4.7-bin-hadoop2.7/ /opt/spark-2.4.7-bin-hadoop2.7-backup/
sudo mkdir /opt/spark-2.4.7-bin-hadoop2.7
cd /opt/spark-2.4.7-bin-hadoop2.7
sudo cp -R ~/axs-dist/* .

# Run Axis init
axs-init-config.sh

# Edit configurations manually:
nano /opt/spark-2.4.7-bin-hadoop2.7/conf/
..
	spark.local.dir        /opt/spark/local
	spark.sql.warehouse.dir        file:///opt/spark/warehouse
	spark.files.maxPartitionBytes   471859200
	spark.memory.offHeap.enabled    true
	spark.memory.offHeap.size	4g
	# spark.jars.packages   org.mariadb.jdbc:mariadb-java-client:2.2.3
	spark.jars	/opt/spark/python/axs/AxsUtilities-1.0-SNAPSHOT.jar
	spark.scheduler.minRegisteredResourcesRatio     0.75
	spark.master            yarn
	spark.driver.memory              5g
	spark.yarn.am.memory            5g
	spark.executor.memory          5g
	spark.eventLog.enabled  true
	spark.driver.maxResultSize	8192m
	spark.executor.cores            3
	spark.eventLog.enabled  true
	spark.eventLog.dir	hdfs://master01:9000/spark-log
	# END Ansible managed Spark configuration
	# BEGIN Ansible managed Spark environment
	# https://spark.apache.org/docs/3.0.0-preview2/configuration.html#inheriting-hadoop-cluster-configuration
	spark.yarn.appMasterEnv.YARN_CONF_DIR=/opt/hadoop/etc/hadoop
	spark.yarn.appMasterEnv.HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
..

# Remove spark-env and slaves file
rm /opt/spark-2.4.7-bin-hadoop2.7/slaves
rm /opt/spark-2.4.7-bin-hadoop2.7/spark-env.sh


stop-all.sh
start-all.sh


 spark-submit \
>             --class org.apache.spark.examples.SparkPi \
>             --master yarn \
>             --deploy-mode cluster \
>             --driver-memory 1g \
>             --executor-memory 1g \
>             --executor-cores 1 \
>             examples/jars/spark-examples*.jar \
>                 10
20/10/07 16:19:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/10/07 16:19:22 WARN DependencyUtils: Local jar /opt/spark-2.4.7-bin-hadoop2.7/conf/examples/jars/spark-examples*.jar does not exist, skipping.
Exception in thread "main" java.lang.NoSuchMethodError: org.apache.hadoop.io.retry.RetryPolicies.retryOtherThanRemoteException(Lorg/apache/hadoop/io/retry/RetryPolicy;Ljava/util/Map;)Lorg/apache/hadoop/io/retry/RetryPolicy;
	at org.apache.hadoop.yarn.client.RMProxy.createRetryPolicy(RMProxy.java:255)
	at org.apache.hadoop.yarn.client.RMProxy.createRMProxy(RMProxy.java:91)
	at org.apache.hadoop.yarn.client.ClientRMProxy.createRMProxy(ClientRMProxy.java:72)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceStart(YarnClientImpl.java:187)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:161)
	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1134)
	at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1526)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:935)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

