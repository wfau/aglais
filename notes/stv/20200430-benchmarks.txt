#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#


# ------------------------------------------------#
# Benchmarking our Prototype with the stv-hdbscan #
# ------------------------------------------------#

TODO: 

    a) Turn these into either pyUnit tests or something equivalent
    b) Create configuration template for the setup 
  

Setup:

7 Nodes 
   22GB RAM / node
   6 VCores / node


Spark Configuration:

	num-executors  6
	spark.driver.memory              13g
	spark.yarn.am.memory            13g
	spark.executor.memory          13g
	spark.driver.maxResultSize	4096m

Hadoop Configuration:

  mapred-site.xml
	yarn.app.mapreduce.am.resource.mb	15000
	mapreduce.map.memory.mb		7000
	mapreduce.reduce.memory.mb	7000

   yarn-site.xml
	yarn.nodemanager.resource.memory-mb	15000
	yarn.scheduler.maximum-allocation-mb	15000
	yarn.scheduler.minimum-allocation-mb	2000



Test 1
------
%spark.pyspark

# define the data frame source on the given column selection/predicates:
df = sqlContext.read.parquet(
    "/hadoop/gaia/parquet/gdr2/gaia_source/*.parquet"
    ).select(
    ["designation","source_id","ra","ra_error","dec","dec_error","parallax","parallax_error","parallax_over_error","pmra","pmra_error","pmdec","pmdec_error","l","b"]
    ).where(
    "abs(b) < 30.0 AND parallax > 1.0 and parallax_over_error > 10.0 AND phot_g_mean_flux_over_error > 36.19 AND astrometric_sigma5d_max < 0.3 AND visibility_periods_used > 8 AND (astrometric_excess_noise < 1 OR (astrometric_excess_noise > 1 AND astrometric_excess_noise_sig < 2))"
    )

# sanity check
df.show()
print ("Data frame rows: ",df.count())


Time: 3 min 23 sec



Test 2	(Depends on Test 1)
---------------------------
%spark.pyspark

import pandas as pd
import numpy as np
spark.conf.set("spark.sql.execution.arrow.enabled", "true")


pandas_df = df.select("*").limit(1000).toPandas()
print(type(pandas_df))
pandas_df.head()


Time: 6 min 32 sec



Test 3	
--------

%spark.pyspark

import random
NUM_SAMPLES = 1000000000

def inside(p):
    x, y = random.random(), random.random()
    return x*x + y*y < 1

count = sc.parallelize(xrange(0, NUM_SAMPLES)) \
             .filter(inside).count()
print "Pi is roughly %f" % (4.0 * count / NUM_SAMPLES)



Time: 3 min 12 sec



Test 4	
--------

%spark.pyspark

from random import choice
from string import digits, ascii_lowercase

chars = digits + ascii_lowercase
seq = ["".join([choice(chars) for i in range(3)]) for j in range(50000000)]
data = sc.parallelize(seq)
counts = data.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b).top(5)
dict(counts)


Time: 1 min 56 sec






