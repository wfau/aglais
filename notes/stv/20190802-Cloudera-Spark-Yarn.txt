#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2018, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#


## Notes on experiments with Cloudera, using the official Docker Image
## Run Spark with Yarn



## Links
https://medium.com/@SnazzyHam/how-to-get-up-and-running-with-clouderas-quickstart-docker-container-732c04ed0280
https://www.cloudera.com/documentation/enterprise/5-6-x/topics/quickstart_docker_container.html
https://justtech.blog/2019/01/31/howto-run-cloudera-quickstart-in-docker/




## Pull official Cloudera image
docker pull cloudera/quickstart:latest




## Start Cloudera container
docker run -it \
 -p 81:80 \
 -p 7180:7180 \
 -p 4040:4040 \
 -p 8020:8020 \
 -p 8022:8022 \
 -p 8030:8030 \
 -p 8032:8032 \
 -p 8033:8033 \
 -p 8040:8040 \
 -p 8042:8042 \
 -p 8088:8088 \
 -p 8480:8480 \
 -p 8485:8485 \
 -p 8888:8888 \
 -p 9083:9083 \
 -p 10020:10020 \
 -p 10033:10033 \
 -p 18088:18088 \
 -p 19888:19888 \
 -p 25000:25000 \
 -p 25010:25010 \
 -p 25020:25020 \
 -p 50010:50010 \
 -p 50020:50020 \
 -p 50070:50070 \
 -p 50075:50075 \
 -h quickstart.cloudera --privileged=true \
   cloudera/quickstart  /usr/bin/docker-quickstart;



## By default, cloudera manager is not started in container, so, lets enable it first.
[root@quickstart /]# /home/cloudera/cloudera-manager --express


## Open in browser

# Hue: http://0.0.0.0:8888 username: cloudera / password: cloudera
# Cloudera Live: http://0.0.0.0:81/#/

# Cloudera Manager http://0.0.0.0:32771/cmf/home
# username: cloudera
# password: cloudera



## Number of errors show up in Cloudera Manager

date      # will show difference between real date and server one
sudo chkconfig --add ntpd
sudo service ntpd restart 
date      # to make sure that ntpd is working and date is sync




## -----------------------------------------------------------------------------





## Check our Zeppelin and Hadoop Version

## In Cloudera Container

[root@quickstart zeppelin]# spark-submit --version
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.0
      /_/
                        
Type --help for more information.



[root@quickstart zeppelin]# hadoop version
Hadoop 2.6.0-cdh5.7.0
Subversion http://github.com/cloudera/hadoop -r c00978c67b0d3fe9f3b896b5030741bd40bf541a
Compiled by jenkins on 2016-03-23T18:36Z
Compiled with protoc 2.5.0
From source with checksum b2eabfa328e763c88cb14168f9b372
This command was run using /usr/jars/hadoop-common-2.6.0-cdh5.7.0.jar





## Can we submit a job to Spark in Yarn mode?
## Example command, files and class are not there, so this should fail gracefully (expecting missing jar / classes message)

spark-submit --class org.apache.spark.examples.SparkPi     --master yarn     --deploy-mode cluster     --driver-memory 4g     --executor-memory 2g     --executor-cores 1     --queue thequeue     examples/jars/spark-examples*.jar     10

..
## 19/08/02 17:54:59 INFO ipc.Client: Retrying connect to server: quickstart.cloudera/172.17.0.2:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)

## Restart Yarn Resource Manager

sudo service hadoop-yarn-resourcemanager restart

## Restart Hadoop Node

service hadoop-hdfs-namenode restart



## And Try again

spark-submit --class org.apache.spark.examples.SparkPi     --master yarn     --deploy-mode cluster     --driver-memory 4g     --executor-memory 2g     --executor-cores 1     --queue thequeue     examples/jars/spark-examples*.jar     10

Exception in thread "main" java.lang.IllegalArgumentException: Required AM memory (4096+409 MB) is above the max threshold (2816 MB) of this cluster! Please increase the value of 'yarn.scheduler.maximum-allocation-mb'.
	at org.apache.spark.deploy.yarn.Client.verifyClusterResources(Client.scala:291)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:140)
	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1023)
	at org.apache.spark.deploy.yarn.Client$.main(Client.scala:1083)
	at org.apache.spark.deploy.yarn.Client.main(Client.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)



## Reduce Executor & driver memory 
## And Try again


19/08/02 18:11:28 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/172.17.0.2:8032
19/08/02 18:11:28 INFO yarn.Client: Requesting a new application from cluster with 0 NodeManagers
19/08/02 18:11:28 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (2816 MB per container)
19/08/02 18:11:28 INFO yarn.Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead
19/08/02 18:11:28 INFO yarn.Client: Setting up container launch context for our AM
19/08/02 18:11:28 INFO yarn.Client: Setting up the launch environment for our AM container
19/08/02 18:11:28 INFO yarn.Client: Preparing resources for our AM container
Exception in thread "main" org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /user/root/.sparkStaging/application_1564769048389_0003. Name node is in safe mode.
The reported blocks 0 needs additional 914 blocks to reach the threshold 0.9990 of total blocks 914.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.



## Remove Node from Safemode
## And Try again

sudo su hdfs -l -c 'hdfs dfsadmin -safemode leave'

 ## Safe mode is OFF


19/08/02 18:14:29 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/172.17.0.2:8032
19/08/02 18:14:29 INFO yarn.Client: Requesting a new application from cluster with 0 NodeManagers
19/08/02 18:14:30 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (2816 MB per container)
19/08/02 18:14:30 INFO yarn.Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead
19/08/02 18:14:30 INFO yarn.Client: Setting up container launch context for our AM
19/08/02 18:14:30 INFO yarn.Client: Setting up the launch environment for our AM container
19/08/02 18:14:30 INFO yarn.Client: Preparing resources for our AM container
19/08/02 18:14:30 INFO yarn.Client: Uploading resource file:/opt/zeppelin/conf/examples/jars/spark-examples*.jar -> hdfs://quickstart.cloudera:8020/user/root/.sparkStaging/application_1564769048389_0004/spark-examples*.jar
19/08/02 18:14:30 INFO yarn.Client: Deleting staging directory .sparkStaging/application_1564769048389_0004
Exception in thread "main" java.io.FileNotFoundException: File file:/opt/zeppelin/conf/examples/jars/spark-examples*.jar does not exist



## Missing Files message
## We now need to setup a real data example

cd ~
mkdir examples
cd examples


nano words.txt
   blah blah ..
....


hdfs dfs -copyFromLocal /root/examples/words.txt


[root@quickstart examples]# hdfs dfs -copyFromLocal /root/examples/words.txt
19/08/02 18:48:56 WARN hdfs.DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /user/root/words.txt._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.

## Restart hdfs nodes
## Namenode not restarting 

	
[root@quickstart examples]# service hadoop-hdfs-namenode start
## Failed to start Hadoop namenode. Return value: 1           [FAILED]


## ...... 


## After trying many things, solution for Namenode to start was to give full permissions (777) to /var/lib/hadoop-hdfs/cache/hdfs/dfs/name directory
## 777 should only be used as a temporary solution, need to figure out later what is the minimum permissions needed (and what group ownership is needed


## Now try copying file from local to hadoop
hadoop fs -copyFromLocal /root/examples/words.txt 
    
    copyFromLocal: `.': No such file or directory


hdfs dfs -ls 

    copyFromLocal: `.': No such file or directory



## Need to specify directory to copy to
# https://stackoverflow.com/questions/48128736/hadoop-copyfromlocal-no-such-file-or-directory

[root@quickstart ~]# hadoop fs -copyFromLocal /root/examples/words.txt  /root
[root@quickstart ~]# hdfs dfs -ls /root
Found 1 items
-rw-r--r--   1 root root        157 2019-08-03 11:21 /root/words.txt






## Create count.py (Spark job file)

from pyspark import SparkConf
from pyspark import SparkContext
import sys
import time


conf = SparkConf()
#conf.setMaster('spark://localhost:7077')
conf.setAppName('spark-basic')
sc = SparkContext(conf=conf)

text_file = sc.textFile("hdfs:/root/words.txt")
counts = text_file.flatMap(lambda line: line.split(" ")) \
             .map(lambda word: (word, 1)) \
             .reduceByKey(lambda a, b: a + b)
counts.saveAsTextFile("hdfs:/root/words-output.txt")




[root@quickstart examples]# spark-submit     --master yarn     --deploy-mode cluster     /root/examples/count.py    
19/08/03 12:43:36 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/172.17.0.2:8032
19/08/03 12:43:37 INFO yarn.Client: Requesting a new application from cluster with 0 NodeManagers
19/08/03 12:43:37 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (2816 MB per container)
19/08/03 12:43:37 INFO yarn.Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead
19/08/03 12:43:37 INFO yarn.Client: Setting up container launch context for our AM
19/08/03 12:43:37 INFO yarn.Client: Setting up the launch environment for our AM container
19/08/03 12:43:37 INFO yarn.Client: Preparing resources for our AM container
Exception in thread "main" org.apache.hadoop.security.AccessControlException: Permission denied: user=root, access=WRITE, inode="/":hdfs:supergroup:drwxr-xr-x



 
## Permission Denied Error
## Change hadoop fs permissions to 777 (Using 777 just for this example, this probably should be a more strict one, or we need to add the user as a superuser?)   

sudo -u hdfs hadoop fs -chmod -R 777 /root
sudo -u hdfs hadoop fs -chmod -R 777 /




## Spark History Node is down
## Logs show a missing directory, create it

sudo -u hdfs hadoop fs -mkdir /user/spark/applicationHistory
sudo -u hdfs hadoop fs -chmod -R 777 /user/spark/applicationHistory




## Retry submitting ..

spark-submit --master yarn --deploy-mode client --executor-memory 1g \
  --name wordcount --conf "spark.app.id=wordcount" count.py \
  hdfs:/root/words.txt 2




## ..  Gets stuck in a loop  ..

19/08/03 13:49:23 INFO yarn.Client: Application report for application_1564832896083_0022 (state: ACCEPTED)
19/08/03 13:49:24 INFO yarn.Client: Application report for application_1564832896083_0022 (state: ACCEPTED)
19/08/03 13:49:25 INFO yarn.Client: Application report for application_1564832896083_0022 (state: ACCEPTED)




## Check Hadoop Yarn node manager and resource manager

[root@quickstart examples]# sudo service hadoop-yarn-nodemanager status
Hadoop nodemanager is not running                          [FAILED]

[root@quickstart examples]# sudo service hadoop-yarn-resourcemanager status
Hadoop resourcemanager is running                          [  OK  ]



[root@quickstart examples]# sudo service hadoop-yarn-nodemanager restart
no nodemanager to stop
Stopped Hadoop nodemanager:                                [  OK  ]
starting nodemanager, logging to /var/log/hadoop-yarn/yarn-yarn-nodemanager-quickstart.cloudera.out
log4j:ERROR Could not find value for key log4j.appender.RFA
log4j:ERROR Could not instantiate appender named "RFA".
log4j:WARN No appenders could be found for logger (org.apache.hadoop.yarn.server.nodemanager.NodeManager).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/llama/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Started Hadoop nodemanager:                                [  OK  ]

[root@quickstart examples]# sudo service hadoop-yarn-nodemanager status
Hadoop nodemanager is running                              [  OK  ]
[root@quickstart examples]# sudo service hadoop-yarn-resourcemanager status
Hadoop resourcemanager is running                          [  OK  ]




## Retry submitting ..

spark-submit --master yarn --deploy-mode client --executor-memory 1g \
  --name wordcount --conf "spark.app.id=wordcount" count.py \
  hdfs:/root/words.txt 2



..

19/08/03 13:56:13 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.1 KB, free 223.3 KB)
19/08/03 13:56:13 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.1 KB, free 228.4 KB)
19/08/03 13:56:13 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.17.0.2:43257 (size: 5.1 KB, free: 530.3 MB)
19/08/03 13:56:13 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
19/08/03 13:56:13 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /root/examples/count.py:15)
19/08/03 13:56:13 INFO cluster.YarnScheduler: Adding task set 0.0 with 2 tasks
19/08/03 13:56:14 INFO spark.ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)
19/08/03 13:56:15 INFO spark.ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)
19/08/03 13:56:28 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
19/08/03 13:56:43 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources

## ----
## Stuck Here




## Initialize with different values

spark-submit      --master yarn     --deploy-mode cluster     --driver-memory 2g     --executor-memory 1g     --executor-cores 1     --queue default     count.py  


19/08/03 14:39:28 INFO yarn.Client: Application report for application_1564832896083_0031 (state: ACCEPTED)
19/08/03 14:39:29 INFO yarn.Client: Application report for application_1564832896083_0031 (state: FINISHED)
19/08/03 14:39:29 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 172.17.0.2
	 ApplicationMaster RPC port: 0
	 queue: root.root
	 start time: 1564843159529
	 final status: FAILED
	 tracking URL: http://quickstart.cloudera:8088/proxy/application_1564832896083_0031/history/application_1564832896083_0031/2
	 user: root
Exception in thread "main" org.apache.spark.SparkException: Application application_1564832896083_0031 finished with failed status
	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1036)
	at org.apache.spark.deploy.yarn.Client$.main(Client.scala:1083)
	at org.apache.spark.deploy.yarn.Client.main(Client.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)


## Finished with Failed status





## I noticed that restarting the yarn managers via commandline makes the Yarn node & resource managers from the Cloudera manager GUI get shutdown
## It seems that they are separate services?
## Shutdown via command line, and start YARN via the GUI

sudo service hadoop-yarn-nodemanager stop 
sudo service hadoop-yarn-resourcemanager stop


## Node manager started, but job history server failed
## Error message points to missing directories, which we create in hdfs

sudo -u hdfs hadoop fs -mkdir /user/history/done_intermediate
sudo -u hdfs hadoop fs -mkdir /user/history/done
sudo -u hdfs hadoop fs -chmod 777 /user/history/done_intermediate
sudo -u hdfs hadoop fs -chmod 777 /user/history/done
sudo -u hdfs hadoop fs -chmod 777 /user/root/.sparkStaging

..


## Try again

[root@quickstart examples]# spark-submit --master yarn --deploy-mode client   --driver-memory 512m     --executor-memory 128m     --executor-cores 1 --name wordcount --conf "spark.app.id=wordcount" wordcount.py hdfs:/root/words.txt 2
19/08/03 15:31:12 INFO spark.SparkContext: Running Spark version 1.6.0
19/08/03 15:31:12 INFO spark.SecurityManager: Changing view acls to: root
19/08/03 15:31:12 INFO spark.SecurityManager: Changing modify acls to: root
19/08/03 15:31:12 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
19/08/03 15:31:13 INFO util.Utils: Successfully started service 'sparkDriver' on port 43253.
19/08/03 15:31:13 INFO slf4j.Slf4jLogger: Slf4jLogger started
19/08/03 15:31:13 INFO Remoting: Starting remoting
19/08/03 15:31:13 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.17.0.2:39719]
19/08/03 15:31:13 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@172.17.0.2:39719]
19/08/03 15:31:13 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 39719.
19/08/03 15:31:13 INFO spark.SparkEnv: Registering MapOutputTracker
19/08/03 15:31:13 INFO spark.SparkEnv: Registering BlockManagerMaster
19/08/03 15:31:13 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-4125eeda-0df8-4e65-a26c-8c1d5fd5d429
19/08/03 15:31:13 INFO storage.MemoryStore: MemoryStore started with capacity 265.4 MB
19/08/03 15:31:13 INFO spark.SparkEnv: Registering OutputCommitCoordinator
19/08/03 15:31:13 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
19/08/03 15:31:13 INFO ui.SparkUI: Started SparkUI at http://172.17.0.2:4040
19/08/03 15:31:13 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/172.17.0.2:8032
19/08/03 15:31:13 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers
19/08/03 15:31:13 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (2816 MB per container)
19/08/03 15:31:13 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
19/08/03 15:31:13 INFO yarn.Client: Setting up container launch context for our AM
19/08/03 15:31:13 INFO yarn.Client: Setting up the launch environment for our AM container
19/08/03 15:31:13 INFO yarn.Client: Preparing resources for our AM container
19/08/03 15:31:14 INFO yarn.Client: Uploading resource file:/tmp/spark-c3cd77a3-b89f-43eb-a62b-7fa8f1c76f7e/__spark_conf__1790388265448624809.zip -> hdfs://quickstart.cloudera:8020/user/root/.sparkStaging/application_1564845508008_0005/__spark_conf__1790388265448624809.zip
19/08/03 15:31:14 INFO spark.SecurityManager: Changing view acls to: root
19/08/03 15:31:14 INFO spark.SecurityManager: Changing modify acls to: root
19/08/03 15:31:14 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
19/08/03 15:31:14 INFO yarn.Client: Submitting application 5 to ResourceManager
19/08/03 15:31:14 INFO impl.YarnClientImpl: Submitted application application_1564845508008_0005
19/08/03 15:31:15 INFO yarn.Client: Application report for application_1564845508008_0005 (state: ACCEPTED)
19/08/03 15:31:15 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: root.root
	 start time: 1564846274503
	 final status: UNDEFINED
	 tracking URL: http://quickstart.cloudera:8088/proxy/application_1564845508008_0005/
	 user: root
19/08/03 15:31:16 INFO yarn.Client: Application report for application_1564845508008_0005 (state: ACCEPTED)
19/08/03 15:31:17 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(null)
19/08/03 15:31:17 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> quickstart.cloudera, PROXY_URI_BASES -> http://quickstart.cloudera:8088/proxy/application_1564845508008_0005), /proxy/application_1564845508008_0005
19/08/03 15:31:17 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
19/08/03 15:31:17 INFO yarn.Client: Application report for application_1564845508008_0005 (state: RUNNING)
19/08/03 15:31:17 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 172.17.0.2
	 ApplicationMaster RPC port: 0
	 queue: root.root
	 start time: 1564846274503
	 final status: UNDEFINED
	 tracking URL: http://quickstart.cloudera:8088/proxy/application_1564845508008_0005/
	 user: root
19/08/03 15:31:17 INFO cluster.YarnClientSchedulerBackend: Application application_1564845508008_0005 has started running.
19/08/03 15:31:17 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34589.
19/08/03 15:31:17 INFO netty.NettyBlockTransferService: Server created on 34589
19/08/03 15:31:17 INFO storage.BlockManager: external shuffle service port = 7337
19/08/03 15:31:17 INFO storage.BlockManagerMaster: Trying to register BlockManager
19/08/03 15:31:17 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.17.0.2:34589 with 265.4 MB RAM, BlockManagerId(driver, 172.17.0.2, 34589)
19/08/03 15:31:17 INFO storage.BlockManagerMaster: Registered BlockManager
19/08/03 15:31:17 INFO scheduler.EventLoggingListener: Logging events to hdfs://quickstart.cloudera:8020/user/spark/applicationHistory/application_1564845508008_0005
19/08/03 15:31:17 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
19/08/03 15:31:18 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 192.6 KB, free 192.6 KB)
19/08/03 15:31:18 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.6 KB, free 215.2 KB)
19/08/03 15:31:18 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.17.0.2:34589 (size: 22.6 KB, free: 265.4 MB)
19/08/03 15:31:18 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2
19/08/03 15:31:18 INFO mapred.FileInputFormat: Total input paths to process : 1
19/08/03 15:31:18 INFO spark.SparkContext: Starting job: collect at /root/examples/wordcount.py:26
19/08/03 15:31:18 INFO scheduler.DAGScheduler: Registering RDD 3 (reduceByKey at /root/examples/wordcount.py:18)
19/08/03 15:31:18 INFO scheduler.DAGScheduler: Registering RDD 7 (reduceByKey at /root/examples/wordcount.py:24)
19/08/03 15:31:18 INFO scheduler.DAGScheduler: Got job 0 (collect at /root/examples/wordcount.py:26) with 2 output partitions
19/08/03 15:31:18 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at /root/examples/wordcount.py:26)
19/08/03 15:31:18 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
19/08/03 15:31:18 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 1)
19/08/03 15:31:18 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /root/examples/wordcount.py:18), which has no missing parents
19/08/03 15:31:18 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.1 KB, free 223.3 KB)
19/08/03 15:31:18 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.1 KB, free 228.5 KB)
19/08/03 15:31:18 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.17.0.2:34589 (size: 5.1 KB, free: 265.4 MB)
19/08/03 15:31:18 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
19/08/03 15:31:18 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /root/examples/wordcount.py:18)
19/08/03 15:31:18 INFO cluster.YarnScheduler: Adding task set 0.0 with 2 tasks
19/08/03 15:31:19 INFO spark.ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)
19/08/03 15:31:20 INFO spark.ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)
19/08/03 15:31:21 INFO cluster.YarnClientSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (quickstart.cloudera:43824) with ID 1
19/08/03 15:31:21 INFO spark.ExecutorAllocationManager: New executor 1 has registered (new total is 1)
19/08/03 15:31:21 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, quickstart.cloudera, partition 0,NODE_LOCAL, 2137 bytes)
19/08/03 15:31:21 INFO storage.BlockManagerMasterEndpoint: Registering block manager quickstart.cloudera:39777 with 66.7 MB RAM, BlockManagerId(1, quickstart.cloudera, 39777)
19/08/03 15:31:22 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on quickstart.cloudera:39777 (size: 5.1 KB, free: 66.7 MB)
19/08/03 15:31:22 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on quickstart.cloudera:39777 (size: 22.6 KB, free: 66.7 MB)
19/08/03 15:31:22 INFO cluster.YarnClientSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (quickstart.cloudera:43832) with ID 2
19/08/03 15:31:22 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, quickstart.cloudera, partition 1,NODE_LOCAL, 2137 bytes)
19/08/03 15:31:22 INFO spark.ExecutorAllocationManager: New executor 2 has registered (new total is 2)
19/08/03 15:31:22 INFO storage.BlockManagerMasterEndpoint: Registering block manager quickstart.cloudera:44157 with 66.7 MB RAM, BlockManagerId(2, quickstart.cloudera, 44157)
19/08/03 15:31:22 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on quickstart.cloudera:44157 (size: 5.1 KB, free: 66.7 MB)
19/08/03 15:31:23 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on quickstart.cloudera:44157 (size: 22.6 KB, free: 66.7 MB)
19/08/03 15:31:23 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1661 ms on quickstart.cloudera (1/2)
19/08/03 15:31:24 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1479 ms on quickstart.cloudera (2/2)
19/08/03 15:31:24 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (reduceByKey at /root/examples/wordcount.py:18) finished in 5.726 s
19/08/03 15:31:24 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
19/08/03 15:31:24 INFO scheduler.DAGScheduler: looking for newly runnable stages
19/08/03 15:31:24 INFO scheduler.DAGScheduler: running: Set()
19/08/03 15:31:24 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 1, ResultStage 2)
19/08/03 15:31:24 INFO scheduler.DAGScheduler: failed: Set()
19/08/03 15:31:24 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[7] at reduceByKey at /root/examples/wordcount.py:24), which has no missing parents
19/08/03 15:31:24 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.2 KB, free 236.7 KB)
19/08/03 15:31:24 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KB, free 241.8 KB)
19/08/03 15:31:24 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.17.0.2:34589 (size: 5.2 KB, free: 265.4 MB)
19/08/03 15:31:24 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
19/08/03 15:31:24 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[7] at reduceByKey at /root/examples/wordcount.py:24)
19/08/03 15:31:24 INFO cluster.YarnScheduler: Adding task set 1.0 with 2 tasks
19/08/03 15:31:24 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, quickstart.cloudera, partition 0,NODE_LOCAL, 1883 bytes)
19/08/03 15:31:24 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, quickstart.cloudera, partition 1,NODE_LOCAL, 1883 bytes)
19/08/03 15:31:24 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on quickstart.cloudera:44157 (size: 5.2 KB, free: 66.7 MB)
19/08/03 15:31:24 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on quickstart.cloudera:39777 (size: 5.2 KB, free: 66.7 MB)
19/08/03 15:31:24 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to quickstart.cloudera:43832
19/08/03 15:31:24 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to quickstart.cloudera:43824
19/08/03 15:31:24 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 162 bytes
19/08/03 15:31:24 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 162 bytes
19/08/03 15:31:24 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 143 ms on quickstart.cloudera (1/2)
19/08/03 15:31:24 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 180 ms on quickstart.cloudera (2/2)
19/08/03 15:31:24 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
19/08/03 15:31:24 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (reduceByKey at /root/examples/wordcount.py:24) finished in 0.182 s
19/08/03 15:31:24 INFO scheduler.DAGScheduler: looking for newly runnable stages
19/08/03 15:31:24 INFO scheduler.DAGScheduler: running: Set()
19/08/03 15:31:24 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 2)
19/08/03 15:31:24 INFO scheduler.DAGScheduler: failed: Set()
19/08/03 15:31:24 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (PythonRDD[10] at collect at /root/examples/wordcount.py:26), which has no missing parents
19/08/03 15:31:24 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 5.0 KB, free 246.9 KB)
19/08/03 15:31:24 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.1 KB, free 250.0 KB)
19/08/03 15:31:24 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.17.0.2:34589 (size: 3.1 KB, free: 265.4 MB)
19/08/03 15:31:24 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
19/08/03 15:31:24 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[10] at collect at /root/examples/wordcount.py:26)
19/08/03 15:31:24 INFO cluster.YarnScheduler: Adding task set 2.0 with 2 tasks
19/08/03 15:31:24 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, quickstart.cloudera, partition 0,NODE_LOCAL, 1894 bytes)
19/08/03 15:31:24 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, quickstart.cloudera, partition 1,NODE_LOCAL, 1894 bytes)
19/08/03 15:31:24 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on quickstart.cloudera:44157 (size: 3.1 KB, free: 66.7 MB)
19/08/03 15:31:24 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on quickstart.cloudera:39777 (size: 3.1 KB, free: 66.7 MB)
19/08/03 15:31:24 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to quickstart.cloudera:43824
19/08/03 15:31:24 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to quickstart.cloudera:43832
19/08/03 15:31:24 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 163 bytes
19/08/03 15:31:24 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 163 bytes
19/08/03 15:31:24 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 91 ms on quickstart.cloudera (1/2)
19/08/03 15:31:24 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 94 ms on quickstart.cloudera (2/2)
19/08/03 15:31:24 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
19/08/03 15:31:24 INFO scheduler.DAGScheduler: ResultStage 2 (collect at /root/examples/wordcount.py:26) finished in 0.094 s
19/08/03 15:31:24 INFO scheduler.DAGScheduler: Job 0 finished: collect at /root/examples/wordcount.py:26, took 6.110949 s
(u'q', 2), (u'k', 5), (u'w', 2), (u'o', 2), (u'd', 2)
19/08/03 15:31:24 INFO spark.SparkContext: Invoking stop() from shutdown hook
19/08/03 15:31:24 INFO ui.SparkUI: Stopped Spark web UI at http://172.17.0.2:4040
19/08/03 15:31:24 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors
19/08/03 15:31:24 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread
19/08/03 15:31:24 INFO cluster.YarnClientSchedulerBackend: Asking each executor to shut down
19/08/03 15:31:24 INFO cluster.YarnClientSchedulerBackend: Stopped
19/08/03 15:31:24 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/08/03 15:31:24 INFO storage.MemoryStore: MemoryStore cleared
19/08/03 15:31:24 INFO storage.BlockManager: BlockManager stopped
19/08/03 15:31:24 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
19/08/03 15:31:24 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/08/03 15:31:24 INFO spark.SparkContext: Successfully stopped SparkContext
19/08/03 15:31:24 INFO util.ShutdownHookManager: Shutdown hook called
19/08/03 15:31:24 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-c3cd77a3-b89f-43eb-a62b-7fa8f1c76f7e
19/08/03 15:31:24 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-c3cd77a3-b89f-43eb-a62b-7fa8f1c76f7e/pyspark-b3563800-7c18-4c5b-a6df-a3d8dea53f07



## Job completed Successfully



