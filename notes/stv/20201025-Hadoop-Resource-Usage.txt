#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#



## Understanding the Hadoop Usage with the current prototype configuration..
## The following notes are experiments run on the automated version of the Hadoop/Zeppelin deploy at http://zeppelin.aglais.uk:8080/#/

# Cluster deployed using the following instructions:
# https://github.com/wfau/aglais/blob/master/notes/stv/20200915-automated-deploy-02.txt


# [Master Node]

ssh master01


# List all applications
# -------------------------

yarn application -list

Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):1
                Application-Id	    Application-Name	    Application-Type	      User	     Queue	             State	       Final-State	       Progress	                       Tracking-URL
application_1603888980510_0001	            Zeppelin	               SPARK	    fedora	   default	           RUNNING	         UNDEFINED	            10%	               http://zeppelin:4040


# This seems to be an application started previously, that has a state of RUNNING?
# Last job was started a day ago, but the cell was completed.. It looks like completed cell != Completed State of Yarn Application



# Check Hadoop GUI
# [Local Machine]
# -------------------------

ssh -L '8088:master01:8088' test-gateway


firefox http://localhost:8088

# Observations:
# Application application_1603888980510_0001 State: RUNNING, FinalStatus: UNDEFINED, %100 of Cluster used 



# Kill Application
# [Master Node]
# -------------------------
 
yarn application -kill application_1603888980510_0001

yarn application -list
Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):0
                Application-Id	    Application-Name	    Application-Type	      User	     Queue	             State	       Final-State	       Progress	                       Tracking-URL



# Check that we can run a job in Zeppelin
# -----------------------------------------

# Try first cell of:
    http://zeppelin.aglais.uk:8080/#/notebook/2FKJM3WZK
 
Py4JJavaError: An error occurred while calling o78.parquet.
: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:921)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.zeppelin.spark.BaseSparkScalaInterpreter.spark2CreateContext(BaseSparkScalaInterpreter.scala:263)
org.apache.zeppelin.spark.BaseSparkScalaInterpreter.createSparkContext(BaseSparkScalaInterpreter.scala:182)
org.apache.zeppelin.spark.SparkScala211Interpreter.open(SparkScala211Interpreter.scala:90)
org.apache.zeppelin.spark.NewSparkInterpreter.open(NewSparkInterpreter.java:102)
org.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:62)
org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:69)
org.apache.zeppelin.spark.IPySparkInterpreter.getSparkInterpreter(IPySparkInterpreter.java:94)
org.apache.zeppelin.spark.IPySparkInterpreter.open(IPySparkInterpreter.java:54)
org.apache.zeppelin.spark.PySparkInterpreter.open(PySparkInterpreter.java:129)
org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:69)
org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:616)
org.apache.zeppelin.scheduler.Job.run(Job.java:188)
org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:140)
java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
java.util.concurrent.FutureTask.run(FutureTask.java:266)

The currently active SparkContext was created at:

(No active SparkContext.)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)
	at org.apache.spark.SparkContext$$anonfun$parallelize$1.apply(SparkContext.scala:717)
	at org.apache.spark.SparkContext$$anonfun$parallelize$1.apply(SparkContext.scala:716)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.SparkContext.withScope(SparkContext.scala:699)
	at org.apache.spark.SparkContext.parallelize(SparkContext.scala:716)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:207)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:127)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:91)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:67)
	at org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$createInMemoryFileIndex(DataSource.scala:547)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:385)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:242)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:230)
	at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:664)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)



# Ok so we killed the Hadoop application, and we can no longer run any Spark cells?


# Restart Zeppelin Interpreter
# ----------------------------

# GUI / Top Right Menu / Interpreter / Spark -> Restart


# Run same cell..

..


# Success...

# Checking logs, we see the same usage (100% usage)



# Try running a cell as another user
# ------------------------------------

# Works fine, but after checking the application in Yarn, we don't see any new applications, just the same one as before, using the same amount of resources..

# It looks like Zeppelin only starts a single Application (or SparkContext) for all users?




# Check the Spark UI for the application
# ------------------------------------

firefox http://localhost:8088/proxy/application_1603888980510_0002/
..


# We see 9 completed jobs, some from user1 (gaiauser) & some from user2 (admin), which confirms that they are both sharing the same SparkContext


# Start two Spark cells at the same time from both users
# --------------------------------------------------------

# Start cell from gaiauser
  # Starts successfully


# Start cell from admin
  # Pending

..

# Cell from gaiauser completes after 5 mins
# Cell from admin starts 


..

# Cell from admin completes



cat capacity-scheduler.xml

..

  <property>
    <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
    <value>1.0</value>
    <description>
      Maximum percent of resources in the cluster which can be used to run 
      application masters i.e. controls number of concurrent running
      applications.
    </description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.root.default.capacity</name>
    <value>100</value>
    <description>Default queue target capacity.</description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.root.default.user-limit-factor</name>
    <value>1</value>
    <description>
      Default queue user limit a percentage from 0.0 to 1.0.
    </description>
  </property>

..



cat yarn-site.xml

..


<property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>20000</value>
</property>

<property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>20000</value>
</property>

<property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>2000</value>
</property>

<property>
   <name>yarn.scheduler.capacity.root.support.user-limit-factor</name>
   <value>2</value>
</property>

..


# Change the Interpreter setting so that the SparkContext is isolated per note:
# -----------------------------------------------------------------------------

# GUI / Interpreter / Spark /
Option
The interpreter will be instantiated "Per Note" (Changed from Globally) in "Isolated" (From scoped per note) process 


#  Start two cells, one from each user again

[fedora@aglais-20201008-master01 ~]$ yarn application -list

Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):2
                Application-Id	    Application-Name	    Application-Type	      User	     Queue	             State	       Final-State	       Progress	                       Tracking-URL
application_1603907955151_0007	            Zeppelin	               SPARK	    fedora	   default	          ACCEPTED	         UNDEFINED	             0%	                                N/A
application_1603907955151_0008	            Zeppelin	               SPARK	    fedora	   default	           RUNNING	         UNDEFINED	            10%	               http://zeppelin:4041



# First job started and running, second job pending (From Zeppelin GUI it shows up as 0% RUNNING)
# application_1603907955151_0008 (first job) is using up 100% of allocation

# First job completed, second job (from second user) never starts

# Assumption here: With isolated mode, we start a SparkContext for the first user which is allocated 100% of queue. Even though cells are completed the SparkContext is not killed
# So the second application never gets allocated resources to run. 

# TODO: Figure out when SparkContext actually is killed/finished for an application




