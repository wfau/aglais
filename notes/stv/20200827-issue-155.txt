#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#


# When running the Latest Kounkel and Covey notebook, there is a particular cell that crashes our Zeppelin/Spark cluster, making all further Spark jobs fail with either of the two following exception:


java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:182)
	at org.apache.zeppelin.interpreter.remote.ClientFactory.create(ClientFactory.java:51)
	at org.apache.zeppelin.interpreter.remote.ClientFactory.create(ClientFactory.java:37)
	at org.apache.commons.pool2.BasePooledObjectFactory.makeObject(BasePooledObjectFactory.java:60)
	at org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:861)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:435)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:363)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.getClient(RemoteInterpreterProcess.java:62)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.callRemoteFunction(RemoteInterpreterProcess.java:133)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.interpret(RemoteInterpreter.java:228)
	at org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:449)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:188)
	at org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:315)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
	at org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Client.recv_interpret(RemoteInterpreterService.java:274)
	at org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Client.interpret(RemoteInterpreterService.java:258)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter$4.call(RemoteInterpreter.java:233)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter$4.call(RemoteInterpreter.java:229)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.callRemoteFunction(RemoteInterpreterProcess.java:135)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.interpret(RemoteInterpreter.java:228)
	at org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:449)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:188)
	at org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:315)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


# Note: This latest version of the notebook was updated recently, and one of the cells causing the exception was one of the updates



# -------------------------------------------------------------
# Attempt 1
# Run the following Notebook:
http://128.232.224.69:8080/#/notebook/2FG43SCW3

# The Cell with ID 20200605-165000_1292852380 seem to have ran for over 20 minutes. Then failed with the following error

io.grpc.StatusRuntimeException: UNAVAILABLE: Network closed for unknown reason
	at io.grpc.Status.asRuntimeException(Status.java:526)
	at io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)
	at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
	at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
	at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
	at io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)
	at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
	at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
	at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
	at io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:403)
	at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)
	at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)
	at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)



ssh stv-dev-worker-8

# Check the logs of the worker node that was running the job 

2020-08-25 17:45:40,559 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Exception for BP-964714065-10.0.0.14-1577491045302:blk_1073892579_151769
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:212)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-08-25 17:45:40,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-964714065-10.0.0.14-1577491045302:blk_1073892579_151769, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.0.16:9866]: Thread is interrupted.
2020-08-25 17:45:40,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-964714065-10.0.0.14-1577491045302:blk_1073892579_151769, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.0.16:9866] terminating
2020-08-25 17:45:40,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-964714065-10.0.0.14-1577491045302:blk_1073892579_151769 received exception java.io.IOException: Premature EOF from inputStream
2020-08-25 17:45:40,623 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: stv-dev-worker-8:9866:DataXceiver error processing WRITE_BLOCK operation  src: /10.0.0.5:55758 dst: /10.0.0.5:9866
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:212)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-08-25 17:48:07,936 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073892568_151758 replica FinalizedReplica, blk_1073892568_151758, FINALIZED




# -------------------------------------------------------------
# Attempt 2 (27/08/2020)
# Run the following Notebook:
http://128.232.224.69:8080/#/notebook/2FG43SCW3

The Cell with ID 20200605-165000_1292852380 seem to have ran for over 40 minutes this time.

# After refreshing the Zeppelin page, I see the following exception:

org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
	at org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Client.recv_interpret(RemoteInterpreterService.java:274)
	at org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Client.interpret(RemoteInterpreterService.java:258)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter$4.call(RemoteInterpreter.java:233)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter$4.call(RemoteInterpreter.java:229)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.callRemoteFunction(RemoteInterpreterProcess.java:135)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.interpret(RemoteInterpreter.java:228)
	at org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:449)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:188)
	at org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:315)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)


While the job was running, it appeared to be running on the worker node:  stv-dev-worker-3


Looking in the Hadoop UI, two nodes appear with a state of: LOST

/default-rack	LOST	stv-dev-worker-4:44115	N/A	Wed Aug 26 10:51:27 +0000 2020		0		0 B	0 B	0	0	3.1.3
/default-rack	LOST	stv-dev-worker-3:33615	N/A	Thu Aug 27 09:29:27 +0000 2020		0		0 B	0 B	0	0	3.1.3


# Check the DFS Report


hdfs dfsadmin -report
Live datanodes (7):

Name: 10.0.0.13:9866 (stv-dev-worker-7)
Hostname: stv-dev-worker-7
Decommission Status : Normal
Configured Capacity: 528376639488 (492.09 GB)
DFS Used: 387622567936 (361.00 GB)
Non DFS Used: 6427123712 (5.99 GB)
DFS Remaining: 112793006080 (105.05 GB)
DFS Used%: 73.36%
DFS Remaining%: 21.35%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Thu Aug 27 10:08:19 UTC 2020
Last Block Report: Thu Aug 27 04:57:54 UTC 2020
Num of Blocks: 26119


Name: 10.0.0.16:9866 (stv-dev-worker-1)
Hostname: stv-dev-worker-1
Decommission Status : Normal
Configured Capacity: 528376639488 (492.09 GB)
DFS Used: 369958408192 (344.55 GB)
Non DFS Used: 5679837184 (5.29 GB)
DFS Remaining: 131204452352 (122.19 GB)
DFS Used%: 70.02%
DFS Remaining%: 24.83%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Thu Aug 27 10:08:19 UTC 2020
Last Block Report: Thu Aug 27 08:21:51 UTC 2020
Num of Blocks: 21501


Name: 10.0.0.17:9866 (stv-dev-storage)
Hostname: stv-dev-storage
Decommission Status : Decommissioned
Configured Capacity: 1056821088256 (984.24 GB)
DFS Used: 222069616640 (206.82 GB)
Non DFS Used: 591807045632 (551.16 GB)
DFS Remaining: 199936179101 (186.21 GB)
DFS Used%: 21.01%
DFS Remaining%: 18.92%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Thu Aug 27 10:08:19 UTC 2020
Last Block Report: Thu Aug 27 08:42:49 UTC 2020
Num of Blocks: 13386


Name: 10.0.0.28:9866 (stv-dev-worker-5)
Hostname: stv-dev-worker-5
Decommission Status : Normal
Configured Capacity: 528376639488 (492.09 GB)
DFS Used: 367202070528 (341.98 GB)
Non DFS Used: 6508974080 (6.06 GB)
DFS Remaining: 133131653120 (123.99 GB)
DFS Used%: 69.50%
DFS Remaining%: 25.20%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Thu Aug 27 10:08:20 UTC 2020
Last Block Report: Thu Aug 27 09:22:49 UTC 2020
Num of Blocks: 21196


Name: 10.0.0.4:9866 (stv-dev-worker-2)
Hostname: stv-dev-worker-2
Decommission Status : Normal
Configured Capacity: 528376639488 (492.09 GB)
DFS Used: 372985827328 (347.37 GB)
Non DFS Used: 6503141376 (6.06 GB)
DFS Remaining: 127353729024 (118.61 GB)
DFS Used%: 70.59%
DFS Remaining%: 24.10%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Thu Aug 27 10:08:20 UTC 2020
Last Block Report: Thu Aug 27 04:57:31 UTC 2020
Num of Blocks: 21200


Name: 10.0.0.5:9866 (stv-dev-worker-8)
Hostname: stv-dev-worker-8
Decommission Status : Normal
Configured Capacity: 528376639488 (492.09 GB)
DFS Used: 378755100672 (352.74 GB)
Non DFS Used: 6419378176 (5.98 GB)
DFS Remaining: 121668218880 (113.31 GB)
DFS Used%: 71.68%
DFS Remaining%: 23.03%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Thu Aug 27 10:08:21 UTC 2020
Last Block Report: Thu Aug 27 05:52:26 UTC 2020
Num of Blocks: 25448


Name: 10.0.0.6:9866 (stv-dev-worker-3)
Hostname: stv-dev-worker-3
Decommission Status : Normal
Configured Capacity: 528376639488 (492.09 GB)
DFS Used: 362830401536 (337.91 GB)
Non DFS Used: 5826281472 (5.43 GB)
DFS Remaining: 138186014720 (128.70 GB)
DFS Used%: 68.67%
DFS Remaining%: 26.15%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Thu Aug 27 10:08:19 UTC 2020
Last Block Report: Thu Aug 27 07:49:56 UTC 2020
Num of Blocks: 20905


Dead datanodes (1):

Name: 10.0.0.33:9866 (stv-dev-worker-4)
Hostname: stv-dev-worker-4
Decommission Status : Normal
Configured Capacity: 528376639488 (492.09 GB)
DFS Used: 316180230144 (294.47 GB)
Non DFS Used: 6519709696 (6.07 GB)
DFS Remaining: 184142761984 (171.50 GB)
DFS Used%: 59.84%
DFS Remaining%: 34.85%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 0
Last contact: Wed Aug 26 10:52:13 UTC 2020
Last Block Report: Wed Aug 26 10:04:27 UTC 2020
Num of Blocks: 0


# We see just one dead datanode 



# Let's check the job status in the Yarn list (UI)
# http://localhost:8088/cluster/apps

application_1588261403747_0128	fedora	Zeppelin	SPARK	default	0	Wed Aug 26 13:07:41 +0300 2020	Wed Aug 26 13:07:41 +0300 2020	Thu Aug 27 12:45:31 +0300 2020	FAILED	FAILED	


# Status of job is FAILED


# Check the application info 
# http://localhost:8088/cluster/app/application_1588261403747_0128

Diagnostics:	
Application application_1588261403747_0128 failed 2 times due to AM Container for appattempt_1588261403747_0128_000002 exited with exitCode: 13
Failing this attempt.Diagnostics: [2020-08-27 09:45:31.087]Exception from container-launch.
Container id: container_1588261403747_0128_02_000001
Exit code: 13
[2020-08-27 09:45:31.090]Container exited with a non-zero exit code 13. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
Last 4096 bytes of stderr :
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/tmp/hadoop-fedora/nm-local-dir/usercache/fedora/filecache/674/__spark_libs__415931690323199248.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/fedora/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
[2020-08-27 09:45:31.091]Container exited with a non-zero exit code 13. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
Last 4096 bytes of stderr :
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/tmp/hadoop-fedora/nm-local-dir/usercache/fedora/filecache/674/__spark_libs__415931690323199248.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/fedora/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
For more detailed output, check the application tracking page: http://stv-dev-master:8088/cluster/app/application_1588261403747_0128 Then click on links to logs of each attempt.
. Failing the application.


#-----------------------------------------
# Check the logs of the LOST worker node that was running the application

ssh stv-dev-worker-8



# Check Datanode Logs

tail -f -n 1000 logs/hadoop-fedora-datanode-stv-dev-worker-3.novalocal.log

2020-08-27 01:49:55,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x462dd6bc8f81065c,  containing 1 storage report(s), of which we sent 1. The reports had 20911 total blocks and used 1 RPC(s). This took 4 msec to generate and 18 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-08-27 01:49:55,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-964714065-10.0.0.14-1577491045302
2020-08-27 05:27:33,961 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-964714065-10.0.0.14-1577491045302 Total blocks: 20910, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2020-08-27 07:49:56,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x462dd6bc8f81065d,  containing 1 storage report(s), of which we sent 1. The reports had 20911 total blocks and used 1 RPC(s). This took 4 msec to generate and 17 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-08-27 07:49:56,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-964714065-10.0.0.14-1577491045302
2020-08-27 08:38:20,828 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for /home/fedora/hadoop/data/dataNode
2020-08-27 08:54:18,275 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for /home/fedora/hadoop/data/dataNode
2020-08-27 09:19:31,919 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1761ms
No GCs detected
2020-08-27 09:20:32,571 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4644ms
No GCs detected
2020-08-27 09:30:40,852 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1475ms
No GCs detected
2020-08-27 09:30:42,994 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1599ms
No GCs detected
2020-08-27 09:30:45,139 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1630ms
No GCs detected
2020-08-27 09:30:47,299 WARN org.apache.hadoop.fs.CachingGetSpaceUsed: Could not get disk usage information for path /home/fedora/hadoop/data/dataNode/current/BP-964714065-10.0.0.14-1577491045302
java.io.IOException: Expecting a line not the end of stream
	at org.apache.hadoop.fs.DU$DUShell.parseExecResult(DU.java:79)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:995)
	at org.apache.hadoop.util.Shell.run(Shell.java:902)
	at org.apache.hadoop.fs.DU$DUShell.startRefresh(DU.java:62)
	at org.apache.hadoop.fs.DU.refresh(DU.java:53)
	at org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread.run(CachingGetSpaceUsed.java:181)
	at java.lang.Thread.run(Thread.java:748)
2020-08-27 09:31:53,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Exception for BP-964714065-10.0.0.14-1577491045302:blk_1073892589_151780
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:212)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-08-27 09:31:53,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-964714065-10.0.0.14-1577491045302:blk_1073892589_151780, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.0.16:9866]: Thread is interrupted.
2020-08-27 09:31:53,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-964714065-10.0.0.14-1577491045302:blk_1073892589_151780, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.0.16:9866] terminating
2020-08-27 09:31:53,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-964714065-10.0.0.14-1577491045302:blk_1073892589_151780 received exception java.io.IOException: Premature EOF from inputStream
2020-08-27 09:31:53,814 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: stv-dev-worker-3:9866:DataXceiver error processing WRITE_BLOCK operation  src: /10.0.0.6:37702 dst: /10.0.0.6:9866
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:212)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:528)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:971)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:908)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
2020-08-27 09:45:31,522 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073892581_151772 replica FinalizedReplica, blk_1073892581_151772, FINALIZED
  getNumBytes()     = 104443709
  getBytesOnDisk()  = 104443709
  getVisibleLength()= 104443709
  getVolume()       = /home/fedora/hadoop/data/dataNode
  getBlockURI()     = file:/home/fedora/hadoop/data/dataNode/current/BP-964714065-10.0.0.14-1577491045302/current/finalized/subdir2/subdir12/blk_1073892581 for deletion
2020-08-27 09:45:31,524 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073892582_151773 replica FinalizedReplica, blk_1073892582_151773, FINALIZED
  getNumBytes()     = 21151354
  getBytesOnDisk()  = 21151354
  getVisibleLength()= 21151354
  getVolume()       = /home/fedora/hadoop/data/dataNode
  getBlockURI()     = file:/home/fedora/hadoop/data/dataNode/current/BP-964714065-10.0.0.14-1577491045302/current/finalized/subdir2/subdir12/blk_1073892582 for deletion
2020-08-27 09:45:31,524 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073892584_151775 replica FinalizedReplica, blk_1073892584_151775, FINALIZED
  getNumBytes()     = 1623235
  getBytesOnDisk()  = 1623235
  getVisibleLength()= 1623235
  getVolume()       = /home/fedora/hadoop/data/dataNode
  getBlockURI()     = file:/home/fedora/hadoop/data/dataNode/current/BP-964714065-10.0.0.14-1577491045302/current/finalized/subdir2/subdir12/blk_1073892584 for deletion
2020-08-27 09:45:31,524 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073892585_151776 replica FinalizedReplica, blk_1073892585_151776, FINALIZED
  getNumBytes()     = 591770
  getBytesOnDisk()  = 591770
  getVisibleLength()= 591770
  getVolume()       = /home/fedora/hadoop/data/dataNode
  getBlockURI()     = file:/home/fedora/hadoop/data/dataNode/current/BP-964714065-10.0.0.14-1577491045302/current/finalized/subdir2/subdir12/blk_1073892585 for deletion
2020-08-27 09:45:31,524 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073892586_151777 replica FinalizedReplica, blk_1073892586_151777, FINALIZED
  getNumBytes()     = 42437
  getBytesOnDisk()  = 42437
  getVisibleLength()= 42437
  getVolume()       = /home/fedora/hadoop/data/dataNode
  getBlockURI()     = file:/home/fedora/hadoop/data/dataNode/current/BP-964714065-10.0.0.14-1577491045302/current/finalized/subdir2/subdir12/blk_1073892586 for deletion
2020-08-27 09:45:31,618 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-964714065-10.0.0.14-1577491045302 blk_1073892581_151772 URI file:/home/fedora/hadoop/data/dataNode/current/BP-964714065-10.0.0.14-1577491045302/current/finalized/subdir2/subdir12/blk_1073892581
2020-08-27 09:45:31,618 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-964714065-10.0.0.14-1577491045302 blk_1073892582_151773 URI file:/home/fedora/hadoop/data/dataNode/current/BP-964714065-10.0.0.14-1577491045302/current/finalized/subdir2/subdir12/blk_1073892582
2020-08-27 09:45:31,618 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-964714065-10.0.0.14-1577491045302 blk_1073892584_151775 URI file:/home/fedora/hadoop/data/dataNode/current/BP-964714065-10.0.0.14-1577491045302/current/finalized/subdir2/subdir12/blk_1073892584
2020-08-27 09:45:31,618 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-964714065-10.0.0.14-1577491045302 blk_1073892585_151776 URI file:/home/fedora/hadoop/data/dataNode/current/BP-964714065-10.0.0.14-1577491045302/current/finalized/subdir2/subdir12/blk_1073892585
2020-08-27 09:45:31,618 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-964714065-10.0.0.14-1577491045302 blk_1073892586_151777 URI file:/home/fedora/hadoop/data/dataNode/current/BP-964714065-10.0.0.14-1577491045302/current/finalized/subdir2/subdir12/blk_1073892586



# Relevant exception seems to be:
2020-08-27 09:30:47,299 WARN org.apache.hadoop.fs.CachingGetSpaceUsed: Could not get disk usage information for path /home/fedora/hadoop/data/dataNode/current/BP-964714065-10.0.0.14-1577491045302
java.io.IOException: Expecting a line not the end of stream



# Check Nodemanager Logs

tail -f -n 1000 hadoop-fedora-nodemanager-stv-dev-worker-3.novalocal.log


2020-08-27 08:43:30,629 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 10545195702, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
2020-08-27 08:53:30,629 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 10545195702, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
2020-08-27 09:03:30,700 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 10545195702, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
2020-08-27 09:04:36,453 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 5302ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1604ms
2020-08-27 09:10:28,149 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4142ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1421ms
2020-08-27 09:13:30,715 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 10545195702, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
2020-08-27 09:15:08,836 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 7771ms
GC pool 'PS Scavenge' had collection(s): count=1 time=3893ms
2020-08-27 09:20:09,586 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 7934ms
GC pool 'PS Scavenge' had collection(s): count=1 time=3748ms
2020-08-27 09:23:30,745 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 10545195702, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0



# Check secondary namenode info

tail -f -n 1000 hadoop-fedora-secondarynamenode-stv-dev-worker-3.novalocal.log


1045302:CID-fc745354-40f2-499b-891d-fa1258d09dc6&bootstrapstandby=false
2020-08-27 08:07:27,489 INFO org.apache.hadoop.hdfs.server.common.Util: Combined time for file download and fsync to all disks took 0.04s. The file download took 0.04s at 189930.23 KB/s. Synchronous (fsync) write to disk of /tmp/hadoop-fedora/dfs/namesecondary/current/fsimage.ckpt_0000000000001897931 took 0.00s.
2020-08-27 08:07:27,489 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000001897931 size 8363845 bytes.
2020-08-27 08:07:27,515 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://stv-dev-master:9870/imagetransfer?getedit=1&startTxId=1897932&endTxId=1897933&storageInfo=-64:1262747535:1577491045302:CID-fc745354-40f2-499b-891d-fa1258d09dc6
2020-08-27 08:07:27,548 INFO org.apache.hadoop.hdfs.server.common.Util: Combined time for file download and fsync to all disks took 0.00s. The file download took 0.00s at 0.00 KB/s. Synchronous (fsync) write to disk of /tmp/hadoop-fedora/dfs/namesecondary/current/edits_tmp_0000000000001897932-0000000000001897933_0000000022064187570 took 0.00s.
2020-08-27 08:07:27,548 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000001897932-0000000000001897933_0000000022064187570 size 0 bytes.
2020-08-27 08:07:27,630 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 68192 INodes.
2020-08-27 08:07:27,985 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2020-08-27 08:07:27,985 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 1897931 from /tmp/hadoop-fedora/dfs/namesecondary/current/fsimage_0000000000001897931
2020-08-27 08:07:27,985 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 3 entries 13164 lookups
2020-08-27 08:07:27,985 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2020-08-27 08:07:27,985 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-fedora/dfs/namesecondary/current/edits_0000000000001897932-0000000000001897933 expecting start txid #1897932
2020-08-27 08:07:27,985 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-fedora/dfs/namesecondary/current/edits_0000000000001897932-0000000000001897933 maxTxnsToRead = 9223372036854775807
2020-08-27 08:07:27,987 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded 1 edits file(s) (the last named /tmp/hadoop-fedora/dfs/namesecondary/current/edits_0000000000001897932-0000000000001897933) of total size 42.0, total edits 2.0, total load time 2.0 ms
2020-08-27 08:07:28,001 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-fedora/dfs/namesecondary/current/fsimage.ckpt_0000000000001897933 using no compression
2020-08-27 08:07:28,121 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-fedora/dfs/namesecondary/current/fsimage.ckpt_0000000000001897933 of size 8363845 bytes saved in 0 seconds .
2020-08-27 08:07:28,146 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 1897931
2020-08-27 08:07:28,146 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-fedora/dfs/namesecondary/current/fsimage_0000000000001897915, cpktTxId=0000000000001897915)
2020-08-27 08:07:28,147 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-fedora/dfs/namesecondary/current/fsimage_0000000000001897913, cpktTxId=0000000000001897913)
2020-08-27 08:07:28,216 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-fedora/dfs/namesecondary/current/fsimage_0000000000001897933, fileSize: 8363845. Sent total: 8363845 bytes. Size of last segment intended to send: -1 bytes.
2020-08-27 08:07:28,358 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 1897933 to namenode at http://stv-dev-master:9870 in 0.159 seconds
2020-08-27 08:07:28,358 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 8363845
2020-08-27 09:08:27,406 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2020-08-27 09:08:27,445 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://stv-dev-master:9870/imagetransfer?getimage=1&txid=1904463&storageInfo=-64:1262747535:1577491045302:CID-fc745354-40f2-499b-891d-fa1258d09dc6&bootstrapstandby=false
2020-08-27 09:08:33,496 INFO org.apache.hadoop.hdfs.server.common.Util: Combined time for file download and fsync to all disks took 5.29s. The file download took 5.29s at 1544.44 KB/s. Synchronous (fsync) write to disk of /tmp/hadoop-fedora/dfs/namesecondary/current/fsimage.ckpt_0000000000001904463 took 0.00s.
2020-08-27 09:08:33,527 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000001904463 size 8363845 bytes.
2020-08-27 09:08:33,813 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://stv-dev-master:9870/imagetransfer?getedit=1&startTxId=1904464&endTxId=1904465&storageInfo=-64:1262747535:1577491045302:CID-fc745354-40f2-499b-891d-fa1258d09dc6
2020-08-27 09:08:34,176 INFO org.apache.hadoop.hdfs.server.common.Util: Combined time for file download and fsync to all disks took 0.12s. The file download took 0.12s at 0.00 KB/s. Synchronous (fsync) write to disk of /tmp/hadoop-fedora/dfs/namesecondary/current/edits_tmp_0000000000001904464-0000000000001904465_0000000022067853868 took 0.00s.
2020-08-27 09:08:34,208 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000001904464-0000000000001904465_0000000022067853868 size 0 bytes.
2020-08-27 09:08:35,003 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 68192 INodes.





# Can we ssh from master to worker 3?
ssh stv-dev-master
  ssh stv-dev-worker-3
    Last login: Thu Aug 27 10:26:21 2020 from 10.0.0.20
    [fedora@stv-dev-worker-3 ~]$ exit
  
# SSH works fine..



# To summarise: 
# We're running the HDBSCAN on a single worker node via a Spark job. This job takes a long time to continue, and eventuall causes the worker node to detach from the Yarn/Hadoop Cluster
# Any further Spark jobs fail with a "Connection Refused" exception
# ...


# Short term solution:
# Restart Hadoop

ssh stv-dev-master
  stop-all.sh
  start-all.sh
exit


# ....
# Why isn't Hadoop/Yarn able to recover the lost node?
# Why can Zeppelin no longer successfully run a Spark job, even though its only a single worker that has failed?
# Is Zeppelin only using the SparkContext that was opened last that is accessing that failed worker node? Even so, why would further jobs only go to that one worker node?
# Do Spark jobs via the command line still work?



# Testing out that last one..
# ------------------------------------------------
ssh stv-dev-master
cd ~/spark

    spark-submit --class org.apache.spark.examples.SparkPi     --master yarn     --deploy-mode cluster     --driver-memory 4g     --executor-memory 2g     --executor-cores 1       examples/jars/spark-examples*.jar     10

2020-08-27 19:21:53,914 INFO yarn.Client: Application report for application_1588261403747_0131 (state: RUNNING)
2020-08-27 19:21:54,916 INFO yarn.Client: Application report for application_1588261403747_0131 (state: FINISHED)
2020-08-27 19:21:54,916 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: stv-dev-worker-1
	 ApplicationMaster RPC port: 36105
	 queue: default
	 start time: 1598556080836
	 final status: SUCCEEDED
	 tracking URL: http://stv-dev-master:8088/proxy/application_1588261403747_0131/
	 user: fedora
2020-08-27 19:21:54,924 INFO util.ShutdownHookManager: Shutdown hook called
2020-08-27 19:21:54,925 INFO util.ShutdownHookManager: Deleting directory /home/fedora/spark/local/spark-119d0225-b0a8-4178-86a0-682dbff66579
2020-08-27 19:21:54,935 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-3fd11748-706d-4705-8dc5-99d1b2eb68d6


exit


# Spark job via command line client works fine
# Checking the Yarn list, the application shows up as SUCCESSFUL, and having been run on stv-dev-worker-1



# Let's check the Hadoop logs on master
# ---------------------------------------------------

ssh stv-dev-master
cd hadoop/logs

tail -f -n 1000 hadoop-fedora-namenode-stv-dev-master.novalocal.log

2020-08-27 19:51:37,586 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=7, selected=[], unavailable=[DISK, ARCHIVE], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2020-08-27 19:51:37,586 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 7 (unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) All required storage types are unavailable:  unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2020-08-27 19:51:37,586 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Not enough replicas was chosen. Reason:{NOT_IN_SERVICE=1}
2020-08-27 19:51:37,586 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 7 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2020-08-27 19:51:37,586 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 7 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology
2020-08-27 19:51:37,586 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=7, selected=[], unavailable=[DISK, ARCHIVE], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2020-08-27 19:51:37,586 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 7 (unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) All required storage types are unavailable:  unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}


# This shows the following issues:

> Failed to place enough replicas, still in need of 1 to reach 7
> Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected 


# Is this something else or related?
# This message repeats every 3 seconds..



# Let's check the resource manager logs

tail -f -n 1000 hadoop-fedora-resourcemanager-stv-dev-master.novalocal.log

2020-08-27 09:31:53,720 INFO org.apache.http.impl.client.DefaultHttpClient: I/O exception (org.apache.http.NoHttpResponseException) caught when processing request to stv-dev-master/10.0.0.14->{}->http://stv-dev-worker-3:44789: The target server failed to respond
2020-08-27 09:31:53,720 INFO org.apache.http.impl.client.DefaultHttpClient: Retrying request to stv-dev-master/10.0.0.14->{}->http://stv-dev-worker-3:44789
2020-08-27 09:31:53,726 WARN org.eclipse.jetty.servlet.ServletHandler: /proxy/application_1588261403747_0128/
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at org.apache.http.conn.scheme.PlainSocketFactory.connectSocket(PlainSocketFactory.java:120)
	at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:179)
	at org.apache.http.impl.conn.ManagedClientConnectionImpl.open(ManagedClientConnectionImpl.java:328)
	at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:675)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:488)
	at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:884)
	at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)
	at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:107)
	at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:251)
	at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.methodAction(WebAppProxyServlet.java:457)
	at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:300)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:941)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:875)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:178)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)
	at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)
	at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)
	at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)
	at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)
	at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:644)
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.doFilter(DelegationTokenAuthenticationFilter.java:304)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:592)
	at org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter.doFilter(RMAuthenticationFilter.java:82)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1620)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:539)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
2020-08-27 09:31:56,804 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1588261403747_0128_01_000014 Container Transitioned from RUNNING to COMPLETED
2020-08-27 09:31:56,804 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=fedora	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1588261403747_0128	CONTAINERID=container_1588261403747_0128_01_000014	RESOURCE=<memory:20000, vCores:1>	QUEUENAME=default
2020-08-27 09:31:56,805 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1588261403747_0128_000001 container=null queue=default clusterResource=<memory:120000, vCores:48> type=OFF_SWITCH requestedPartition=
2020-08-27 09:31:56,806 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1588261403747_0128_01_000015 Container Transitioned from NEW to ALLOCATED
2020-08-27 09:31:56,806 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1588261403747_0128_01_000015 of capacity <memory:20000, vCores:1> on host stv-dev-worker-8:44525, which has 1 containers, <memory:20000, vCores:1> used and <memory:0, vCores:7> available after allocation
2020-08-27 09:31:56,806 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=fedora	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1588261403747_0128	CONTAINERID=container_1588261403747_0128_01_000015	RESOURCE=<memory:20000, vCores:1>	QUEUENAME=default
2020-08-27 09:31:56,806 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=1.0 absoluteUsedCapacity=1.0 used=<memory:120000, vCores:6> cluster=<memory:120000, vCores:48>
2020-08-27 09:31:56,806 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2020-08-27 09:31:57,375 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1588261403747_0128_01_000002 Container Transitioned from RUNNING to COMPLETED
2020-08-27 09:31:57,375 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=fedora	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1588261403747_0128	CONTAINERID=container_1588261403747_0128_01_000002	RESOURCE=<memory:20000, vCores:1>	QUEUENAME=default
2020-08-27 09:31:57,738 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1588261403747_0128_01_000007 Container Transitioned from RUNNING to COMPLETED
2020-08-27 09:31:57,738 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=fedora	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1588261403747_0128	CONTAINERID=container_1588261403747_0128_01_000007	RESOURCE=<memory:20000, vCores:1>	QUEUENAME=default
2020-08-27 09:31:58,175 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1588261403747_0128_01_000004 Container Transitioned from RUNNING to COMPLETED
2020-08-27 09:31:58,175 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=fedora	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1588261403747_0128	CONTAINERID=container_1588261403747_0128_01_000004	RESOURCE=<memory:20000, vCores:1>	QUEUENAME=default
2020-08-27 09:31:58,519 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1588261403747_0128_01_000003 Container Transitioned from RUNNING to COMPLETED
2020-08-27 09:31:58,519 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=fedora	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1588261403747_0128	CONTAINERID=container_1588261403747_0128_01_000003	RESOURCE=<memory:20000, vCores:1>	QUEUENAME=default
2020-08-27 09:42:22,413 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://stv-dev-worker-3:44789 which is the app master GUI of application_1588261403747_0128 owned by fedora
2020-08-27 09:42:22,417 WARN org.eclipse.jetty.servlet.ServletHandler: /proxy/application_1588261403747_0128/
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at org.apache.http.conn.scheme.PlainSocketFactory.connectSocket(PlainSocketFactory.java:120)
	at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:179)
	at org.apache.http.impl.conn.ManagedClientConnectionImpl.open(ManagedClientConnectionImpl.java:328)
	at org.apache.http.impl.client.DefaultRequestDirector.tryConnect(DefaultRequestDirector.java:612)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:447)
	at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:884)
	at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)
	at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:107)
	at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:251)
	at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.methodAction(WebAppProxyServlet.java:457)
	at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:300)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:941)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:875)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:178)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)
	at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)
	at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)
	at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)
	at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)
	at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:644)
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.doFilter(DelegationTokenAuthenticationFilter.java:304)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:592)
	at org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter.doFilter(RMAuthenticationFilter.java:82)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1620)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:539)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
2020-08-27 09:42:24,296 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://stv-dev-worker-3:44789 which is the app master GUI of application_1588261403747_0128 owned by fedora
2020-08-27 09:42:24,297 WARN org.eclipse.jetty.servlet.ServletHandler: /proxy/application_1588261403747_0128/
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at org.apache.http.conn.scheme.PlainSocketFactory.connectSocket(PlainSocketFactory.java:120)
	at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:179)
	at org.apache.http.impl.conn.ManagedClientConnectionImpl.open(ManagedClientConnectionImpl.java:328)
	at org.apache.http.impl.client.DefaultRequestDirector.tryConnect(DefaultRequestDirector.java:612)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:447)
	at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:884)
	at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)
	at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:107)
	at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:251)
	at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.methodAction(WebAppProxyServlet.java:457)
	at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:300)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:941)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:875)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:178)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)
	at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)
	at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)
	at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)
	at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)
	at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:644)
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.doFilter(DelegationTokenAuthenticationFilter.java:304)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:592)
	at org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter.doFilter(RMAuthenticationFilter.java:82)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1620)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:539)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
2020-08-27 09:42:35,445 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://stv-dev-worker-3:44789 which is the app master GUI of application_1588261403747_0128 owned by fedora
2020-08-27 09:42:35,447 WARN org.eclipse.jetty.servlet.ServletHandler: /proxy/application_1588261403747_0128/
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at org.apache.http.conn.scheme.PlainSocketFactory.connectSocket(PlainSocketFactory.java:120)
	at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:179)
	at org.apache.http.impl.conn.ManagedClientConnectionImpl.open(ManagedClientConnectionImpl.java:328)
	at org.apache.http.impl.client.DefaultRequestDirector.tryConnect(DefaultRequestDirector.java:612)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:447)
	at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:884)
	at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)
	at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:107)
	at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:251)
	at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.methodAction(WebAppProxyServlet.java:457)
	at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:300)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:941)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:875)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:178)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)
	at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)
	at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)
	at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)
	at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)
	at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:644)
	at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.doFilter(DelegationTokenAuthenticationFilter.java:304)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:592)
	at org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter.doFilter(RMAuthenticationFilter.java:82)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1620)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:539)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
2020-08-27 09:43:34,190 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: Expired:appattempt_1588261403747_0128_000001 Timed out after 600 secs



#

