#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#


# -----------------------------------------------------
# Create Clouds YAML file
#[user@desktop]

cat > "${HOME}/clouds.yaml" << EOF

clouds:


  gaia-test:
    auth:
      auth_url: https://cumulus.openstack.hpc.cam.ac.uk:5000/v3
      application_credential_id:     '$(secret 'stv-gaia-test.CREDENTIAL_ID')'
      application_credential_secret: '$(secret 'stv-gaia-test.CREDENTIAL_SECRET')'
    region_name: "RegionOne"
    interface: "public"
    identity_api_version: 3
    auth_type: "v3applicationcredential"

  gaia-test-super:
    auth:
      auth_url: https://cumulus.openstack.hpc.cam.ac.uk:5000/v3
      application_credential_id:     '$(secret 'stv-gaia-test.CREDENTIAL_ID')'
      application_credential_secret: '$(secret 'stv-gaia-test.CREDENTIAL_SECRET')'
    region_name: "RegionOne"
    interface: "public"
    identity_api_version: 3
    auth_type: "v3applicationcredential"

EOF



# -----------------------------------------------------
# Create our project config file.
#[user@desktop]

    cat > "${HOME:?}/aglais.env" << 'EOF'

AGLAIS_REPO='git@github.com:stvoutsin/aglais.git'
AGLAIS_HOME="${PROJECTS_ROOT:?}/aglais"
AGLAIS_CODE="${AGLAIS_HOME:?}"
AGLAIS_CLOUD=gaia-test
AGLAIS_USER=stv

EOF




# -----------------------------------------------------
# Edit hosts.yml file 
#[user@desktop]

  source "${HOME}/aglais.settings"
  nano ${AGLAIS_CODE:?}/experiments/zrq/ansible/hosts.yml
	..	
	keypair: ''
	...


# -----------------------------------------------------
# Create a container to work with.
# https://podman.readthedocs.io/en/latest/markdown/podman-run.1.html
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock" \
        --env "clouduser=${AGLAIS_USER:?}" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml" \
        --env "ANSIBLE_CODE=/mnt/ansible" \
        --volume "${AGLAIS_CODE:?}/experiments/zrq/ansible:/mnt/ansible" \
        atolmis/ansible-client:latest \
        bash

	# Success



# -----------------------------------------------------
# Create our Ansible include vars file.
#[root@ansibler]

    cat > /tmp/ansible-vars.yml << EOF
buildtag:  'aglais-$(date '+%Y%m%d')'
cloudname: '${cloudname}'
clouduser: '${clouduser}'
EOF


# -----------------------------------------------------
# Run the scripts from the ansible directory.
#[root@ansibler]

    cd "${ANSIBLE_CODE:?}"



# -----------------------------------------------------
# Run the initial part of our deployment.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-01.yml"

PLAY RECAP **************************************************************************************************************************************************************************************************
gateway                    : ok=8    changed=6    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
localhost                  : ok=32   changed=22   unreachable=0    failed=0    skipped=2    rescued=0    ignored=0   
master01                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
master02                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker01                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker02                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
zeppelin                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0  


# -----------------------------------------------------
# Run the Hadoop part of our deployment.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-02.yml"

PLAY RECAP **************************************************************************************************************************************************************************************************
localhost                  : ok=24   changed=21   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
master01                   : ok=24   changed=18   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
master02                   : ok=19   changed=13   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker01                   : ok=22   changed=19   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker02                   : ok=22   changed=19   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
zeppelin                   : ok=23   changed=22   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   


# -----------------------------------------------------
# Format the HDFS NameNode on master01.
#[root@ansibler]

    ssh master01 \
        '
        hdfs namenode -format
        '

2020-09-14 20:25:43,976 INFO util.GSet: Computing capacity for map cachedBlocks
2020-09-14 20:25:43,976 INFO util.GSet: VM type       = 64-bit
2020-09-14 20:25:43,977 INFO util.GSet: 0.25% max memory 4.8 GB = 12.3 MB
2020-09-14 20:25:43,977 INFO util.GSet: capacity      = 2^21 = 2097152 entries
2020-09-14 20:25:43,984 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-09-14 20:25:43,984 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-09-14 20:25:43,984 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-09-14 20:25:43,987 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
2020-09-14 20:25:43,987 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-09-14 20:25:43,989 INFO util.GSet: Computing capacity for map NameNodeRetryCache
2020-09-14 20:25:43,989 INFO util.GSet: VM type       = 64-bit
2020-09-14 20:25:43,989 INFO util.GSet: 0.029999999329447746% max memory 4.8 GB = 1.5 MB
2020-09-14 20:25:43,989 INFO util.GSet: capacity      = 2^18 = 262144 entries
2020-09-14 20:25:44,015 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1773428827-10.10.2.185-1600115144009
2020-09-14 20:25:44,060 INFO common.Storage: Storage directory /var/local/hadoop/namenode/fsimage has been successfully formatted.
2020-09-14 20:25:44,084 INFO namenode.FSImageFormatProtobuf: Saving image file /var/local/hadoop/namenode/fsimage/current/fsimage.ckpt_0000000000000000000 using no compression
2020-09-14 20:25:44,170 INFO namenode.FSImageFormatProtobuf: Image file /var/local/hadoop/namenode/fsimage/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved in 0 seconds .
2020-09-14 20:25:44,179 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
2020-09-14 20:25:44,183 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
2020-09-14 20:25:44,184 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at master01/10.10.2.185
************************************************************/




# -----------------------------------------------------
# Start the HDFS services.
#[root@ansibler]

    ssh master01 \
        '
        start-all.sh
	'	

WARNING: Attempting to start all Apache Hadoop daemons as fedora in 10 seconds.
WARNING: This is not a recommended production deployment configuration.
WARNING: Use CTRL-C to abort.
Starting namenodes on [master01]
Starting datanodes
Starting secondary namenodes [aglais-20200914-master01.novalocal]
aglais-20200914-master01.novalocal: Warning: Permanently added 'aglais-20200914-master01.novalocal,fe80::f816:3eff:fe74:6272%eth0' (ECDSA) to the list of known hosts.
Starting resourcemanager
Starting nodemanagers




# -----------------------------------------------------
# Check the HDFS status.
#[root@ansibler]

    ssh master01 \
        '
        hdfs dfsadmin -report
	'


Configured Capacity: 1099511627776 (1 TB)
Present Capacity: 1095180492800 (1019.97 GB)
DFS Remaining: 1095180484608 (1019.97 GB)
DFS Used: 8192 (8 KB)
DFS Used%: 0.00%
Replicated Blocks:
	Under replicated blocks: 0
	Blocks with corrupt replicas: 0
	Missing blocks: 0
	Missing blocks (with replication factor 1): 0
	Low redundancy blocks with highest priority to recover: 0
	Pending deletion blocks: 0
Erasure Coded Block Groups: 
	Low redundancy block groups: 0
	Block groups with corrupt internal blocks: 0
	Missing block groups: 0
	Low redundancy blocks with highest priority to recover: 0
	Pending deletion blocks: 0

-------------------------------------------------
Live datanodes (2):

Name: 10.10.1.84:9866 (worker02)
Hostname: worker02
Decommission Status : Normal
Configured Capacity: 549755813888 (512 GB)
DFS Used: 4096 (4 KB)
Non DFS Used: 17297408 (16.50 MB)
DFS Remaining: 547590242304 (509.98 GB)
DFS Used%: 0.00%
DFS Remaining%: 99.61%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Mon Sep 14 20:26:38 UTC 2020
Last Block Report: Mon Sep 14 20:26:11 UTC 2020
Num of Blocks: 0


Name: 10.10.3.160:9866 (worker01)
Hostname: worker01
Decommission Status : Normal
Configured Capacity: 549755813888 (512 GB)
DFS Used: 4096 (4 KB)
Non DFS Used: 17297408 (16.50 MB)
DFS Remaining: 547590242304 (509.98 GB)
DFS Used%: 0.00%
DFS Remaining%: 99.61%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Mon Sep 14 20:26:38 UTC 2020
Last Block Report: Mon Sep 14 20:26:11 UTC 2020
Num of Blocks: 0




# -----------------------------------------------------
# Install the Spark binaries.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "20-install-spark.yml"


PLAY RECAP **************************************************************************************************************************************************************************************************
master01                   : ok=3    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
zeppelin                   : ok=3    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   



# -----------------------------------------------------
# Add the security rules for Spark.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "21-config-spark-security.yml"

      

PLAY RECAP **************************************************************************************************************************************************************************************************
localhost                  : ok=6    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   




# -----------------------------------------------------
# Create our Spark configuration.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "22-config-spark-master.yml"

	
PLAY RECAP **************************************************************************************************************************************************************************************************
gateway                    : ok=2    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
master01                   : ok=2    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   



# -----------------------------------------------------
# Create our HDFS log directory.
#[root@ansibler]

    ssh master01 \
        '
        hdfs dfs -mkdir /spark-log
        '


# -----------------------------------------------------
# Run the SparkPi example from the Spark install instructtions.
# https://spark.apache.org/docs/3.0.0-preview2/running-on-yarn.html#launching-spark-on-yarn
#[root@ansibler]

    ssh master01 \
        '
        cd "${SPARK_HOME:?}"

        spark-submit \
            --class org.apache.spark.examples.SparkPi \
            --master yarn \
            --deploy-mode cluster \
            --driver-memory 1g \
            --executor-memory 1g \
            --executor-cores 1 \
            examples/jars/spark-examples*.jar \
                10
        '


       > ..
 
2020-09-15 08:57:42,674 INFO yarn.Client: Application report for application_1600115178561_0001 (state: RUNNING)
2020-09-15 08:57:43,677 INFO yarn.Client: Application report for application_1600115178561_0001 (state: RUNNING)
2020-09-15 08:57:44,680 INFO yarn.Client: Application report for application_1600115178561_0001 (state: RUNNING)
2020-09-15 08:57:45,682 INFO yarn.Client: Application report for application_1600115178561_0001 (state: RUNNING)
2020-09-15 08:57:46,685 INFO yarn.Client: Application report for application_1600115178561_0001 (state: RUNNING)
2020-09-15 08:57:47,687 INFO yarn.Client: Application report for application_1600115178561_0001 (state: RUNNING)
2020-09-15 08:57:48,692 INFO yarn.Client: Application report for application_1600115178561_0001 (state: RUNNING)
2020-09-15 08:57:49,694 INFO yarn.Client: Application report for application_1600115178561_0001 (state: FINISHED)
2020-09-15 08:57:49,694 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: worker01
	 ApplicationMaster RPC port: 36719
	 queue: default
	 start time: 1600160253527
	 final status: SUCCEEDED
	 tracking URL: http://master01:8088/proxy/application_1600115178561_0001/
	 user: fedora
2020-09-15 08:57:49,705 INFO util.ShutdownHookManager: Shutdown hook called
2020-09-15 08:57:49,706 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f4370a1e-c1ae-41d3-acb7-d5c45abe8520
2020-09-15 08:57:49,709 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-735eb9e5-b6a6-45a1-b2d0-b2f332f039a2





# -----------------------------------------------------
# Run the Zeppelin install.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-04.yml"
	

PLAY RECAP **************************************************************************************************************************************************************************************************
localhost                  : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
master01                   : ok=7    changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
master02                   : ok=7    changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker01                   : ok=7    changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker02                   : ok=7    changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
zeppelin                   : ok=12   changed=11   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   


# -----------------------------------------------------
# Fix Permissions
 #[root@ansibler]

    ssh zeppelin \
        '
        chown -R fedora:fedora /home/fedora/zeppelin-0.8.2-bin-all/
        '


# -----------------------------------------------------
# Start the YARN services.
#[root@ansibler]

    ssh zeppelin \
        '
        /home/fedora/zeppelin-0.8.2-bin-all/bin/zeppelin-daemon.sh start
        '
Pid dir doesn't exist, create /home/fedora/zeppelin-0.8.2-bin-all/run
Zeppelin start                                             [  OK  ]



# -----------------------------------------------------
# Try some Spark jobs via the Zeppelin GUI.
# http://128.232.227.203:8080/#/notebook/2FMUJCXDZ


%spark.pyspark
x = 1


# First time running, we get an exception:
java.lang.RuntimeException: SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/zeppelin-0.8.2-bin-all/interpreter/spark/spark-interpreter-0.8.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/zeppelin-0.8.2-bin-all/lib/interpreter/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/spark-3.0.1-bin-hadoop3.2/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Exception in thread "main" org.apache.spark.SparkException: Master must either be yarn or start with spark, mesos, k8s, or local
	at org.apache.spark.deploy.SparkSubmit.error(SparkSubmit.scala:936)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:238)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)



# Second time running task we see:

java.lang.NullPointerException
	at org.apache.thrift.transport.TSocket.open(TSocket.java:170)
	at org.apache.zeppelin.interpreter.remote.ClientFactory.create(ClientFactory.java:51)
	at org.apache.zeppelin.interpreter.remote.ClientFactory.create(ClientFactory.java:37)
	at org.apache.commons.pool2.BasePooledObjectFactory.makeObject(BasePooledObjectFactory.java:60)
	at org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:861)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:435)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:363)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.getClient(RemoteInterpreterProcess.java:62)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.callRemoteFunction(RemoteInterpreterProcess.java:133)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.internal_create(RemoteInterpreter.java:165)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:132)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:299)
	at org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:408)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:188)
	at org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:315)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)



# 


# -----------------------------------------------------
# In Zeppelin GUI change Spark interpreter settings
# Set master = yarn
# Set spark.submit.deployMode = cluster


# Run Spark cell again


020-09-15 09:39:58,641 INFO conf.Configuration: resource-types.xml not found
2020-09-15 09:39:58,642 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2020-09-15 09:39:58,661 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
2020-09-15 09:39:58,662 INFO yarn.Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead
2020-09-15 09:39:58,662 INFO yarn.Client: Setting up container launch context for our AM
2020-09-15 09:39:58,669 INFO yarn.Client: Setting up the launch environment for our AM container
2020-09-15 09:39:58,688 INFO yarn.Client: Preparing resources for our AM container
2020-09-15 09:40:18,739 INFO ipc.Client: Retrying connect to server: master01/10.10.2.185:9000. Already tried 0 time(s); maxRetries=45
2020-09-15 09:40:38,748 INFO ipc.Client: Retrying connect to server: master01/10.10.2.185:9000. Already tried 1 time(s); maxRetries=45

	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterManagedProcess.start(RemoteInterpreterManagedProcess.java:205)
	at org.apache.zeppelin.interpreter.ManagedInterpreterGroup.getOrCreateInterpreterProcess(ManagedInterpreterGroup.java:64)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getOrCreateInterpreterProcess(RemoteInterpreter.java:111)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.internal_create(RemoteInterpreter.java:164)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:132)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:299)
	at org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:408)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:188)
	at org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:315)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

# Error connecting to Master,
# Looks like we have to fix the Port settings

# Fix permissions in Openstack Security Groups
# Allow all incoming on workers and masters
# TODO: Fix in Ansible scirpts with appropriate ports


# Try Zeppelin cell again..

java.lang.RuntimeException: 2020-09-15 10:04:42,232 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-09-15 10:04:42,552 INFO client.RMProxy: Connecting to ResourceManager at master01/10.10.2.185:8032
2020-09-15 10:04:42,803 INFO yarn.Client: Requesting a new application from cluster with 2 NodeManagers
2020-09-15 10:04:43,210 INFO conf.Configuration: resource-types.xml not found
2020-09-15 10:04:43,210 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2020-09-15 10:04:43,227 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
2020-09-15 10:04:43,228 INFO yarn.Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead
2020-09-15 10:04:43,228 INFO yarn.Client: Setting up container launch context for our AM
2020-09-15 10:04:43,235 INFO yarn.Client: Setting up the launch environment for our AM container
2020-09-15 10:04:43,253 INFO yarn.Client: Preparing resources for our AM container
2020-09-15 10:04:43,404 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
2020-09-15 10:04:45,496 INFO yarn.Client: Uploading resource file:/tmp/spark-58b1a0d6-d05f-4e6b-8854-34d66b736534/__spark_libs__7317748828345994596.zip -> hdfs://master01:9000/user/fedora/.sparkStaging/application_1600164241047_0001/__spark_libs__7317748828345994596.zip

	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterManagedProcess.start(RemoteInterpreterManagedProcess.java:205)
	at org.apache.zeppelin.interpreter.ManagedInterpreterGroup.getOrCreateInterpreterProcess(ManagedInterpreterGroup.java:64)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getOrCreateInterpreterProcess(RemoteInterpreter.java:111)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.internal_create(RemoteInterpreter.java:164)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:132)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:299)
	at org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:408)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:188)
	at org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:315)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)



# No luck
# Check Hadoop GUI

ssh -L '8088:master01:8088' test-gateway

  http://localhost:8088



Our failed Jobs shows up as:
  State: ACCEPTED
  FinalStatus: UNDEFINED

# Change Spark conf settings to:

# BEGIN Ansible managed Spark configuration
# https://spark.apache.org/docs/3.0.0-preview2/running-on-yarn.html#spark-properties
spark.master            yarn
spark.eventLog.enabled  true
spark.eventLog.dir      hdfs://master01:9000/spark-log
spark.driver.memory    5g
spark.yarn.am.memory    5g
spark.executor.memory   5g
# END Ansible managed Spark configuration
# BEGIN Ansible managed Spark environment
# https://spark.apache.org/docs/3.0.0-preview2/configuration.html#inheriting-hadoop-cluster-configuration
spark.yarn.appMasterEnv.YARN_CONF_DIR=/opt/hadoop/etc/hadoop
spark.yarn.appMasterEnv.HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
# END Ansible managed Spark environment




# Restart Zeppelin, Haddop and try again


java.lang.RuntimeException: 2020-09-15 11:00:52,913 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-09-15 11:00:53,257 INFO client.RMProxy: Connecting to ResourceManager at master01/10.10.2.185:8032
2020-09-15 11:00:53,490 INFO yarn.Client: Requesting a new application from cluster with 2 NodeManagers
2020-09-15 11:00:53,859 INFO conf.Configuration: resource-types.xml not found
2020-09-15 11:00:53,860 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2020-09-15 11:00:53,876 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
2020-09-15 11:00:53,877 INFO yarn.Client: Will allocate AM container, with 5632 MB memory including 512 MB overhead
2020-09-15 11:00:53,877 INFO yarn.Client: Setting up container launch context for our AM
2020-09-15 11:00:53,882 INFO yarn.Client: Setting up the launch environment for our AM container
2020-09-15 11:00:53,891 INFO yarn.Client: Preparing resources for our AM container
2020-09-15 11:00:53,923 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
2020-09-15 11:00:56,066 INFO yarn.Client: Uploading resource file:/tmp/spark-140c6c52-ebc8-4363-b484-47ff4aa490b4/__spark_libs__2483860225930853582.zip -> hdfs://master01:9000/user/fedora/.sparkStaging/application_1600166552242_0003/__spark_libs__2483860225930853582.zip
2020-09-15 11:00:57,065 INFO yarn.Client: Uploading resource file:/home/fedora/zeppelin-0.8.2-bin-all/interpreter/spark/spark-interpreter-0.8.2.jar -> hdfs://master01:9000/user/fedora/.sparkStaging/application_1600166552242_0003/spark-interpreter-0.8.2.jar
2020-09-15 11:00:57,204 INFO yarn.Client: Uploading resource file:/home/fedora/zeppelin-0.8.2-bin-all/conf/log4j_yarn_cluster.properties -> hdfs://master01:9000/user/fedora/.sparkStaging/application_1600166552242_0003/log4j_yarn_cluster.properties
2020-09-15 11:00:57,235 INFO yarn.Client: Uploading resource file:/opt/spark/R/lib/sparkr.zip#sparkr -> hdfs://master01:9000/user/fedora/.sparkStaging/application_1600166552242_0003/sparkr.zip
2020-09-15 11:00:57,280 INFO yarn.Client: Uploading resource file:/opt/spark/python/lib/pyspark.zip -> hdfs://master01:9000/user/fedora/.sparkStaging/application_1600166552242_0003/pyspark.zip
2020-09-15 11:00:57,311 INFO yarn.Client: Uploading resource file:/opt/spark/python/lib/py4j-0.10.9-src.zip -> hdfs://master01:9000/user/fedora/.sparkStaging/application_1600166552242_0003/py4j-0.10.9-src.zip
2020-09-15 11:00:57,441 INFO yarn.Client: Uploading resource file:/tmp/spark-140c6c52-ebc8-4363-b484-47ff4aa490b4/__spark_conf__7591663273839642286.zip -> hdfs://master01:9000/user/fedora/.sparkStaging/application_1600166552242_0003/__spark_conf__.zip
2020-09-15 11:00:57,504 INFO spark.SecurityManager: Changing view acls to: fedora
2020-09-15 11:00:57,505 INFO spark.SecurityManager: Changing modify acls to: fedora
2020-09-15 11:00:57,505 INFO spark.SecurityManager: Changing view acls groups to: 
2020-09-15 11:00:57,506 INFO spark.SecurityManager: Changing modify acls groups to: 
2020-09-15 11:00:57,507 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fedora); groups with view permissions: Set(); users  with modify permissions: Set(fedora); groups with modify permissions: Set()
2020-09-15 11:00:57,554 INFO yarn.Client: Submitting application application_1600166552242_0003 to ResourceManager
2020-09-15 11:00:57,589 INFO impl.YarnClientImpl: Submitted application application_1600166552242_0003
2020-09-15 11:00:58,592 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:00:58,595 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1600167657567
	 final status: UNDEFINED
	 tracking URL: http://master01:8088/proxy/application_1600166552242_0003/
	 user: fedora
2020-09-15 11:00:59,598 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:00,600 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:01,603 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:02,605 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:03,607 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:04,610 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:05,612 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:06,615 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:07,618 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:08,620 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:09,623 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:10,625 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:11,632 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:12,636 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:13,638 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:14,641 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:15,643 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:16,645 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:17,648 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:18,650 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:19,653 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:20,659 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:21,662 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:22,665 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:23,667 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:24,670 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:25,673 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:26,676 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:27,679 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:28,681 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:29,683 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:30,686 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:31,688 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:32,691 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:33,693 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:34,696 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:35,699 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:36,701 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:37,704 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:38,707 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:39,709 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:40,712 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:41,715 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:42,718 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:43,720 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:44,722 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:45,725 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:46,728 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:47,730 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:48,733 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:49,735 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)
2020-09-15 11:01:50,737 INFO yarn.Client: Application report for application_1600166552242_0003 (state: ACCEPTED)

	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterManagedProcess.start(RemoteInterpreterManagedProcess.java:205)
	at org.apache.zeppelin.interpreter.ManagedInterpreterGroup.getOrCreateInterpreterProcess(ManagedInterpreterGroup.java:64)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getOrCreateInterpreterProcess(RemoteInterpreter.java:111)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.internal_create(RemoteInterpreter.java:164)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:132)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:299)
	at org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:408)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:188)
	at org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:315)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)



# Looking through the logs, two warning stand out:

# 2020-09-15 11:13:17,506 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.

# Probably not, because same warning appears when running examples using spark-submit, but the examples work.




# Looking at the Master logs:

WARN org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: maximum-am-resource-percent is insufficient to start a single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start



# Looking at the worker logs:

2020-09-15 12:34:48,254 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 1002050152, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
2020-09-15 12:36:06,245 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1600171489715_0004_01_000001 is : 13
2020-09-15 12:36:06,245 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exception from container-launch with container ID: container_1600171489715_0004_01_000001 and exit code: 13
ExitCodeException exitCode=13: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:1008)
	at org.apache.hadoop.util.Shell.run(Shell.java:901)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:294)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.launchContainer(ContainerLaunch.java:491)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:101)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

