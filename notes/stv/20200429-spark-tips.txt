#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#



# ---------------------------#
#         Spark Tips         #
# ---------------------------#


# The following are some notes on using Spark, as found in: 
# https://campus.datacamp.com/courses/introduction-to-pyspark/getting-to-know-pyspark?ex=10




# Spark Context & Spark Session
# -----------------------------

SparkContext is the connection to the cluster
SparkSession is the interface with that connection.



# Create Spark Sessions

# Import SparkSession from pyspark.sql
from pyspark.sql import SparkSession

# Create my_spark
my_spark = SparkSession.builder.getOrCreate()

# Print my_spark
print(my_spark)



# Print the tables in the catalog
print(spark.catalog.listTables())

 > [Table(name='flights', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]



# Spark SQL Query
# ------------------------------
query = "FROM flights SELECT * LIMIT 10"

# Get the first 10 rows of flights
flights10 = spark.sql(query)

# Show the results
flights10.show()



# Dataframe to Pandas
# ------------------------------
query = "SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest"

# Run the query
flight_counts = spark.sql(query)

# Convert the results to a pandas DataFrame
pd_counts = flight_counts.toPandas()

# Print the head of pd_counts
print(pd_counts.head())




# Creating Dataframe copies, Creating Temp Views
# ----------------------------------------------


# Create pd_temp
pd_temp = pd.DataFrame(np.random.random(10))

# Create spark_temp from pd_temp
spark_temp = spark.createDataFrame(pd_temp)

# Examine the tables in the catalog
print(spark.catalog.listTables())

# Add spark_temp to the catalog
spark_temp.createOrReplaceTempView("temp")

# Examine the tables in the catalog again
print(spark.catalog.listTables())



# Read a CSV file into a Dataframe
# ----------------------------------------------

file_path = "/usr/local/share/datasets/airports.csv"

# Read in the airports data
airports = spark.read.csv(file_path, header=True)

# Show the data
airports.show()



# Dealing with Dataframes
# ----------------------------------------------

# Dataframes are immutable

# Create a new Dataframe based on an existing one, with a column altered
# df = df.withColumn("newCol", df.oldCol + 1)


# Using Filter:
# The following is the equivalent of using "WHERE air_time > 120" in SQL
flights.filter("air_time > 120").show()

 
# Using select
# Similarly  flights.select(..) is the same as a "SELECT" in SQL

# We can also use aliases:
flights.selectExpr("air_time/60 as duration_hrs") 
# or
flights.select((flights.air_time/60).alias("duration_hrs"))


# GroupBy:
df.groupBy().min("col").show()
# This returns a GroupedData object, the result is a DataFrame

# Another couple examples
flights.filter(___).filter(___).groupBy().avg(___).show()
flights.withColumn("duration_hrs", flights.air_time/60).groupBy().sum(___).show()
by_origin = flights.groupBy("origin")


# In addition to the GroupedData methods there is also the .agg() method. 
# This method lets us pass an aggregate column expression that uses any of the aggregate functions from the pyspark.sql.functions submodule.
# This submodule contains many useful functions for computing things like standard deviations. All the aggregation functions in this submodule take the name of a column in a GroupedData table.


# We can also do joins, similar to SQL joins as:
# df1.join(df2, df1("col1") === df2("col1"), "left_outer")

























