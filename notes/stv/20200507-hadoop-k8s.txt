#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2019, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#



# ----------------------------------------
# Deploying HDFS on a Kubernetes cluster
# ----------------------------------------

# Deployment on existing Kubernetes Cluster, with Spark installed on it

4 Worker Nodes
2 Master Nodes

# Follow Instructions here:
https://hub.kubeapps.com/charts/stable/hadoop
Original K8S Hadoop adaptation this chart was derived from: https://github.com/Comcast/kube-yarn




# ---------------------------------------------
# Create 100gb Volumes, and attach to each node
# ---------------------------------------------

For the first deployment, this was done using the GUI	


# ---------------------------------------------
# Install Helm
# ---------------------------------------------
# user@master

wget https://get.helm.sh/helm-v2.16.7-linux-amd64.tar.gz
sudo mv linux-amd64/helm /usr/local/bin/helm

# Init Helm
helm init


# Setup Tiller account

kubectl create serviceaccount --namespace kube-system tiller

kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller

kubectl patch deploy --namespace kube-system tiller-deploy -p '{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}'




# Follow instructions to install Hadoop Helm Chart
# https://hub.kubeapps.com/charts/stable/hadoop

# ---------------------------------------------
# Add Chart repo
# ---------------------------------------------
helm repo add stable https://kubernetes-charts.storage.googleapis.com



# ---------------------------------------------
# Install Hadoop chart
# ---------------------------------------------
helm install --name hadoop --namespace hadoop stable/hadoop 

NAME:   hadoop
LAST DEPLOYED: Thu May  7 21:46:49 2020
NAMESPACE: hadoop
STATUS: DEPLOYED

RESOURCES:
==> v1/ConfigMap
NAME           DATA  AGE
hadoop-hadoop  8     0s

==> v1/Pod(related)
NAME                     READY  STATUS             RESTARTS  AGE
hadoop-hadoop-hdfs-dn-0  0/1    ContainerCreating  0         0s
hadoop-hadoop-hdfs-nn-0  0/1    ContainerCreating  0         0s
hadoop-hadoop-yarn-nm-0  0/1    ContainerCreating  0         1s
hadoop-hadoop-yarn-rm-0  0/1    ContainerCreating  0         0s

==> v1/Service
NAME                   TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)                     AGE
hadoop-hadoop-hdfs-dn  ClusterIP  None           <none>       9000/TCP,50075/TCP          0s
hadoop-hadoop-hdfs-nn  ClusterIP  None           <none>       9000/TCP,50070/TCP          0s
hadoop-hadoop-yarn-nm  ClusterIP  None           <none>       8088/TCP,8082/TCP,8042/TCP  0s
hadoop-hadoop-yarn-rm  ClusterIP  None           <none>       8088/TCP                    0s
hadoop-hadoop-yarn-ui  ClusterIP  10.254.87.178  <none>       8088/TCP                    0s

==> v1/StatefulSet
NAME                   READY  AGE
hadoop-hadoop-hdfs-dn  0/1    0s
hadoop-hadoop-hdfs-nn  0/1    0s
hadoop-hadoop-yarn-nm  0/2    0s
hadoop-hadoop-yarn-rm  0/1    0s

==> v1beta1/PodDisruptionBudget
NAME                   MIN AVAILABLE  MAX UNAVAILABLE  ALLOWED DISRUPTIONS  AGE
hadoop-hadoop-hdfs-dn  1              N/A              0                    0s
hadoop-hadoop-hdfs-nn  1              N/A              0                    0s
hadoop-hadoop-yarn-nm  1              N/A              0                    0s
hadoop-hadoop-yarn-rm  1              N/A              0                    0s


NOTES:
1. You can check the status of HDFS by running this command:
   kubectl exec -n hadoop -it hadoop-hadoop-hdfs-nn-0 -- /usr/local/hadoop/bin/hdfs dfsadmin -report

2. You can list the yarn nodes by running this command:
   kubectl exec -n hadoop -it hadoop-hadoop-yarn-rm-0 -- /usr/local/hadoop/bin/yarn node -list

3. Create a port-forward to the yarn resource manager UI:
   kubectl port-forward -n hadoop hadoop-hadoop-yarn-rm-0 8088:8088

   Then open the ui in your browser:

   open http://localhost:8088

4. You can run included hadoop tests like this:
   kubectl exec -n hadoop -it hadoop-hadoop-yarn-nm-0 -- /usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.0-tests.jar TestDFSIO -write -nrFiles 5 -fileSize 128MB -resFile /tmp/TestDFSIOwrite.txt

5. You can list the mapreduce jobs like this:
   kubectl exec -n hadoop -it hadoop-hadoop-yarn-rm-0 -- /usr/local/hadoop/bin/mapred job -list

6. This chart can also be used with the zeppelin chart
    helm install --namespace hadoop --set hadoop.useConfigMap=true,hadoop.configMapName=hadoop-hadoop stable/zeppelin

7. You can scale the number of yarn nodes like this:
   helm upgrade hadoop --set yarn.nodeManager.replicas=4 stable/hadoop

   Make sure to update the values.yaml if you want to make this permanent.



# ----------------------------------------------------------
# Remove Hadoop and reinstall with  persistent volumes
# ----------------------------------------------------------


helm install --name hadoop --set persistence.nameNode.enabled=true   --set persistence.nameNode.storageClass=standard   --set persistence.dataNode.enabled=true   --set persistence.dataNode.storageClass=standard   stable/hadoop


NAME:   hadoop
E0508 12:38:33.301184   14746 portforward.go:372] error copying from remote stream to local connection: readfrom tcp4 127.0.0.1:46115->127.0.0.1:39210: write tcp4 127.0.0.1:46115->127.0.0.1:39210: write: broken pipe
LAST DEPLOYED: Fri May  8 12:38:33 2020
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==> v1/ConfigMap
NAME           DATA  AGE
hadoop-hadoop  8     0s

==> v1/PersistentVolumeClaim
NAME                   STATUS   VOLUME    CAPACITY  ACCESS MODES  STORAGECLASS  AGE
hadoop-hadoop-hdfs-dn  Pending  standard  0s
hadoop-hadoop-hdfs-nn  Pending  standard  0s

==> v1/Pod(related)
NAME                     READY  STATUS             RESTARTS  AGE
hadoop-hadoop-hdfs-dn-0  0/1    Pending            0         0s
hadoop-hadoop-hdfs-nn-0  0/1    Pending            0         0s
hadoop-hadoop-yarn-nm-0  0/1    ContainerCreating  0         0s
hadoop-hadoop-yarn-rm-0  0/1    ContainerCreating  0         0s

==> v1/Service
NAME                   TYPE       CLUSTER-IP    EXTERNAL-IP  PORT(S)                     AGE
hadoop-hadoop-hdfs-dn  ClusterIP  None          <none>       9000/TCP,50075/TCP          0s
hadoop-hadoop-hdfs-nn  ClusterIP  None          <none>       9000/TCP,50070/TCP          0s
hadoop-hadoop-yarn-nm  ClusterIP  None          <none>       8088/TCP,8082/TCP,8042/TCP  0s
hadoop-hadoop-yarn-rm  ClusterIP  None          <none>       8088/TCP                    0s
hadoop-hadoop-yarn-ui  ClusterIP  10.254.142.3  <none>       8088/TCP                    0s

==> v1/StatefulSet
NAME                   READY  AGE
hadoop-hadoop-hdfs-dn  0/1    0s
hadoop-hadoop-hdfs-nn  0/1    0s
hadoop-hadoop-yarn-nm  0/2    0s
hadoop-hadoop-yarn-rm  0/1    0s

==> v1beta1/PodDisruptionBudget
NAME                   MIN AVAILABLE  MAX UNAVAILABLE  ALLOWED DISRUPTIONS  AGE
hadoop-hadoop-hdfs-dn  1              N/A              0                    0s
hadoop-hadoop-hdfs-nn  1              N/A              0                    0s
hadoop-hadoop-yarn-nm  1              N/A              0                    0s
hadoop-hadoop-yarn-rm  1              N/A              0                    0s


NOTES:
1. You can check the status of HDFS by running this command:
   kubectl exec -n default -it hadoop-hadoop-hdfs-nn-0 -- /usr/local/hadoop/bin/hdfs dfsadmin -report

2. You can list the yarn nodes by running this command:
   kubectl exec -n default -it hadoop-hadoop-yarn-rm-0 -- /usr/local/hadoop/bin/yarn node -list

3. Create a port-forward to the yarn resource manager UI:
   kubectl port-forward -n default hadoop-hadoop-yarn-rm-0 8088:8088

   Then open the ui in your browser:

   open http://localhost:8088

4. You can run included hadoop tests like this:Original K8S Hadoop adaptation this chart was derived from: https://github.com/Comcast/kube-yarn

   kubectl exec -n default -it hadoop-hadoop-yarn-nm-0 -- /usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.9.0-tests.jar TestDFSIO -write -nrFiles 5 -fileSize 128MB -resFile /tmp/TestDFSIOwrite.txt

5. You can list the mapreduce jobs like this:
   kubectl exec -n default -it hadoop-hadoop-yarn-rm-0 -- /usr/local/hadoop/bin/mapred job -list

6. This chart can also be used with the zeppelin chart
    helm install --namespace default --set hadoop.useConfigMap=true,hadoop.configMapName=hadoop-hadoop stable/zeppelin

7. You can scale the number of yarn nodes like this:
   helm upgrade hadoop --set yarn.nodeManager.replicas=4 stable/hadoop

   Make sure to update the values.yaml if you want to make this permanent.




# Note: This also includes some information on deploying a Zeppelin Chart
#  helm install --namespace default --set hadoop.useConfigMap=true,hadoop.configMapName=hadoop-hadoop stable/zeppelin
# TODO: Try this later



# ----------------------------------------------------------
# Configurations
# ----------------------------------------------------------

# The above can be redeployed with different configuration options listed here:
# https://hub.kubeapps.com/charts/stable/hadoop

# These include:

image.repository	
    Default: danisla/hadoop


hadoopVersion
    Default: 2.9.0


hdfs.dataNode.replicas
    Default: 1

persistence.nameNode.enabled	
    Default: false


# Check the source of the danisla/hadoop Docker image:
# https://github.com/Comcast/kube-yarn/tree/master/image

# Dockerfile 
..

	FROM java:8-jre

	# Add native libs
	ARG HADOOP_VERSION=
	ADD hadoop-${HADOOP_VERSION}.tar.gz /usr/local
	ADD hadoop-native-${HADOOP_VERSION}.tar /usr/local/hadoop-${HADOOP_VERSION}/lib/native

	ENV HADOOP_PREFIX=/usr/local/hadoop \
	    HADOOP_COMMON_HOME=/usr/local/hadoop \
	    HADOOP_HDFS_HOME=/usr/local/hadoop \
	    HADOOP_MAPRED_HOME=/usr/local/hadoop \
	    HADOOP_YARN_HOME=/usr/local/hadoop \
	    HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop \
	    YARN_CONF_DIR=/usr/local/hadoop/etc/hadoop \
	    PATH=${PATH}:/usr/local/hadoop/bin

	RUN \
	  cd /usr/local && ln -s ./hadoop-${HADOOP_VERSION} hadoop && \
	  rm -f ${HADOOP_PREFIX}/logs/*

	WORKDIR $HADOOP_PREFIX

	# Hdfs ports
	EXPOSE 50010 50020 50070 50075 50090 8020 9000
	# Mapred ports
	EXPOSE 19888
	#Yarn ports
	EXPOSE 8030 8031 8032 8033 8040 8042 8088
	#Other ports
	EXPOSE 49707 2122



# ----------------------------------------------------------
# Checking what was deployed
# ----------------------------------------------------------



# Get Pods
# -------------
kubectl get pods -n default
NAME                            READY   STATUS    RESTARTS   AGE
hadoop-hadoop-hdfs-dn-0         1/1     Running   2          128m
hadoop-hadoop-hdfs-nn-0         1/1     Running   0          128m
hadoop-hadoop-yarn-nm-0         1/1     Running   0          128m
hadoop-hadoop-yarn-nm-1         1/1     Running   0          128m
hadoop-hadoop-yarn-rm-0         1/1     Running   0          128m
spark-master-controller-4jnbd   1/1     Running   0          9d
spark-worker-controller-4jvh5   1/1     Running   0          9d
spark-worker-controller-rw4pq   1/1     Running   0          9d


# There are 5 Hadoop pods:
1 data node
3 name nodes
1 resource manager node

# I assume 1 datanode cause "replicas" param defaults to one?



# Get Services
# -------------
kubectl get services     
NAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
hadoop-hadoop-hdfs-dn   ClusterIP   None             <none>        9000/TCP,50075/TCP           132m
hadoop-hadoop-hdfs-nn   ClusterIP   None             <none>        9000/TCP,50070/TCP           132m
hadoop-hadoop-yarn-nm   ClusterIP   None             <none>        8088/TCP,8082/TCP,8042/TCP   132m
hadoop-hadoop-yarn-rm   ClusterIP   None             <none>        8088/TCP                     132m
hadoop-hadoop-yarn-ui   ClusterIP   10.254.142.3     <none>        8088/TCP                     132m
kubernetes              ClusterIP   10.254.0.1       <none>        443/TCP                      10d
spark-master            ClusterIP   10.254.252.249   <none>        7077/TCP,8080/TCP            9d
spark-master-headless   ClusterIP   None             <none>        <none>                       9d


# Get Persistent Storage
# ----------------------
kubectl get pv --sort-by=.spec.capacity.storage
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                           STORAGECLASS   REASON   AGE
pvc-e62eef27-6a06-447d-867f-13407c6e6fa7   50Gi       RWO            Delete           Bound    default/hadoop-hadoop-hdfs-nn   standard                139m
pvc-b995c154-ddd0-4fdb-a48a-0a25b75a5f06   200Gi      RWO            Delete           Bound    default/hadoop-hadoop-hdfs-dn   standard                140m






# ----------------------------------------------------------
# Put some example files into HDFS
# ----------------------------------------------------------
# Exec into a NameNode pod, and run some HDFS commands
#
#

kubectl exec -it -n default hadoop-hadoop-hdfs-nn-0 bash

cd ~
hdfs dfs -mkdir /hadoop/
hdfs dfs -mkdir /hadoop/books/

wget -O alice.txt https://www.gutenberg.org/files/11/11-0.txt
wget -O holmes.txt https://www.gutenberg.org/files/1661/1661-0.txt
wget -O frankenstein.txt https://www.gutenberg.org/files/84/84-0.txt

hdfs dfs -put alice.txt holmes.txt frankenstein.txt /hadoop/books
hdfs dfs -ls /hadoop/books
Found 3 items
-rw-r--r--   3 root supergroup     174481 2020-05-08 15:26 /hadoop/books/alice.txt
-rw-r--r--   3 root supergroup     450783 2020-05-08 15:26 /hadoop/books/frankenstein.txt
-rw-r--r--   3 root supergroup     607788 2020-05-08 15:26 /hadoop/books/holmes.txt


# Looks like HDFS commands work successfully. How else can we validate what is going on behind the scenes?
# Can we see these files from another hadoop node, on a different VM (K8s noed)??
# The above pod (hadoop-hadoop-hdfs-nn-0) was on k8s-test-1-d47yw44xh3fn-node-1, let's try and list the HDFS directory we just created from another node, which has a different HDFS pod running


# Pod hadoop-hadoop-yarn-nm-0 is running on k8s-test-1-d47yw44xh3fn-node-3, let's check that

kubectl exec -it -n default hadoop-hadoop-yarn-nm-0 bash
hdfs dfs -ls /hadoop/books
Found 3 items
-rw-r--r--   3 root supergroup     174481 2020-05-08 15:26 /hadoop/books/alice.txt
-rw-r--r--   3 root supergroup     450783 2020-05-08 15:26 /hadoop/books/frankenstein.txt
-rw-r--r--   3 root supergroup     607788 2020-05-08 15:26 /hadoop/books/holmes.txt


# Looks like we have a working HDFS system


# ----------------------------------------------------------
# Check the Yarn GUI
# ----------------------------------------------------------

From local tunnel connection to the yarn ui service (10.254.142.3:8088)
ssh -L '8088:10.254.142.3:8088' k8s-test-1-d47yw44xh3fn-master-1

user@local:~$ firefox http://localhost:8088/cluster

# .. Shows 2 active Nodes




