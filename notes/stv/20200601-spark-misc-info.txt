#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#


# Client vs Cluster Mode
# ----------------------

In the Spark interpreter, (also in zeppelin-env.sh conf) we can set the Spark "master" parameter:

    master  yarn-client / yarn-cluster
  
In Client mode, the Spark Driver is launched as part of the client process.
So in the current prototype, since we are using the "client" mode, the driver is spawned in the Zeppeln python process, and Spark uses the values defined in:

spark.local.dir         /home/fedora/spark/local

to store temporary metadata and data. 

If we switch master: yarn-cluster, the Driver would no longer run on the Zeppelin node, potentially putting less stress on it.



# Spark Configurations
# ---------------------

(spark-defaults.conf)
spark.cleaner.ttl	(infinite)	

Duration (seconds) of how long Spark will remember any metadata (stages generated, tasks generated, etc.). Periodic cleanups will ensure that metadata older than this duration will be forgotten. This is useful for running Spark for many hours / days (for example, running 24/7 in case of Spark Streaming applications). Note that any RDD that persists in memory for more than this duration will be cleared as well.



# Enabling Dynamic Resource Allocation
# -------------------------------------

Set following param in the spark configuration (spark-defaults.conf)
spark.dynamicAllocation.enabled  true

https://spark.apache.org/docs/latest/running-on-yarn.html#configuring-the-external-shuffle-service
"Locate the spark-<version>-yarn-shuffle.jar. This should be under $SPARK_HOME/common/network-yarn/target/scala-<version> if you are building Spark yourself, and under yarn if you are using a distribution.
Add this jar to the classpath of all NodeManagers in your cluster.
In the yarn-site.xml on each node, add spark_shuffle to yarn.nodemanager.aux-services, then set yarn.nodemanager.aux-services.spark_shuffle.class to org.apache.spark.network.yarn.YarnShuffleService.
Increase NodeManager's heap size by setting YARN_HEAPSIZE (1000 by default) in etc/hadoop/yarn-env.sh to avoid garbage collection issues during shuffle.
Restart all NodeManagers in your cluster."


# Debugging Query plans
# ----------------------

We can get a description of an RDD and its history using the "toDebugString()" method on an RDD
We can also check the query plan through the Dataframe API using the "explain()" method.



# Other Notes 
# ---------------------

reduceByKey, groupByKey, cogroup, repartition and join all lead to shuffles





