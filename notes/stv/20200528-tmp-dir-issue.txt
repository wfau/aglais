


## Try running a few of the existing notebooks, which we're previously run (about 1 month ago)
## ------------------------------------------------------------------------------------------------

%spark.pyspark

# define the data frame source on the given column selection/predicates:
df = sqlContext.read.parquet(
    "/hadoop/gaia/parquet/gdr2/gaia_source/*.parquet"
    ).select(
    ["designation","source_id","ra","ra_error","dec","dec_error","parallax","parallax_error","parallax_over_error","pmra","pmra_error","pmdec","pmdec_error","l","b"]
    ).where(
    "abs(b) < 30.0 AND parallax > 1.0 and parallax_over_error > 10.0 AND phot_g_mean_flux_over_error > 36.19 AND astrometric_sigma5d_max < 0.3 AND visibility_periods_used > 8 AND (astrometric_excess_noise < 1 OR (astrometric_excess_noise > 1 AND astrometric_excess_noise_sig < 2))"
    )

# sanity check
df.show()
print ("Data frame rows: ",df.count())



Py4JJavaError: An error occurred while calling o291.parquet.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.io.IOException: Failed to create local dir in /tmp/spark-temp/blockmgr-9f7f5a7d-a832-4be7-b1d7-08fe21e7a4da/02.
java.io.IOException: Failed to create local dir in /tmp/spark-temp/blockmgr-9f7f5a7d-a832-4be7-b1d7-08fe21e7a4da/02.
	at org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:70)
	at org.apache.spark.storage.DiskStore.remove(DiskStore.scala:117)
	at org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:1588)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1118)
	at org.apache.spark.storage.BlockManager.doPutBytes(BlockManager.scala:979)
	at org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:955)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$writeBlocks$1.apply(TorrentBroadcast.scala:137)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$writeBlocks$1.apply(TorrentBroadcast.scala:131)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:131)
	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:88)
	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)
	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1489)
	at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1161)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:1069)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1013)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2067)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1171)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:1069)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1013)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2067)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:237)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:126)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:91)
	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:67)
	at org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$createInMemoryFileIndex(DataSource.scala:533)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:371)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:644)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to create local dir in /tmp/spark-temp/blockmgr-9f7f5a7d-a832-4be7-b1d7-08fe21e7a4da/02.
	at org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:70)
	at org.apache.spark.storage.DiskStore.remove(DiskStore.scala:117)
	at org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:1588)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1118)
	at org.apache.spark.storage.BlockManager.doPutBytes(BlockManager.scala:979)
	at org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:955)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$writeBlocks$1.apply(TorrentBroadcast.scala:137)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$writeBlocks$1.apply(TorrentBroadcast.scala:131)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:131)
	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:88)
	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)
	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1489)
	at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1161)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:1069)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1013)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2067)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling o291.parquet.\n', JavaObject id=o294), <traceback object at 0x7f9151a99780>)





## Try another example
## ------------------------------
%spark.pyspark

NUM_SAMPLES = 1000000000

def inside(p):
    x, y = random.random(), random.random()
    return x*x + y*y < 1

count = sc.parallelize(xrange(0, NUM_SAMPLES)) \
             .filter(inside).count()
print "Pi is roughly %f" % (4.0 * count / NUM_SAMPLES)

Fail to execute line 7: count = sc.parallelize(xrange(0, NUM_SAMPLES)) \
Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-237895005518745498.py", line 375, in <module>
  File "<stdin>", line 7, in <module>
  File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/context.py", line 513, in parallelize
    return self.parallelize([], numSlices).mapPartitionsWithIndex(f)
  File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/context.py", line 527, in parallelize
    jrdd = self._serialize_to_jvm(c, serializer, reader_func, createRDDServer)
  File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/context.py", line 556, in _serialize_to_jvm
    tempFile = NamedTemporaryFile(delete=False, dir=self._temp_dir)
  File "/usr/lib64/python2.7/tempfile.py", line 475, in NamedTemporaryFile
    (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags)
  File "/usr/lib64/python2.7/tempfile.py", line 244, in _mkstemp_inner
    fd = _os.open(file, flags, 0600)
OSError: [Errno 2] No such file or directory: '/tmp/spark-temp/spark-f8efede5-5ffe-4b5d-a916-3197bd1f1232/pyspark-4a1de947-7798-4aae-9d81-91baa6ee2a1f/tmpoeV_tI'




## Try creating a new notebook and running
## ----------------------------------------

%spark.pyspark

NUM_SAMPLES = 2000000000

def inside(p):
    x, y = random.random(), random.random()
    return x*x + y*y < 1

count = sc.parallelize(xrange(0, NUM_SAMPLES)) \
             .filter(inside).count()
print "Pi is roughly %f" % (4.0 * count / NUM_SAMPLES)


Fail to execute line 7: count = sc.parallelize(xrange(0, NUM_SAMPLES)) \
Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-237895005518745498.py", line 375, in <module>
  File "<stdin>", line 7, in <module>
  File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/context.py", line 513, in parallelize
    return self.parallelize([], numSlices).mapPartitionsWithIndex(f)
  File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/context.py", line 527, in parallelize
    jrdd = self._serialize_to_jvm(c, serializer, reader_func, createRDDServer)
  File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/context.py", line 556, in _serialize_to_jvm
    tempFile = NamedTemporaryFile(delete=False, dir=self._temp_dir)
  File "/usr/lib64/python2.7/tempfile.py", line 475, in NamedTemporaryFile
    (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags)
  File "/usr/lib64/python2.7/tempfile.py", line 244, in _mkstemp_inner
    fd = _os.open(file, flags, 0600)
OSError: [Errno 2] No such file or directory: '/tmp/spark-temp/spark-f8efede5-5ffe-4b5d-a916-3197bd1f1232/pyspark-4a1de947-7798-4aae-9d81-91baa6ee2a1f/tmpyUi9_1'


## Note:
## If we run the above multiple times, we get the same exception, but witha different filename, see "tmpyUi9_1" in the above exception




## The above two examples have the same error, even though they are from different notebooks




## Try the above example in a new Scala notebook cell
## ---------------------------------------------------

val count = sc.parallelize(1 to NUM_SAMPLES).filter { _ =>
  val x = math.random
  val y = math.random
  x*x + y*y < 1
}.count()


println(s"Pi is roughly ${4.0 * count / NUM_SAMPLES}")

org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.io.IOException: Failed to create local dir in /tmp/spark-temp/blockmgr-9f7f5a7d-a832-4be7-b1d7-08fe21e7a4da/00.
java.io.IOException: Failed to create local dir in /tmp/spark-temp/blockmgr-9f7f5a7d-a832-4be7-b1d7-08fe21e7a4da/00.
	at org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:70)
	at org.apache.spark.storage.DiskStore.remove(DiskStore.scala:117)
	at org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:1588)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1118)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:914)
	at org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:1481)
	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:123)
	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:88)
	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)
	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1489)
	at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1161)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:1069)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1013)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2067)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
  at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1171)
  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:1069)
  at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1013)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2067)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
  at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
  ... 47 elided
Caused by: java.io.IOException: Failed to create local dir in /tmp/spark-temp/blockmgr-9f7f5a7d-a832-4be7-b1d7-08fe21e7a4da/00.
  at org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:70)
  at org.apache.spark.storage.DiskStore.remove(DiskStore.scala:117)
  at org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:1588)
  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1118)
  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
  at org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:914)
  at org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:1481)
  at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:123)
  at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:88)
  at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)
  at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)
  at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1489)
  at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1161)
  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:1069)
  at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1013)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2067)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)




## Try manually creating the directories that the Exception is showing as missing:
## On each worker node, master node and Zeppelin node create dir: 

mkdir -p /tmp/spark-temp/spark-f8efede5-5ffe-4b5d-a916-3197bd1f1232/pyspark-4a1de947-7798-4aae-9d81-91baa6ee2a1f/



# Try running again:

# In Zeppelin: 

%spark.pyspark

import random
NUM_SAMPLES = 2000000000

def inside(p):
    x, y = random.random(), random.random()
    return x*x + y*y < 1

count = sc.parallelize(xrange(0, NUM_SAMPLES)) \
             .filter(inside).count()
print "Pi is roughly %f" % (4.0 * count / NUM_SAMPLES)

Pi is roughly 3.141510





# Now try deleting /tmp/spark-temp/ directory and running the above again:

sudo rm -r /tmp/spark-temp/


# In Zeppelin: 

%spark.pyspark

import random
NUM_SAMPLES = 2000000000

def inside(p):
    x, y = random.random(), random.random()
    return x*x + y*y < 1

count = sc.parallelize(xrange(0, NUM_SAMPLES)) \
             .filter(inside).count()
print "Pi is roughly %f" % (4.0 * count / NUM_SAMPLES)


..
 OSError: [Errno 2] No such file or directory: ..
...'




# Add spark temp directory on just the Zeppelin node

# On Zeppelin Node:

mkdir -p /tmp/spark-temp/spark-f8efede5-5ffe-4b5d-a916-3197bd1f1232/pyspark-4a1de947-7798-4aae-9d81-91baa6ee2a1f/


## Check the Spark configuration on the Zeppelin Node:
## ---------------------------------------------------

cat /home/fedora/spark/conf/spark-defaults.conf

spark.master                     yarn
spark.driver.memory              17g
spark.yarn.am.memory            17g
spark.executor.memory          17g
spark.eventLog.enabled  true
spark.eventLog.dir hdfs://stv-dev-master:9000/spark-log
spark.driver.maxResultSize	8192m
spark.local.dir         /tmp/spark-temp
spark.executor.cores            5
#spark.dynamicAllocation.enabled         true
#spark.shuffle.service.enabled           true


# Looks like this is set here: spark.local.dir   /tmp/spark-temp
# /tmp/ directories are probably getting cleared, and running Spark notebooks can't find them anymore


## Check Fedora /tmp/ rules:
## ----------------------------------
 
cat /usr/lib/tmpfiles.d/tmp.conf 
#  This file is part of systemd.
#
#  systemd is free software; you can redistribute it and/or modify it
#  under the terms of the GNU Lesser General Public License as published by
#  the Free Software Foundation; either version 2.1 of the License, or
#  (at your option) any later version.

# See tmpfiles.d(5) for details

# Clear tmp directories separately, to make them easier to override
q /tmp 1777 root root 10d
q /var/tmp 1777 root root 30d

# Exclude namespace mountpoints created with PrivateTmp=yes
x /tmp/systemd-private-%b-*
X /tmp/systemd-private-%b-*/tmp
x /var/tmp/systemd-private-%b-*
X /var/tmp/systemd-private-%b-*/tmp

# Remove top-level private temporary directories on each boot
R! /tmp/systemd-private-*
R! /var/tmp/systemd-private-*


## Change spark local dir parameter
## ----------------------------------
nano /home/fedora/spark/conf/spark-defaults.conf
..

spark.master                     yarn
spark.driver.memory              17g
spark.yarn.am.memory            17g
spark.executor.memory          17g
spark.eventLog.enabled  true
spark.eventLog.dir hdfs://stv-dev-master:9000/spark-log
spark.driver.maxResultSize	8192m
spark.local.dir         /home/fedora/spark/local
spark.executor.cores            5
#spark.dynamicAllocation.enabled         true
#spark.shuffle.service.enabled           true

..


## Restart Zeppelin
## ----------------------------------

/home/fedora/zeppelin/bin/zeppelin-daemon.sh stop
/home/fedora/zeppelin/bin/zeppelin-daemon.sh start



ls -al /home/fedora/spark/local/
total 16
drwxr-xr-x.  4 fedora fedora 4096 May 28 17:27 .
drwxrwxr-x. 14 fedora fedora 4096 May 28 17:07 ..
drwxrwxr-x.  5 fedora fedora 4096 May 28 17:30 blockmgr-7a28d729-2842-4b16-803b-a6082d30a0fd
drwx------.  4 fedora fedora 4096 May 28 17:27 spark-a55fe940-ba73-4b62-a30c-4e5df79d0ef7



## Try running again:
## ----------------------

# Try running example again
# In Zeppelin: 

%spark.pyspark

import random
NUM_SAMPLES = 2000000000

def inside(p):
    x, y = random.random(), random.random()
    return x*x + y*y < 1

count = sc.parallelize(xrange(0, NUM_SAMPLES)) \
             .filter(inside).count()
print "Pi is roughly %f" % (4.0 * count / NUM_SAMPLES)

Pi is roughly 3.141510



# All other Notebooks seem to be running successfully now.
#
# We need to now:
#
#   1) Check whether this happens again with the existing configuration
#   2) Figure out what is stored in the spark local dir
#   3) Check if we need a process to clear this directory, if it starts using up too much disk space


