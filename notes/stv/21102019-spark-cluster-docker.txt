#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2015, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

########################################################
## Create 3 VMs
########################################################
Stevedore@Cadelicia (Master Node)
Stevedore@Froeseth (Worker Node)
Stevedore@Abecien (Worker Node)



########################################################
## Create Docker Swarm and connect them
########################################################
ssh Stevedore@Cadelicia
	docker swarm init --advertise-addr 192.168.201.11


	## Firewall Ports for Swarm

	sudo su
	    firewall-cmd --add-port=2377/tcp --permanent
	    firewall-cmd --add-port=2377/tcp --permanent
	    firewall-cmd --add-port=7946/tcp --permanent
	    firewall-cmd --add-port=4789/tcp --permanent
	    firewall-cmd --add-port=4789/udp --permanent
	    firewall-cmd --add-port=9000/tcp --permanent
	    firewall-cmd --add-port=9000/udp --permanent
	    firewall-cmd --add-port=9866/tcp --permanent
	    firewall-cmd --add-port=9866/udp --permanent
	    iptables -A INPUT -p 50 -j ACCEPT    
	    firewall-cmd --reload
	exit
exit


Stevedore@Froeseth
        docker swarm join --token SWMTKN-1-1dpw6aqhjgu14bwv53smgrb1ftyvnhtd5b5ja946v9g352jah0-012oy90p9zgq3l2x4ybl2vvjy 192.168.201.11:2377

	sudo su
	    firewall-cmd --add-port=2377/tcp --permanent
	    firewall-cmd --add-port=2377/tcp --permanent
	    firewall-cmd --add-port=7946/tcp --permanent
	    firewall-cmd --add-port=4789/tcp --permanent
	    firewall-cmd --add-port=4789/udp --permanent
	    firewall-cmd --add-port=9000/tcp --permanent
	    firewall-cmd --add-port=9000/udp --permanent
	    firewall-cmd --add-port=9866/tcp --permanent
	    firewall-cmd --add-port=9866/udp --permanent
	    iptables -A INPUT -p 50 -j ACCEPT    
	    firewall-cmd --reload
	exit
exit


Stevedore@Abecien
        docker swarm join --token SWMTKN-1-1dpw6aqhjgu14bwv53smgrb1ftyvnhtd5b5ja946v9g352jah0-012oy90p9zgq3l2x4ybl2vvjy 192.168.201.11:2377


	sudo su
	    firewall-cmd --add-port=2377/tcp --permanent
	    firewall-cmd --add-port=2377/tcp --permanent
	    firewall-cmd --add-port=7946/tcp --permanent
	    firewall-cmd --add-port=4789/tcp --permanent
	    firewall-cmd --add-port=4789/udp --permanent
	    firewall-cmd --add-port=9000/tcp --permanent
	    firewall-cmd --add-port=9000/udp --permanent
	    firewall-cmd --add-port=9866/tcp --permanent
	    firewall-cmd --add-port=9866/udp --permanent
	    iptables -A INPUT -p 50 -j ACCEPT    
	    firewall-cmd --reload
	exit
exit

########################################################
## Update Hosts file on each node
########################################################
..

127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

192.168.201.8       Delild
192.168.201.9       Abecien
192.168.201.10      Saewan
192.168.201.11      Cadelicia
192.168.201.12      Froeseth
192.168.201.13      Astoalith
192.168.201.14      Erennon
192.168.201.15      Gworewia

..


########################################################
## Fetch Hadoop Binaries on each node
########################################################

ssh Stevedore@Cadelicia

	wget http://apache.cs.utah.edu/hadoop/common/current/hadoop-3.1.3.tar.gz
	tar -xzf hadoop-3.1.3.tar.gz
	mv hadoop-3.1.3 hadoop


## Fetch Hadoop Binaries on Worker node
ssh Stevedore@Froeseth

	wget http://apache.cs.utah.edu/hadoop/common/current/hadoop-3.1.3.tar.gz
	tar -xzf hadoop-3.1.3.tar.gz
	mv hadoop-3.1.3 hadoop


## Fetch Hadoop Binaries on Worker node
ssh Stevedore@Abecien

	wget http://apache.cs.utah.edu/hadoop/common/current/hadoop-3.1.3.tar.gz
	tar -xzf hadoop-3.1.3.tar.gz
	mv hadoop-3.1.3 hadoop


########################################################
## Setup Hadoop Configuration
########################################################
ssh Stevedore@Cadelicia

	## Install java
	sudo  dnf install java-11-openjdk.x86_64

	nano .profile
	..
	PATH=/home/Stevedore/hadoop/bin:/home/Stevedore/hadoop/sbin:$PATH
	..


	nano .bashrc
	..
	export HADOOP_HOME=/home/Stevedore/hadoop
	export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin
	export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.212.b04-0.fc28.x86_64/jre
	..

## Setup Configuration files


## hadoop/etc/hadoop/core-site.xml 

<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>fs.default.name</name>
            <value>hdfs://Cadelicia:9000</value>
        </property>
    </configuration>




## hadoop/etc/hadoop/yarn-site.xml
<configuration>
    <property>
            <name>yarn.acl.enable</name>
            <value>0</value>
    </property>

    <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>192.168.201.11</value>
    </property>

    <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
    </property>
<property>
	<name>yarn.nodemanager.resource.memory-mb</name>
        <value>1536</value>
</property>

<property>
	<name>yarn.scheduler.maximum-allocation-mb</name>
        <value>1536</value>
</property>

<property>
	<name>yarn.scheduler.minimum-allocation-mb</name>
        <value>128</value>
</property>

<property>
	<name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
</property>
</configuration>



## hadoop/etc/hadoop/workers

Abecien
Froeseth




## hadoop/etc/hadoop/mapred-site.xml
<configuration>
    <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
    </property>
    <property>
            <name>yarn.app.mapreduce.am.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
    <property>
            <name>mapreduce.map.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
    <property>
            <name>mapreduce.reduce.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
<property>
	<name>yarn.app.mapreduce.am.resource.mb</name>
        <value>512</value>
</property>

<property>
	<name>mapreduce.map.memory.mb</name>
        <value>256</value>
</property>

<property>
	<name>mapreduce.reduce.memory.mb</name>
        <value>256</value>
</property>
</configuration>



## hadoop/etc/hadoop/core-site.xml

<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>fs.default.name</name>
            <value>hdfs://Cadelicia:9000</value>
        </property>
    </configuration>


################################################
## Format HDFS Master Node
################################################

mkdir /home/hadoop/
sudo chown -R Stevedore:root /home/hadoop/

source /home/Stevedore/hadoop/bin/hdfs namenode -format


...



2019-10-22 18:30:51,857 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
2019-10-22 18:30:51,857 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-10-22 18:30:51,859 INFO util.GSet: Computing capacity for map NameNodeRetryCache
2019-10-22 18:30:51,859 INFO util.GSet: VM type       = 64-bit
2019-10-22 18:30:51,860 INFO util.GSet: 0.029999999329447746% max memory 876.5 MB = 269.3 KB
2019-10-22 18:30:51,860 INFO util.GSet: capacity      = 2^15 = 32768 entries
2019-10-22 18:30:51,892 INFO namenode.FSImage: Allocated new BlockPoolId: BP-105882924-192.168.201.11-1571765451881
2019-10-22 18:30:51,909 INFO common.Storage: Storage directory /home/hadoop/data/nameNode has been successfully formatted.
2019-10-22 18:30:51,953 INFO namenode.FSImageFormatProtobuf: Saving image file /home/hadoop/data/nameNode/current/fsimage.ckpt_0000000000000000000 using no compression
2019-10-22 18:30:52,144 INFO namenode.FSImageFormatProtobuf: Image file /home/hadoop/data/nameNode/current/fsimage.ckpt_0000000000000000000 of size 396 bytes saved in 0 seconds .
2019-10-22 18:30:52,167 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
2019-10-22 18:30:52,176 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid = 0 when meet shutdown.
2019-10-22 18:30:52,178 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at Cadelicia/192.168.201.11
************************************************************/
Connection to cadelicia closed.


################################################
## Start and Stop HDFS
################################################

start-dfs.sh

..

Starting namenodes on [Cadelicia]
Cadelicia: namenode is running as process 8597.  Stop it first.
Starting datanodes
Abecien: WARNING: /home/Stevedore/hadoop/logs does not exist. Creating.
Froeseth: WARNING: /home/Stevedore/hadoop/logs does not exist. Creating.
Starting secondary namenodes [Cadelicia]
Cadelicia: secondarynamenode is running as process 8838.  Stop it first.
[Stevedore@Cadelicia bin]$ 




## Create a directory
hdfs dfs -mkdir /user/hadoop/


## LS directory
hdfs dfs -ls /user/hadoop/





