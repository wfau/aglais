#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2019, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#



################################################################
##  Setup Zepellin/Yarn/Spark/Hadoop Cluster                  ##
################################################################


# Read up and try to follow
# https://docs.hpc.cam.ac.uk/cloud-training/#16


#-------------------------------------------------------------------------
# GUI Steps to Setup Networks, Security Groups & Instances
#-------------------------------------------------------------------------


## Step 1:

Create Private Network 

   Name: stv-aglais-router

   Create Subnet
      Name - stv-aglais-subnet 
      CIDR - 10.0.0.0/24
      DNS Name Servers - 131.111.8.42 131.111.12.20      



## Step 2:

Create Router, connected to Internet
 
   Name: stv-aglais-router
   External Network: Internet

   Create Interface
      Subnet - stv-aglais-network


## Step 3

Create Security Groups (based on training presentation)

external-bastion: 22 / ICMP
internal-bastion: default
internal-webserver: 22 - internal-bastion
external-webserver: 80
stv-aglais-zeppelin : 8080
stv-aglais-master: *
stv-aglais-worker: * - stv-aglais-master



## Step 4:

Create Bastion Instance (Gateway)
    Fedora-30-1.2
    (external-bastion + internal-bastion)


## Step 5:

Create Master node
    Fedora-30-1.2
    (internal-bastion + internal-webserver + aglais-master)



## Step 6:

Create 3 Worker Nodes 
    Fedora-30-1.2
    (external-bastion + internal-webserver + aglais-worker)



## Step 7:

Create Zeppelin Node 
(external-bastion + internal-webserver + aglais-zeppelin)





stv-aglais-bastion-ip=
stv-aglais-worker-1-ip=
stv-aglais-worker-2-ip=
stv-aglais-worker-3-ip=
stv-aglais-master-ip=
stv-aglais-zeppelin-ip=




#-------------------------------------------------------------------------
# Setup .ssh config locally
#-------------------------------------------------------------------------

~/.ssh/config
..


Host stv-aglais-bastion
    User centos
    IdentityFile ~/.ssh/stv-master
    Hostname ${stv-aglais-bastion-ip:?}

Host stv-aglais-master
    User fedora
    IdentityFile ~/.ssh/stv-master
    Hostname${stv-aglais-master-ip:?}
    ProxyCommand ssh -W %h:%p stv-aglais-bastion

Host stv-aglais-worker-1
    User fedora
    IdentityFile ~/.ssh/stv-master
    Hostname ${stv-aglais-worker-1-ip:?}
    ProxyCommand ssh -W %h:%p stv-aglais-bastion

Host stv-aglais-worker-2
    User fedora
    IdentityFile ~/.ssh/stv-master
    Hostname ${stv-aglais-worker-2-ip:?}
    ProxyCommand ssh -W %h:%p stv-aglais-bastion


Host stv-aglais-worker-3
    User fedora
    IdentityFile ~/.ssh/stv-master
    Hostname ${stv-aglais-worker-3-ip:?}
    ProxyCommand ssh -W %h:%p stv-aglais-bastion

Host stv-aglais-zeppelin
    User fedora
    IdentityFile ~/.ssh/stv-master
    Hostname ${stv-aglais-zeppelin-ip:?}
    ProxyCommand ssh -W %h:%p stv-aglais-bastion

..



#-------------------------------------------------------------------------
## Step 5: Install HDFS Cluster
#-------------------------------------------------------------------------

## Fetch Hadoop Binaries on each node & install java


## Run on all nodes

sudo yum install wget
wget http://apache.cs.utah.edu/hadoop/common/current/hadoop-3.1.3.tar.gz
tar -xzf hadoop-3.1.3.tar.gz
mv hadoop-3.1.3 hadoop
sudo yum install java-1.8.0-openjdk





#-------------------------------------------------------------------------
# Setup Environment variables on each node
#-------------------------------------------------------------------------

# For each Worker node & Master node

cat > "${HOME:?}/.profile" << EOF
PATH=/home/fedora/spark/bin:/home/fedora/.local/bin:/home/fedora/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/bin:/sbin:/home/fedora/hadoop/bin:/home/fedora/hadoop/sbin
EOF

cat <<EOF >> "${HOME:?}/.bashrc"
export HADOOP_HOME=/home/fedora/hadoop
export PATH=/home/fedora/.local/bin:/home/fedora/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/bin:/sbin:/home/fedora/hadoop/bin:/home/fedora/hadoop/sbin:/home/fedora/spark/bin
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre/
export HADOOP_CONF_DIR=/home/fedora/hadoop/etc/hadoop
export SPARK_HOME=/home/fedora/spark
export LD_LIBRARY_PATH=/home/fedora/hadoop/lib/native
EOF





#-------------------------------------------------------------------------
## Setup /etc/hosts file and ~/.ssh/config file
#-------------------------------------------------------------------------


sudo su
cat <<EOF >> "/etc/hosts"

127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

	
10.0.0.15       stv-aglais-master.novalocal
10.0.0.15       stv-aglais-master
10.0.0.16       stv-aglais-worker-1
10.0.0.9       stv-aglais-worker-2
10.0.0.10       stv-aglais-worker-3
10.0.0.5      stv-aglais-zeppelin

EOF
  
exit


cat > "${HOME:?}/.ssh/config" << EOF

    Host stv-aglais-worker-1
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

    Host stv-aglais-worker-2 
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

    Host stv-aglais-worker-3
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

    Host stv-aglais-master.novalocal
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

    Host stv-aglais-master
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

    Host stv-aglais-zeppelin
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

EOF


sudo chmod 600 ~/.ssh/config
sudo chmod 600 ~/.ssh/stv-master





#-------------------------------------------------------------------------
# Setup Hadoop 
#-------------------------------------------------------------------------

## Setup Hadoop Configurations


cat > "${HOME:?}/hadoop/etc/hadoop/core-site.xml" << EOF

<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>fs.default.name</name>
            <value>hdfs://stv-aglais-master:9000</value>
        </property>
    </configuration>

EOF


cat > "${HOME:?}/hadoop/etc/hadoop/yarn-site.xml" << EOF

<configuration>
    <property>
            <name>yarn.acl.enable</name>
            <value>0</value>
    </property>

    <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>stv-aglais-master</value>
    </property>

    <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
    </property>
<property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>50000</value>
</property>

<property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>50000</value>
</property>

<property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>2000</value>
</property>

<property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
</property>
<property>
   <name>yarn.scheduler.capacity.root.support.user-limit-factor</name>  
   <value>2</value>
</property>
<property>
   <name>yarn.nodemanager.disk-health-checker.min-healthy-disks</name>
   <value>0.0</value>
</property>
<property>
   <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
   <value>100.0</value>
</property>

 <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>stv-aglais-master:8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.address</name>
    <value>stv-aglais-master:8032</value>
  </property>
  <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>stv-aglais-master:8088</value>
  </property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>stv-aglais-master:8031</value>
  </property>
  <property>
    <name>yarn.resourcemanager.admin.address</name>
    <value>stv-aglais-master:8033</value>
  </property>

</configuration>

EOF




cat > "${HOME:?}/hadoop/etc/hadoop/workers" << EOF

stv-aglais-worker-1
stv-aglais-worker-2
stv-aglais-worker-3

EOF




cat > "${HOME:?}/hadoop/etc/hadoop/mapred-site.xml" << EOF

<configuration>
    <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
    </property>
    <property>
            <name>yarn.app.mapreduce.am.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
    <property>
            <name>mapreduce.map.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
    <property>
            <name>mapreduce.reduce.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
<property>
	<name>yarn.app.mapreduce.am.resource.mb</name>
        <value>50000</value>
</property>

<property>
	<name>mapreduce.map.memory.mb</name>
        <value>25000</value>
</property>

<property>
	<name>mapreduce.reduce.memory.mb</name>
        <value>25000</value>
</property>
</configuration>

EOF




cat > "${HOME:?}/hadoop/etc/hadoop/core-site.xml" << EOF

<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>fs.default.name</name>
            <value>hdfs://stv-aglais-master:9000</value>
        </property>
    </configuration>

EOF






#-------------------------------------------------------------------------
## Format HDFS Master Node on each node
#-------------------------------------------------------------------------

## for each worker node

sudo mkdir /home/hadoop/
sudo chown -R fedora:root /home/hadoop/
source /home/fedora/hadoop/bin/hdfs namenode -format




#-------------------------------------------------------------------------
## Install Python3 and set it as default for each worker node
#-------------------------------------------------------------------------

## for each worker node

sudo yum install python3
sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 10




#-------------------------------------------------------------------------
## Start and Stop HDFS services
#-------------------------------------------------------------------------



## On Master Node: (ssh stv-aglais-master)

start-all.sh


## Create a directory
hdfs dfs -mkdir /hadoop/
hdfs dfs -mkdir /hadoop/books/


## Run test job

cd /home/hadoop
wget -O alice.txt https://www.gutenberg.org/files/11/11-0.txt
wget -O holmes.txt https://www.gutenberg.org/files/1661/1661-0.txt
wget -O frankenstein.txt https://www.gutenberg.org/files/84/84-0.txt


hdfs dfs -put alice.txt holmes.txt frankenstein.txt books




## LS directory
hdfs dfs -ls /hadoop/books


yarn jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount "/hadoop/books/*" output
## Job Completed Successfully
## Check Results

hdfs dfs -ls output
hdfs dfs -cat output/part-r-00000 | less






#-------------------------------------------------------------------------
# Spark
#-------------------------------------------------------------------------

## Setup Spark on Existing yarn & hdfs cluster


## Download and Install Spark Binaries
## On Master Node: (ssh stv-aglais-master)

cd /home/fedora
wget https://www-eu.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz
tar -xvf spark-2.4.4-bin-hadoop2.7.tgz
mv spark-2.4.4-bin-hadoop2.7.tgz spark


## Setup Default Spark Config

mv /home/fedora/spark/conf/spark-defaults.conf.template /home/fedora/spark/conf/spark-defaults.conf

cat <<EOF >> "/home/fedora/spark/conf/spark-defaults.conf"
spark.master                     yarn
spark.driver.memory              50g
spark.yarn.am.memory            50g
spark.executor.memory          50g
spark.eventLog.enabled  true
spark.eventLog.dir hdfs://${stv-aglais-master-ip:?}:9000/spark-log
EOF


## Create the log directory in HDFS:
hdfs dfs -mkdir /spark-log


./bin/spark-submit --class org.apache.spark.examples.SparkPi     --master yarn-client     --num-executors 1     --driver-memory 512m     --executor-memory 512m     --executor-cores 1     examples/jars/spark-examples*.jar 10

   Exception in thread "main" java.lang.IllegalArgumentException: Required AM memory (46080+4608 MB) is above the max threshold (50000 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'.


## Change Spark Configuration to 

spark.master                     yarn
spark.driver.memory              40g
spark.yarn.am.memory            40g
spark.executor.memory          40g
spark.eventLog.enabled  true
spark.eventLog.dir hdfs://${stv-aglais-master-ip:?}:9000/spark-log


2019-11-20 19:04:39,482 INFO scheduler.DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 0.706995 s
Pi is roughly 3.1443231443231445




#-------------------------------------------------------------------------
# Zeppelin
#-------------------------------------------------------------------------



## On Zeppelin Node: (ssh stv-aglais-zeppelin)

## Fetch Zeppelin Binaries
sudo yum install wget
sudo yum install nano
sudo yum install java-1.8.0-openjdk

wget https://www-eu.apache.org/dist/zeppelin/zeppelin-0.8.2/zeppelin-0.8.2-bin-all.tgz
tar -xzvf zeppelin-0.8.2-bin-all.tgz 
mv zeppelin-0.8.2-bin-all zeppelin
rm zeppelin-0.8.2-bin-all.tgz 



## Setup Zeppelin Configurations

## Setup Users in shiro.ini
cp /home/fedora/zeppelin/conf/shiro.ini.template /home/fedora/zeppelin/conf/shiro.ini


## Set Zeppelin Configuration
cp /home/fedora/zeppelin/conf/zeppelin-site.xml.template /home/fedora/zeppelin/conf/zeppelin-site.xml
nano /home/fedora/zeppelin/conf/zeppelin-site.xml



sudo su
cat <<EOF >> "/etc/hosts"

127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

	
10.0.0.15       stv-aglais-master.novalocal
10.0.0.15       stv-aglais-master
10.0.0.16       stv-aglais-worker-1
10.0.0.9       stv-aglais-worker-2
10.0.0.10       stv-aglais-worker-3
10.0.0.5      stv-aglais-zeppelin

EOF
  
exit


cat > "${HOME:?}/.ssh/config" << EOF

    Host stv-aglais-worker-1
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

    Host stv-aglais-worker-2 
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

    Host stv-aglais-worker-3
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

    Host stv-aglais-master.novalocal
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

    Host stv-aglais-master
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

    Host stv-aglais-zeppelin
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

EOF


sudo chmod 600 ~/.ssh/config



mkdir /home/fedora/spark

## Copy Hadoop & Spark Config files from master
ssh stv-aglais-master
    scp ~/hadoop/etc/hadoop/* stv-aglais-master:/home/fedora/zeppelin/hadoop-conf;
    scp -r ~/spark/* stv-aglais-zeppelin:/home/fedora/spark;
exit



## Enable Python3 and matplotlib for python3 notebooks
## Update python interpreter, set it to python3

## Install matplotlib
sudo python3 -m pip install -U pip
sudo python3 -m pip install -U matplotlib
sudo python3 -m pip install -U healpy



## Start Server
/home/fedora/zeppelin/bin/zeppelin-daemon.sh start


## Tunnel connection to 8080 and go to
## On Local Machine (Browser)
## Go to: http://localhost:8080/#/



## Update Spark Interpreter from admin GUI
## Set master=yarn-client




#-------------------------------------------------------------------------
#  Gaiasource & Tgas as Parquet files
#-------------------------------------------------------------------------

## From Gaia-spark virtual image (GDAF), copy ascii, parquet files from /archive/gaia/gdr1 to /hadoop/gaia/parquet & /hadoop/gaia/parquet on master node
## If we want to use the Spark cluster to generate the parquet files from the ascii ones by running the asciiToParquetConverter jar through spark, we'd have to copy
## the ascii files on each worker node?

## To convert, run:
## Run from directory where asciiToParquetConverter.jar is

    spark-submit --num-executors 3 --driver-memory 4096m --executor-memory 1024m --executor-cores 2 asciiToParquetConverter.jar -input file:///hadoop/gaia/ascii/gaia/data -output file:///hadoop/gaia/parquet/gaia/gdr1/gaiaSource/ -schema file:///hadoop/gaia/ascii/gaia/GaiaSourceHeaderSchema
  


## Put parquet files on HDFS:
  
    hdfs dfs -put gdr1/ /hadoop/gaia/parquet/




#-------------------------------------------------------------------------
#  Connecting to HDFS from Zeppelin to read Parquet
#-------------------------------------------------------------------------


%pyspark
df = sqlContext.read.parquet("/hadoop/gaia/parquet/gdr1/tgas/*.parquet")
df.show()

