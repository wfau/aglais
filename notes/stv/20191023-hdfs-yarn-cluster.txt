#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2015, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

########################################################
## Create 3 VMs
########################################################
Stevedore@Cadelicia (Master Node)
Stevedore@Froeseth (Worker Node)
Stevedore@Abecien (Worker Node)



########################################################
## Create Docker Swarm and connect them
########################################################
ssh Stevedore@Cadelicia
	docker swarm init --advertise-addr 192.168.201.11


	## Firewall Ports for Swarm

	sudo su
	    firewall-cmd --add-port=2377/tcp --permanent
	    firewall-cmd --add-port=2377/tcp --permanent
	    firewall-cmd --add-port=7946/tcp --permanent
	    firewall-cmd --add-port=4789/tcp --permanent
	    firewall-cmd --add-port=4789/udp --permanent
	    firewall-cmd --add-port=9000/tcp --permanent
	    firewall-cmd --add-port=9000/udp --permanent
	    firewall-cmd --add-port=9866/tcp --permanent
	    firewall-cmd --add-port=9866/udp --permanent
	    firewall-cmd --add-port=8030/tcp --permanent
	    firewall-cmd --add-port=8030/udp --permanent
	    firewall-cmd --add-port=8031/tcp --permanent
	    firewall-cmd --add-port=8031/udp --permanent
	    firewall-cmd --add-port=8032/tcp --permanent
	    firewall-cmd --add-port=8032/udp --permanent
	    firewall-cmd --add-port=8033/tcp --permanent
	    firewall-cmd --add-port=8033/udp --permanent
	    firewall-cmd --add-port=8042/tcp --permanent
	    firewall-cmd --add-port=8042/udp --permanent
	    firewall-cmd --add-port=8050/tcp --permanent
	    firewall-cmd --add-port=8050/udp --permanent
	    iptables -A INPUT -p 50 -j ACCEPT    
	    firewall-cmd --reload
	exit
exit


Stevedore@Froeseth
        docker swarm join --token SWMTKN-1-1dpw6aqhjgu14bwv53smgrb1ftyvnhtd5b5ja946v9g352jah0-012oy90p9zgq3l2x4ybl2vvjy 192.168.201.11:2377

	sudo su
	    firewall-cmd --add-port=2377/tcp --permanent
	    firewall-cmd --add-port=2377/tcp --permanent
	    firewall-cmd --add-port=7946/tcp --permanent
	    firewall-cmd --add-port=4789/tcp --permanent
	    firewall-cmd --add-port=4789/udp --permanent
	    firewall-cmd --add-port=9000/tcp --permanent
	    firewall-cmd --add-port=9000/udp --permanent
	    firewall-cmd --add-port=9866/tcp --permanent
	    firewall-cmd --add-port=9866/udp --permanent
	    firewall-cmd --add-port=8030/tcp --permanent
	    firewall-cmd --add-port=8030/udp --permanent
	    firewall-cmd --add-port=8031/tcp --permanent
	    firewall-cmd --add-port=8031/udp --permanent
	    firewall-cmd --add-port=8032/tcp --permanent
	    firewall-cmd --add-port=8032/udp --permanent
	    firewall-cmd --add-port=8033/tcp --permanent
	    firewall-cmd --add-port=8033/udp --permanent
	    firewall-cmd --add-port=8042/tcp --permanent
	    firewall-cmd --add-port=8042/udp --permanent
	    firewall-cmd --add-port=8050/tcp --permanent
	    firewall-cmd --add-port=8050/udp --permanent
	    iptables -A INPUT -p 50 -j ACCEPT    
	    firewall-cmd --reload
	exit
exit


Stevedore@Abecien
        docker swarm join --token SWMTKN-1-1dpw6aqhjgu14bwv53smgrb1ftyvnhtd5b5ja946v9g352jah0-012oy90p9zgq3l2x4ybl2vvjy 192.168.201.11:2377

	sudo su
	    firewall-cmd --add-port=2377/tcp --permanent
	    firewall-cmd --add-port=2377/tcp --permanent
	    firewall-cmd --add-port=7946/tcp --permanent
	    firewall-cmd --add-port=4789/tcp --permanent
	    firewall-cmd --add-port=4789/udp --permanent
	    firewall-cmd --add-port=9000/tcp --permanent
	    firewall-cmd --add-port=9000/udp --permanent
	    firewall-cmd --add-port=9866/tcp --permanent
	    firewall-cmd --add-port=9866/udp --permanent
	    firewall-cmd --add-port=8030/tcp --permanent
	    firewall-cmd --add-port=8030/udp --permanent
	    firewall-cmd --add-port=8031/tcp --permanent
	    firewall-cmd --add-port=8031/udp --permanent
	    firewall-cmd --add-port=8032/tcp --permanent
	    firewall-cmd --add-port=8032/udp --permanent
	    firewall-cmd --add-port=8033/tcp --permanent
	    firewall-cmd --add-port=8033/udp --permanent
	    firewall-cmd --add-port=8042/tcp --permanent
	    firewall-cmd --add-port=8042/udp --permanent
	    firewall-cmd --add-port=8050/tcp --permanent
	    firewall-cmd --add-port=8050/udp --permanent
	    iptables -A INPUT -p 50 -j ACCEPT    
	    firewall-cmd --reload
	exit
exit

########################################################
## Update Hosts file on each node
########################################################
..

127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

192.168.201.8       Delild
192.168.201.9       Abecien
192.168.201.10      Saewan
192.168.201.11      Cadelicia
192.168.201.12      Froeseth
192.168.201.13      Astoalith
192.168.201.14      Erennon
192.168.201.15      Gworewia

..



## ---------------------------------------------------------------------- Setup Hadoop -----------------------------------------------------------------------



########################################################
## Fetch Hadoop Binaries on each node
########################################################

ssh Stevedore@Cadelicia

	wget http://apache.cs.utah.edu/hadoop/common/current/hadoop-3.1.3.tar.gz
	tar -xzf hadoop-3.1.3.tar.gz
	mv hadoop-3.1.3 hadoop


## Fetch Hadoop Binaries on Worker node
ssh Stevedore@Froeseth

	wget http://apache.cs.utah.edu/hadoop/common/current/hadoop-3.1.3.tar.gz
	tar -xzf hadoop-3.1.3.tar.gz
	mv hadoop-3.1.3 hadoop


## Fetch Hadoop Binaries on Worker node
ssh Stevedore@Abecien

	wget http://apache.cs.utah.edu/hadoop/common/current/hadoop-3.1.3.tar.gz
	tar -xzf hadoop-3.1.3.tar.gz
	mv hadoop-3.1.3 hadoop


########################################################
## Setup Hadoop Configuration
########################################################
ssh Stevedore@Cadelicia

	## Install java
	sudo  dnf install java-11-openjdk.x86_64

	nano .profile
	..
	PATH=/home/Stevedore/hadoop/bin:/home/Stevedore/hadoop/sbin:$PATH
	..


	nano .bashrc
	..
	export HADOOP_HOME=/home/Stevedore/hadoop
	export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin
	export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.212.b04-0.fc28.x86_64/jre
	..

## Setup Configuration files


## hadoop/etc/hadoop/core-site.xml 

<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>fs.default.name</name>
            <value>hdfs://Cadelicia:9000</value>
        </property>
    </configuration>




## hadoop/etc/hadoop/yarn-site.xml
<configuration>
    <property>
            <name>yarn.acl.enable</name>
            <value>0</value>
    </property>

    <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>192.168.201.11</value>
    </property>

    <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
    </property>
<property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>1536</value>
</property>

<property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>1536</value>
</property>

<property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>128</value>
</property>

<property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
</property>
<property>
   <name>yarn.scheduler.capacity.root.support.user-limit-factor</name>  
   <value>2</value>
</property>
<property>
   <name>yarn.nodemanager.disk-health-checker.min-healthy-disks</name>
   <value>0.0</value>
</property>
<property>
   <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
   <value>100.0</value>
</property>

 <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>Cadelicia:8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.address</name>
    <value>Cadelicia:8032</value>
  </property>
  <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>Cadelicia:8088</value>
  </property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>Cadelicia:8031</value>
  </property>
  <property>
    <name>yarn.resourcemanager.admin.address</name>
    <value>Cadelicia:8033</value>
  </property>

</configuration>



## hadoop/etc/hadoop/workers

Abecien
Froeseth




## hadoop/etc/hadoop/mapred-site.xml
<configuration>
    <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
    </property>
    <property>
            <name>yarn.app.mapreduce.am.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
    <property>
            <name>mapreduce.map.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
    <property>
            <name>mapreduce.reduce.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
<property>
	<name>yarn.app.mapreduce.am.resource.mb</name>
        <value>512</value>
</property>

<property>
	<name>mapreduce.map.memory.mb</name>
        <value>256</value>
</property>

<property>
	<name>mapreduce.reduce.memory.mb</name>
        <value>256</value>
</property>
</configuration>



## hadoop/etc/hadoop/core-site.xml

<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>fs.default.name</name>
            <value>hdfs://Cadelicia:9000</value>
        </property>
    </configuration>


## "Memory allocation can be tricky on low RAM nodes because default values are not suitable for nodes with less than 8GB of RAM"

## Copy Configuration files to Worker nodes

for node in Abecien Froeseth; do
    scp ~/hadoop/etc/hadoop/* $node:/home/Stevedore/hadoop/etc/hadoop/;
done



################################################
## Format HDFS Master Node
################################################

mkdir /home/hadoop/
sudo chown -R Stevedore:root /home/hadoop/

source /home/Stevedore/hadoop/bin/hdfs namenode -format


...



2019-10-22 18:30:51,857 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
2019-10-22 18:30:51,857 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-10-22 18:30:51,859 INFO util.GSet: Computing capacity for map NameNodeRetryCache
2019-10-22 18:30:51,859 INFO util.GSet: VM type       = 64-bit
2019-10-22 18:30:51,860 INFO util.GSet: 0.029999999329447746% max memory 876.5 MB = 269.3 KB
2019-10-22 18:30:51,860 INFO util.GSet: capacity      = 2^15 = 32768 entries
2019-10-22 18:30:51,892 INFO namenode.FSImage: Allocated new BlockPoolId: BP-105882924-192.168.201.11-1571765451881
2019-10-22 18:30:51,909 INFO common.Storage: Storage directory /home/hadoop/data/nameNode has been successfully formatted.
2019-10-22 18:30:51,953 INFO namenode.FSImageFormatProtobuf: Saving image file /home/hadoop/data/nameNode/current/fsimage.ckpt_0000000000000000000 using no compression
2019-10-22 18:30:52,144 INFO namenode.FSImageFormatProtobuf: Image file /home/hadoop/data/nameNode/current/fsimage.ckpt_0000000000000000000 of size 396 bytes saved in 0 seconds .
2019-10-22 18:30:52,167 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
2019-10-22 18:30:52,176 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid = 0 when meet shutdown.
2019-10-22 18:30:52,178 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at Cadelicia/192.168.201.11
************************************************************/
Connection to cadelicia closed.


################################################
## Start and Stop HDFS
################################################

start-dfs.sh

..

Starting namenodes on [Cadelicia]
Cadelicia: namenode is running as process 8597.  Stop it first.
Starting datanodes
Abecien: WARNING: /home/Stevedore/hadoop/logs does not exist. Creating.
Froeseth: WARNING: /home/Stevedore/hadoop/logs does not exist. Creating.
Starting secondary namenodes [Cadelicia]
Cadelicia: secondarynamenode is running as process 8838.  Stop it first.
[Stevedore@Cadelicia bin]$ 




## Create a directory
hdfs dfs -mkdir /user/hadoop/


## LS directory
hdfs dfs -ls /user/hadoop/




################################################
## Monitoring HDFS
################################################ 

## Get HDFS Report
hdfs dfsadmin -report

Configured Capacity: 66033025024 (61.50 GB)
Present Capacity: 41644593152 (38.78 GB)
DFS Remaining: 41643319296 (38.78 GB)
DFS Used: 1273856 (1.21 MB)
DFS Used%: 0.00%
Replicated Blocks:
	Under replicated blocks: 0
	Blocks with corrupt replicas: 0
	Missing blocks: 0
	Missing blocks (with replication factor 1): 0
	Low redundancy blocks with highest priority to recover: 0
	Pending deletion blocks: 0
Erasure Coded Block Groups: 
	Low redundancy block groups: 0
	Block groups with corrupt internal blocks: 0
	Missing block groups: 0
	Low redundancy blocks with highest priority to recover: 0
	Pending deletion blocks: 0

-------------------------------------------------
Live datanodes (2):

Name: 192.168.201.12:9866 (Froeseth)
Hostname: Froeseth
Decommission Status : Normal
Configured Capacity: 33016512512 (30.75 GB)
DFS Used: 1085440 (1.04 MB)
Non DFS Used: 15318958080 (14.27 GB)
DFS Remaining: 16428544000 (15.30 GB)
DFS Used%: 0.00%
DFS Remaining%: 49.76%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Wed Oct 23 12:07:44 BST 2019
Last Block Report: Wed Oct 23 06:21:49 BST 2019
Num of Blocks: 2


Name: 192.168.201.9:9866 (Abecien)
Hostname: Abecien
Decommission Status : Normal
Configured Capacity: 33016512512 (30.75 GB)
DFS Used: 188416 (184 KB)
Non DFS Used: 6057390080 (5.64 GB)
DFS Remaining: 25214775296 (23.48 GB)
DFS Used%: 0.00%
DFS Remaining%: 76.37%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Wed Oct 23 12:07:44 BST 2019
Last Block Report: Wed Oct 23 07:29:02 BST 2019
Num of Blocks: 1


## Check HDFS GUI

ssh -L '*:8084:Cadelicia:9870' Stevedore@Cadelicia

curl http://localhost:8084
...



## ---------------------------------------------------------------------- Setup Yarn -----------------------------------------------------------------------

"HDFS is a distributed storage system, and doesn’t provide any services for running and scheduling tasks in the cluster. This is the role of the YARN framework"

## Start Yarn

start-yarn.sh
# Starting resourcemanager
# Starting nodemanagers

yarn node -list

## Had to open some additional ports, and add some configuration parameters before I could get the additional worker nodes to show up

2019-10-23 14:11:24,504 INFO client.RMProxy: Connecting to ResourceManager at Cadelicia/192.168.201.11:8032
Total Nodes:2
         Node-Id	     Node-State	Node-Http-Address	Number-of-Running-Containers
  Froeseth:37629	        RUNNING	    Froeseth:8042	                           0
   Abecien:37539	        RUNNING	     Abecien:8042	                           0


## Submit a sample job

yarn jar ~/hadoop/	/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.2.jar wordcount "books/*" output



041_0005_000002. Got exception: java.net.NoRouteToHostException: No Route to Host from  Cadelicia/192.168.201.11 to Froeseth:37629 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor39.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy84.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:128)
	at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy85.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:126)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:311)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 19 more
. Failing the application.



## Looks like we may need to disable Firewall on worker nodes

ssh Stevedore@Froeseth
   sudo systemctl stop firewalld
exit

ssh Stevedore@Abecien
   sudo systemctl stop firewalld
exit


## Try again
## Stevedore@Cadelicia
yarn jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.2.jar wordcount "books/*" output

2019-10-23 15:57:51,682 INFO client.RMProxy: Connecting to ResourceManager at Cadelicia/192.168.201.11:8032
2019-10-23 15:57:52,344 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/Stevedore/.staging/job_1571833181041_0007
2019-10-23 15:57:52,542 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2019-10-23 15:57:52,792 INFO input.FileInputFormat: Total input files to process : 3
2019-10-23 15:57:52,891 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2019-10-23 15:57:53,000 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2019-10-23 15:57:53,034 INFO mapreduce.JobSubmitter: number of splits:3
2019-10-23 15:57:53,294 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2019-10-23 15:57:53,716 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1571833181041_0007
2019-10-23 15:57:53,716 INFO mapreduce.JobSubmitter: Executing with tokens: []
2019-10-23 15:57:54,081 INFO conf.Configuration: resource-types.xml not found
2019-10-23 15:57:54,082 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2019-10-23 15:57:54,195 INFO impl.YarnClientImpl: Submitted application application_1571833181041_0007
2019-10-23 15:57:54,256 INFO mapreduce.Job: The url to track the job: http://Cadelicia:8088/proxy/application_1571833181041_0007/
2019-10-23 15:57:54,257 INFO mapreduce.Job: Running job: job_1571833181041_0007
2019-10-23 15:57:55,275 INFO mapreduce.Job: Job job_1571833181041_0007 running in uber mode : false
2019-10-23 15:57:55,278 INFO mapreduce.Job:  map 0% reduce 0%
2019-10-23 15:57:55,307 INFO mapreduce.Job: Job job_1571833181041_0007 failed with state FAILED due to: Application application_1571833181041_0007 failed 2 times due to Error launching appattempt_1571833181041_0007_000002. Got exception: org.apache.hadoop.yarn.exceptions.YarnException: Unauthorized request to start container.
This token is expired. current time is 1571846320138 found 1571843274740
Note: System times on machines may be out of sync. Check system time and time zones.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.yarn.api.records.impl.pb.SerializedExceptionPBImpl.instantiateExceptionImpl(SerializedExceptionPBImpl.java:171)
	at org.apache.hadoop.yarn.api.records.impl.pb.SerializedExceptionPBImpl.instantiateException(SerializedExceptionPBImpl.java:182)
	at org.apache.hadoop.yarn.api.records.impl.pb.SerializedExceptionPBImpl.deSerialize(SerializedExceptionPBImpl.java:106)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:130)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:311)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
. Failing the application.
2019-10-23 15:57:55,384 INFO mapreduce.Job: Counters: 0



## Synchronize time on all 3 nodes and try again


yarn jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.2.jar wordcount "books/*" output

2019-10-23 17:22:23,806 INFO client.RMProxy: Connecting to ResourceManager at Cadelicia/192.168.201.11:8032
2019-10-23 17:22:24,382 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/Stevedore/.staging/job_1571833181041_0012
2019-10-23 17:22:24,567 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2019-10-23 17:22:24,793 INFO input.FileInputFormat: Total input files to process : 3
2019-10-23 17:22:24,905 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2019-10-23 17:22:25,018 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2019-10-23 17:22:25,035 INFO mapreduce.JobSubmitter: number of splits:3
2019-10-23 17:22:25,278 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2019-10-23 17:22:25,309 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1571833181041_0012
2019-10-23 17:22:25,310 INFO mapreduce.JobSubmitter: Executing with tokens: []
2019-10-23 17:22:25,548 INFO conf.Configuration: resource-types.xml not found
2019-10-23 17:22:25,549 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2019-10-23 17:22:25,655 INFO impl.YarnClientImpl: Submitted application application_1571833181041_0012
2019-10-23 17:22:25,737 INFO mapreduce.Job: The url to track the job: http://Cadelicia:8088/proxy/application_1571833181041_0012/
2019-10-23 17:22:25,738 INFO mapreduce.Job: Running job: job_1571833181041_0012
2019-10-23 17:22:34,955 INFO mapreduce.Job: Job job_1571833181041_0012 running in uber mode : false
2019-10-23 17:22:34,958 INFO mapreduce.Job:  map 0% reduce 0%
2019-10-23 17:22:42,087 INFO mapreduce.Job:  map 100% reduce 0%
2019-10-23 17:22:48,144 INFO mapreduce.Job:  map 100% reduce 100%
2019-10-23 17:22:49,173 INFO mapreduce.Job: Job job_1571833181041_0012 completed successfully
2019-10-23 17:22:49,346 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=486417
		FILE: Number of bytes written=1843683
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1232516
		HDFS: Number of bytes written=274568
		HDFS: Number of read operations=14
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=3
		Launched reduce tasks=1
		Data-local map tasks=3
		Total time spent by all maps in occupied slots (ms)=25840
		Total time spent by all reduces in occupied slots (ms)=7078
		Total time spent by all map tasks (ms)=12920
		Total time spent by all reduce tasks (ms)=3539
		Total vcore-milliseconds taken by all map tasks=12920
		Total vcore-milliseconds taken by all reduce tasks=3539
		Total megabyte-milliseconds taken by all map tasks=3307520
		Total megabyte-milliseconds taken by all reduce tasks=905984
	Map-Reduce Framework
		Map input records=23880
		Map output records=215164
		Map output bytes=2060450
		Map output materialized bytes=486429
		Input split bytes=350
		Combine input records=215164
		Combine output records=33442
		Reduce input groups=25011
		Reduce shuffle bytes=486429
		Reduce input records=33442
		Reduce output records=25011
		Spilled Records=66884
		Shuffled Maps =3
		Failed Shuffles=0
		Merged Map outputs=3
		GC time elapsed (ms)=324
		CPU time spent (ms)=6510
		Physical memory (bytes) snapshot=1027063808
		Virtual memory (bytes) snapshot=7807578112
		Total committed heap usage (bytes)=711458816
		Peak Map Physical memory (bytes)=283967488
		Peak Map Virtual memory (bytes)=1951109120
		Peak Reduce Physical memory (bytes)=176230400
		Peak Reduce Virtual memory (bytes)=1956368384
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=1232166
	File Output Format Counters 
		Bytes Written=274568


## Job Completed Successfully

## Check Results

hdfs dfs -ls output
Found 2 items
-rw-r--r--   1 Stevedore supergroup          0 2019-10-23 17:22 output/_SUCCESS
-rw-r--r--   1 Stevedore supergroup     274568 2019-10-23 17:22 output/part-r-00000

hdfs dfs -cat output/part-r-00000 | less



