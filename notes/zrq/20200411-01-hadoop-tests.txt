#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

# -----------------------------------------------------
# Create a container to work with.
# https://podman.readthedocs.io/en/latest/markdown/podman-run.1.html
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --env "clouduser=${AGLAIS_USER:?}" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --env "ANSIBLE_CODE=/mnt/ansible" \
        --volume "${AGLAIS_CODE:?}/experiments/zrq/ansible:/mnt/ansible:ro,z" \
        atolmis/ansible-client:latest \
        bash


# -----------------------------------------------------
# Create our Ansible include vars file.
#[root@ansibler]

    cat > /tmp/ansible-vars.yml << EOF
buildtag:  'aglais-$(date '+%Y%m%d')'
cloudname: '${cloudname}'
clouduser: '${clouduser}'
EOF


# -----------------------------------------------------
# Run the scripts from the ansible directory.
#[root@ansibler]

    # TODO Make this the working directory in the container ?
    # --env ANSIBLE_CODE=/mnt/ansible

    cd "${ANSIBLE_CODE:?}"


# -----------------------------------------------------
# Run the initial part of our deplyment.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-01.yml"

    >   ....
    >   ....


# -----------------------------------------------------
# Run the Hadoop part of our deplyment.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-02.yml"

    >   ....
    >   ....


# -----------------------------------------------------
# Format the HDFS NameNode on master01.
#[root@ansibler]

    ssh master01 \
        '
        hdfs namenode -format
        '

    >   2020-04-11 06:01:36,015 INFO namenode.NameNode: STARTUP_MSG: 
    >   /..
    >   STARTUP_MSG: Starting NameNode
    >   STARTUP_MSG:   host = aglais-20200411-master01.novalocal/10.10.0.19
    >   STARTUP_MSG:   args = [-format]
    >   STARTUP_MSG:   version = 3.2.1
    >   STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/....
    >   STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z
    >   STARTUP_MSG:   java = 13.0.2
    >   ************************************************************/
    >   2020-04-11 06:01:36,026 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
    >   2020-04-11 06:01:36,138 INFO namenode.NameNode: createNameNode [-format]
    >   2020-04-11 06:01:36,479 INFO common.Util: Assuming 'file' scheme for path /var/local/hadoop/namenode/fsimage in configuration.
    >   Formatting using clusterid: CID-1894d4d5-1949-4c21-a394-559ccc2c5f0a
    >   2020-04-11 06:01:36,479 INFO common.Util: Assuming 'file' scheme for path /var/local/hadoop/namenode/fsimage in configuration.
    >   2020-04-11 06:01:36,515 INFO namenode.FSEditLog: Edit logging is async:true
    >   2020-04-11 06:01:36,527 INFO namenode.FSNamesystem: KeyProvider: null
    >   2020-04-11 06:01:36,528 INFO namenode.FSNamesystem: fsLock is fair: true
    >   2020-04-11 06:01:36,528 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
    >   2020-04-11 06:01:36,558 INFO namenode.FSNamesystem: fsOwner             = fedora (auth:SIMPLE)
    >   2020-04-11 06:01:36,559 INFO namenode.FSNamesystem: supergroup          = supergroup
    >   2020-04-11 06:01:36,559 INFO namenode.FSNamesystem: isPermissionEnabled = true
    >   2020-04-11 06:01:36,559 INFO namenode.FSNamesystem: HA Enabled: false
    >   2020-04-11 06:01:36,602 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
    >   2020-04-11 06:01:36,612 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
    >   2020-04-11 06:01:36,612 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
    >   2020-04-11 06:01:36,617 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
    >   2020-04-11 06:01:36,617 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Apr 11 06:01:36
    >   2020-04-11 06:01:36,618 INFO util.GSet: Computing capacity for map BlocksMap
    >   2020-04-11 06:01:36,618 INFO util.GSet: VM type       = 64-bit
    >   2020-04-11 06:01:36,620 INFO util.GSet: 2.0% max memory 1.5 GB = 29.8 MB
    >   2020-04-11 06:01:36,620 INFO util.GSet: capacity      = 2^22 = 4194304 entries
    >   2020-04-11 06:01:36,639 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
    >   2020-04-11 06:01:36,639 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
    >   2020-04-11 06:01:36,646 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
    >   2020-04-11 06:01:36,646 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
    >   2020-04-11 06:01:36,646 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
    >   2020-04-11 06:01:36,646 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
    >   2020-04-11 06:01:36,646 INFO blockmanagement.BlockManager: defaultReplication         = 2
    >   2020-04-11 06:01:36,647 INFO blockmanagement.BlockManager: maxReplication             = 512
    >   2020-04-11 06:01:36,647 INFO blockmanagement.BlockManager: minReplication             = 1
    >   2020-04-11 06:01:36,647 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
    >   2020-04-11 06:01:36,647 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
    >   2020-04-11 06:01:36,647 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
    >   2020-04-11 06:01:36,647 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
    >   2020-04-11 06:01:36,665 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
    >   2020-04-11 06:01:36,665 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
    >   2020-04-11 06:01:36,665 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
    >   2020-04-11 06:01:36,665 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
    >   2020-04-11 06:01:36,676 INFO util.GSet: Computing capacity for map INodeMap
    >   2020-04-11 06:01:36,676 INFO util.GSet: VM type       = 64-bit
    >   2020-04-11 06:01:36,676 INFO util.GSet: 1.0% max memory 1.5 GB = 14.9 MB
    >   2020-04-11 06:01:36,676 INFO util.GSet: capacity      = 2^21 = 2097152 entries
    >   2020-04-11 06:01:36,682 INFO namenode.FSDirectory: ACLs enabled? false
    >   2020-04-11 06:01:36,682 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
    >   2020-04-11 06:01:36,682 INFO namenode.FSDirectory: XAttrs enabled? true
    >   2020-04-11 06:01:36,682 INFO namenode.NameNode: Caching file names occurring more than 10 times
    >   2020-04-11 06:01:36,685 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
    >   2020-04-11 06:01:36,687 INFO snapshot.SnapshotManager: SkipList is disabled
    >   2020-04-11 06:01:36,690 INFO util.GSet: Computing capacity for map cachedBlocks
    >   2020-04-11 06:01:36,690 INFO util.GSet: VM type       = 64-bit
    >   2020-04-11 06:01:36,691 INFO util.GSet: 0.25% max memory 1.5 GB = 3.7 MB
    >   2020-04-11 06:01:36,691 INFO util.GSet: capacity      = 2^19 = 524288 entries
    >   2020-04-11 06:01:36,697 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
    >   2020-04-11 06:01:36,698 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
    >   2020-04-11 06:01:36,698 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
    >   2020-04-11 06:01:36,700 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
    >   2020-04-11 06:01:36,701 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
    >   2020-04-11 06:01:36,702 INFO util.GSet: Computing capacity for map NameNodeRetryCache
    >   2020-04-11 06:01:36,702 INFO util.GSet: VM type       = 64-bit
    >   2020-04-11 06:01:36,702 INFO util.GSet: 0.029999999329447746% max memory 1.5 GB = 457.7 KB
    >   2020-04-11 06:01:36,702 INFO util.GSet: capacity      = 2^16 = 65536 entries
    >   2020-04-11 06:01:36,725 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1435812019-10.10.0.19-1586584896719
    >   2020-04-11 06:01:36,755 INFO common.Storage: Storage directory /var/local/hadoop/namenode/fsimage has been successfully formatted.
    >   2020-04-11 06:01:36,785 INFO namenode.FSImageFormatProtobuf: Saving image file /var/local/hadoop/namenode/fsimage/current/fsimage.ckpt_0000000000000000000 using no compression
    >   2020-04-11 06:01:36,862 INFO namenode.FSImageFormatProtobuf: Image file /var/local/hadoop/namenode/fsimage/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved in 0 seconds .
    >   2020-04-11 06:01:36,870 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
    >   2020-04-11 06:01:36,879 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
    >   2020-04-11 06:01:36,880 INFO namenode.NameNode: SHUTDOWN_MSG: 
    >   /..
    >   SHUTDOWN_MSG: Shutting down NameNode at aglais-20200411-master01.novalocal/10.10.0.19
    >   ************************************************************/


# -----------------------------------------------------
# Start the HDFS services.
#[root@ansibler]

    ssh master01 \
        '
        start-dfs.sh
        '

    >   Starting namenodes on [master01]
    >   Starting datanodes
    >   Starting secondary namenodes [aglais-20200411-master01.novalocal]
    >   aglais-20200411-master01.novalocal: Warning: Permanently added 'aglais-20200411-master01.novalocal,fe80::f816:3eff:fe29:c013%eth0' (ECDSA) to the list of known hosts.

    # TODO - secondary namenode on the same host ?
    # TODO - Move the secondary namenode to another host ..


# -----------------------------------------------------
# Check the HDFS status.
#[root@ansibler]

    ssh master01 \
        '
        hdfs dfsadmin -report
        '

    >   Configured Capacity: 4398046511104 (4 TB)
    >   Present Capacity: 4389384241152 (3.99 TB)
    >   DFS Remaining: 4389384224768 (3.99 TB)
    >   DFS Used: 16384 (16 KB)
    >   DFS Used%: 0.00%
    >   Replicated Blocks:
    >   	Under replicated blocks: 0
    >   	Blocks with corrupt replicas: 0
    >   	Missing blocks: 0
    >   	Missing blocks (with replication factor 1): 0
    >   	Low redundancy blocks with highest priority to recover: 0
    >   	Pending deletion blocks: 0
    >   Erasure Coded Block Groups: 
    >   	Low redundancy block groups: 0
    >   	Block groups with corrupt internal blocks: 0
    >   	Missing block groups: 0
    >   	Low redundancy blocks with highest priority to recover: 0
    >   	Pending deletion blocks: 0
    >   
    >   -------------------------------------------------
    >   Live datanodes (4):
    >   
    >   Name: 10.10.0.25:9866 (worker01)
    >   Hostname: worker01
    >   Decommission Status : Normal
    >   Configured Capacity: 1099511627776 (1 TB)
    >   DFS Used: 4096 (4 KB)
    >   Non DFS Used: 17297408 (16.50 MB)
    >   DFS Remaining: 1097346056192 (1021.98 GB)
    >   DFS Used%: 0.00%
    >   DFS Remaining%: 99.80%
    >   Configured Cache Capacity: 0 (0 B)
    >   Cache Used: 0 (0 B)
    >   Cache Remaining: 0 (0 B)
    >   Cache Used%: 100.00%
    >   Cache Remaining%: 0.00%
    >   Xceivers: 1
    >   Last contact: Sat Apr 11 06:06:03 UTC 2020
    >   Last Block Report: Sat Apr 11 06:05:06 UTC 2020
    >   Num of Blocks: 0
    >   
    >   
    >   Name: 10.10.0.29:9866 (worker03)
    >   Hostname: worker03
    >   Decommission Status : Normal
    >   Configured Capacity: 1099511627776 (1 TB)
    >   DFS Used: 4096 (4 KB)
    >   Non DFS Used: 17297408 (16.50 MB)
    >   DFS Remaining: 1097346056192 (1021.98 GB)
    >   DFS Used%: 0.00%
    >   DFS Remaining%: 99.80%
    >   Configured Cache Capacity: 0 (0 B)
    >   Cache Used: 0 (0 B)
    >   Cache Remaining: 0 (0 B)
    >   Cache Used%: 100.00%
    >   Cache Remaining%: 0.00%
    >   Xceivers: 1
    >   Last contact: Sat Apr 11 06:06:03 UTC 2020
    >   Last Block Report: Sat Apr 11 06:05:06 UTC 2020
    >   Num of Blocks: 0
    >   
    >   
    >   Name: 10.10.0.4:9866 (worker02)
    >   Hostname: worker02
    >   Decommission Status : Normal
    >   Configured Capacity: 1099511627776 (1 TB)
    >   DFS Used: 4096 (4 KB)
    >   Non DFS Used: 17297408 (16.50 MB)
    >   DFS Remaining: 1097346056192 (1021.98 GB)
    >   DFS Used%: 0.00%
    >   DFS Remaining%: 99.80%
    >   Configured Cache Capacity: 0 (0 B)
    >   Cache Used: 0 (0 B)
    >   Cache Remaining: 0 (0 B)
    >   Cache Used%: 100.00%
    >   Cache Remaining%: 0.00%
    >   Xceivers: 1
    >   Last contact: Sat Apr 11 06:06:03 UTC 2020
    >   Last Block Report: Sat Apr 11 06:05:06 UTC 2020
    >   Num of Blocks: 0
    >   
    >   
    >   Name: 10.10.0.6:9866 (worker04)
    >   Hostname: worker04
    >   Decommission Status : Normal
    >   Configured Capacity: 1099511627776 (1 TB)
    >   DFS Used: 4096 (4 KB)
    >   Non DFS Used: 17297408 (16.50 MB)
    >   DFS Remaining: 1097346056192 (1021.98 GB)
    >   DFS Used%: 0.00%
    >   DFS Remaining%: 99.80%
    >   Configured Cache Capacity: 0 (0 B)
    >   Cache Used: 0 (0 B)
    >   Cache Remaining: 0 (0 B)
    >   Cache Used%: 100.00%
    >   Cache Remaining%: 0.00%
    >   Xceivers: 1
    >   Last contact: Sat Apr 11 06:06:03 UTC 2020
    >   Last Block Report: Sat Apr 11 06:05:06 UTC 2020
    >   Num of Blocks: 0


# -----------------------------------------------------
# -----------------------------------------------------
# Tail the logs (separate terminals).
#[user@desktop]


    podman exec -it $(
        podman ps --filter 'name=ansibler' --format "{{.ID}}"
        ) \
        ssh master01 \
            '
            tail -f /var/local/hadoop/logs/hadoop-fedora-namenode-$(hostname).log
            '

    >   ....
    >   2020-04-11 06:05:06,146 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 93ef822c-2028-4b31-9fbc-c392bcc62576 (10.10.0.6:9866).
    >   2020-04-11 06:05:06,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-25b0108d-414f-40b3-9a87-0262c3850155 for DN 10.10.0.6:9866
    >   2020-04-11 06:05:06,182 INFO BlockStateChange: BLOCK* processReport 0xab9a08a357b20fc4: Processing first storage report for DS-0b65e482-6c56-4270-8925-395ccd629c2a from datanode 68a1bdc5-5c07-4e4e-8714-6d8545a70895
    >   2020-04-11 06:05:06,187 INFO BlockStateChange: BLOCK* processReport 0xab9a08a357b20fc4: from storage DS-0b65e482-6c56-4270-8925-395ccd629c2a node DatanodeRegistration(10.10.0.29:9866, datanodeUuid=68a1bdc5-5c07-4e4e-8714-6d8545a70895, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1894d4d5-1949-4c21-a394-559ccc2c5f0a;nsid=764607163;c=1586584896719), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
    >   2020-04-11 06:05:06,187 INFO BlockStateChange: BLOCK* processReport 0x392e2bbdbd118d02: Processing first storage report for DS-2645a5fe-b95e-42a5-a7ef-8a1109b58f23 from datanode a6ec9264-1050-480e-959e-9234570ba488
    >   2020-04-11 06:05:06,188 INFO BlockStateChange: BLOCK* processReport 0x392e2bbdbd118d02: from storage DS-2645a5fe-b95e-42a5-a7ef-8a1109b58f23 node DatanodeRegistration(10.10.0.25:9866, datanodeUuid=a6ec9264-1050-480e-959e-9234570ba488, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1894d4d5-1949-4c21-a394-559ccc2c5f0a;nsid=764607163;c=1586584896719), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
    >   2020-04-11 06:05:06,188 INFO BlockStateChange: BLOCK* processReport 0x838942d2c2e97e01: Processing first storage report for DS-26a8671f-c859-4e62-9a2d-e6b52ad78507 from datanode 49a657ee-27c9-49ab-9af6-6b05b8fce8ea
    >   2020-04-11 06:05:06,188 INFO BlockStateChange: BLOCK* processReport 0x838942d2c2e97e01: from storage DS-26a8671f-c859-4e62-9a2d-e6b52ad78507 node DatanodeRegistration(10.10.0.4:9866, datanodeUuid=49a657ee-27c9-49ab-9af6-6b05b8fce8ea, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1894d4d5-1949-4c21-a394-559ccc2c5f0a;nsid=764607163;c=1586584896719), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
    >   2020-04-11 06:05:06,196 INFO BlockStateChange: BLOCK* processReport 0xb73bbd2a3d5884e7: Processing first storage report for DS-25b0108d-414f-40b3-9a87-0262c3850155 from datanode 93ef822c-2028-4b31-9fbc-c392bcc62576
    >   2020-04-11 06:05:06,196 INFO BlockStateChange: BLOCK* processReport 0xb73bbd2a3d5884e7: from storage DS-25b0108d-414f-40b3-9a87-0262c3850155 node DatanodeRegistration(10.10.0.6:9866, datanodeUuid=93ef822c-2028-4b31-9fbc-c392bcc62576, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1894d4d5-1949-4c21-a394-559ccc2c5f0a;nsid=764607163;c=1586584896719), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
    >   ....


    podman exec -it $(
        podman ps --filter 'name=ansibler' --format "{{.ID}}"
        ) \
        ssh master01 \
            '
            tail -f /var/local/hadoop/logs/hadoop-fedora-secondarynamenode-$(hostname).log
            '

    >   ....
    >   2020-04-11 23:49:17,874 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-fedora/dfs/namesecondary/current/edits_0000000000000000035-0000000000000000036 expecting start txid #35
    >   2020-04-11 23:49:17,874 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-fedora/dfs/namesecondary/current/edits_0000000000000000035-0000000000000000036 maxTxnsToRead = 9223372036854775807
    >   2020-04-11 23:49:17,875 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded 1 edits file(s) (the last named /tmp/hadoop-fedora/dfs/namesecondary/current/edits_0000000000000000035-0000000000000000036) of total size 42.0, total edits 2.0, total load time 0.0 ms
    >   2020-04-11 23:49:17,890 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-fedora/dfs/namesecondary/current/fsimage.ckpt_0000000000000000036 using no compression
    >   2020-04-11 23:49:17,899 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-fedora/dfs/namesecondary/current/fsimage.ckpt_0000000000000000036 of size 401 bytes saved in 0 seconds .
    >   2020-04-11 23:49:17,901 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 34
    >   2020-04-11 23:49:17,901 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-fedora/dfs/namesecondary/current/fsimage_0000000000000000032, cpktTxId=0000000000000000032)
    >   2020-04-11 23:49:17,904 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Sending fileName: /tmp/hadoop-fedora/dfs/namesecondary/current/fsimage_0000000000000000036, fileSize: 401. Sent total: 401 bytes. Size of last segment intended to send: -1 bytes.
    >   2020-04-11 23:49:17,916 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 36 to namenode at http://master01:9870 in 0.013 seconds
    >   2020-04-11 23:49:17,916 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 401
    >   ....


    podman exec -it $(
        podman ps --filter 'name=ansibler' --format "{{.ID}}"
        ) \
        ssh worker01 \
            '
            tail -f /var/local/hadoop/logs/hadoop-fedora-datanode-$(hostname).log
            '

    >   ....
    >   2020-04-11 06:05:05,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-1435812019-10.10.0.19-1586584896719: 3ms
    >   2020-04-11 06:05:05,996 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-1435812019-10.10.0.19-1586584896719 on volume /data-01/hdfs/data
    >   2020-04-11 06:05:05,997 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/data-01/hdfs/data, DS-2645a5fe-b95e-42a5-a7ef-8a1109b58f23): finished scanning block pool BP-1435812019-10.10.0.19-1586584896719
    >   2020-04-11 06:05:06,018 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 4/11/20, 9:35 AM with interval of 21600000ms
    >   2020-04-11 06:05:06,019 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/data-01/hdfs/data, DS-2645a5fe-b95e-42a5-a7ef-8a1109b58f23): no suitable block pools found to scan.  Waiting 1814399977 ms.
    >   2020-04-11 06:05:06,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1435812019-10.10.0.19-1586584896719 (Datanode Uuid a6ec9264-1050-480e-959e-9234570ba488) service to master01/10.10.0.19:9000 beginning handshake with NN
    >   2020-04-11 06:05:06,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1435812019-10.10.0.19-1586584896719 (Datanode Uuid a6ec9264-1050-480e-959e-9234570ba488) service to master01/10.10.0.19:9000 successfully registered with NN
    >   2020-04-11 06:05:06,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode master01/10.10.0.19:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
    >   2020-04-11 06:05:06,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x392e2bbdbd118d02,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 71 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
    >   2020-04-11 06:05:06,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1435812019-10.10.0.19-1586584896719
    >   ....


    podman exec -it $(
        podman ps --filter 'name=ansibler' --format "{{.ID}}"
        ) \
        ssh worker02 \
            '
            tail -f /var/local/hadoop/logs/hadoop-fedora-datanode-$(hostname).log
            '

    >   ....
    >   2020-04-11 06:05:06,005 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-1435812019-10.10.0.19-1586584896719: 6ms
    >   2020-04-11 06:05:06,008 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-1435812019-10.10.0.19-1586584896719 on volume /data-01/hdfs/data
    >   2020-04-11 06:05:06,009 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/data-01/hdfs/data, DS-26a8671f-c859-4e62-9a2d-e6b52ad78507): finished scanning block pool BP-1435812019-10.10.0.19-1586584896719
    >   2020-04-11 06:05:06,038 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 4/11/20, 8:32 AM with interval of 21600000ms
    >   2020-04-11 06:05:06,039 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/data-01/hdfs/data, DS-26a8671f-c859-4e62-9a2d-e6b52ad78507): no suitable block pools found to scan.  Waiting 1814399967 ms.
    >   2020-04-11 06:05:06,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1435812019-10.10.0.19-1586584896719 (Datanode Uuid 49a657ee-27c9-49ab-9af6-6b05b8fce8ea) service to master01/10.10.0.19:9000 beginning handshake with NN
    >   2020-04-11 06:05:06,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1435812019-10.10.0.19-1586584896719 (Datanode Uuid 49a657ee-27c9-49ab-9af6-6b05b8fce8ea) service to master01/10.10.0.19:9000 successfully registered with NN
    >   2020-04-11 06:05:06,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode master01/10.10.0.19:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
    >   2020-04-11 06:05:06,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x838942d2c2e97e01,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 74 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
    >   2020-04-11 06:05:06,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1435812019-10.10.0.19-1586584896719
    >   ....


    #
    # Test the HDFS system ...
    #


# -----------------------------------------------------
# Start the YARN services.
#[root@ansibler]

    ssh master01 \
        '
        start-yarn.sh
        '

    >   Starting resourcemanager
    >   Starting nodemanagers


# -----------------------------------------------------
# -----------------------------------------------------
# Tail the logs (separate terminals).
#[user@desktop]


    podman exec -it $(
        podman ps --filter 'name=ansibler' --format "{{.ID}}"
        ) \
        ssh master01 \
            '
            tail -f /var/local/hadoop/logs/hadoop-fedora-resourcemanager-$(hostname).log
            '

    >   ....
    >   	... 49 more
    >   Caused by: java.lang.ClassNotFoundException: javax.activation.DataSource
    >   	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:602)
    >   	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
    >   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)
    >   	... 83 more
    >   2020-04-12 00:54:35,764 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: SHUTDOWN_MSG: 
    >   /..
    >   SHUTDOWN_MSG: Shutting down ResourceManager at aglais-20200411-master01.novalocal/10.10.0.19
    >   ************************************************************/


    podman exec -it $(
        podman ps --filter 'name=ansibler' --format "{{.ID}}"
        ) \
        ssh worker01 \
            '
            tail -f /var/local/hadoop/logs/hadoop-fedora-nodemanager-$(hostname).log
            '

    >   ....
    >   	... 52 more
    >   Caused by: java.lang.ClassNotFoundException: javax.activation.DataSource
    >   	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:602)
    >   	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
    >   	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)
    >   	... 86 more
    >   2020-04-12 00:54:38,337 INFO org.apache.hadoop.yarn.server.nodemanager.NodeManager: SHUTDOWN_MSG: 
    >   /..
    >   SHUTDOWN_MSG: Shutting down NodeManager at aglais-20200411-worker01.novalocal/10.10.0.25
    >   ************************************************************/

    #
    # Found matching issue in Hadoop Jira.
    # Hadoop code not compatible with Java 9.
    # https://issues.apache.org/jira/browse/HADOOP-14978?focusedCommentId=16619981#comment-16619981
    #

    #
    # Java modules - YANT, yet another new thing
    # http://tutorials.jenkov.com/java/modules.html
    # https://www.oracle.com/corporate/features/understanding-java-9-modules.html
    
    # 
    # Added fixes to [{{hdhome}}/etc/hadoop/yarn-env.sh]

        +   # https://issues.apache.org/jira/browse/HADOOP-14978?focusedCommentId=16619981#comment-16619981
        +   export YARN_RESOURCEMANAGER_OPTS="--add-modules java.activation"

        +   # https://issues.apache.org/jira/browse/HADOOP-14978?focusedCommentId=16619981#comment-16619981
        +   export YARN_NODEMANAGER_OPTS="--add-modules java.activation"


# -----------------------------------------------------
# Start the YARN services.
#[root@ansibler]

    ssh master01 \
        '
        start-yarn.sh
        '

    >   Starting resourcemanager
    >   ERROR: Cannot set priority of resourcemanager process 15644
    >   Starting nodemanagers
    >   worker01: ERROR: Cannot set priority of nodemanager process 12639
    >   worker02: ERROR: Cannot set priority of nodemanager process 12630
    >   worker04: ERROR: Cannot set priority of nodemanager process 12621
    >   worker03: ERROR: Cannot set priority of nodemanager process 12620


# -----------------------------------------------------
# Start the YARN services as root.
#[root@ansibler]

    ssh root@master01 \
        '
        start-yarn.sh
        '

    >   Please login as the user "fedora" rather than the user "root".


# -----------------------------------------------------
# Start the YARN services as root.
#[root@ansibler]

    ssh master01

        sudo su
        
            start-yarn.sh


    >   Starting resourcemanager
    >   ERROR: Attempting to operate on yarn resourcemanager as root
    >   ERROR: but there is no YARN_RESOURCEMANAGER_USER defined. Aborting operation.
    >   Starting nodemanagers
    >   ERROR: Attempting to operate on yarn nodemanager as root
    >   ERROR: but there is no YARN_NODEMANAGER_USER defined. Aborting operation.

    # 
    # We should have separate users accounts for hdfs, yarn etc.
    # Added temp fixes to [{{hdhome}}/etc/hadoop/yarn-env.sh]

        +   export YARN_RESOURCEMANAGER_USER=root
        +   export YARN_NODEMANAGER_USER=root


# -----------------------------------------------------
# Start the YARN services.
#[root@ansibler]

    ssh master01 \
        '
        start-yarn.sh
        '

    >   Starting resourcemanager
    >   ERROR: resourcemanager can only be executed by root.
    >   Starting nodemanagers
    >   worker03: ERROR: Cannot set priority of nodemanager process 12829
    >   worker01: ERROR: Cannot set priority of nodemanager process 12848
    >   worker02: ERROR: Cannot set priority of nodemanager process 12839
    >   worker04: ERROR: Cannot set priority of nodemanager process 12829


# -----------------------------------------------------
# Start the YARN services as root.
#[root@ansibler]

    ssh master01

        sudo su

            start-yarn.sh


    >   Starting resourcemanager
    >   Last login: Sun Apr 12 01:57:52 UTC 2020 on pts/1
    >   ERROR: Cannot set priority of resourcemanager process 17212
    >   Starting nodemanagers
    >   ERROR: Attempting to operate on yarn nodemanager as root
    >   ERROR: but there is no YARN_NODEMANAGER_USER defined. Aborting operation.


# -----------------------------------------------------
# Try using the existing user rather than root.
#[root@ansibler]

    # Added temp fixes to [{{hdhome}}/etc/hadoop/yarn-env.sh]

        +   export YARN_RESOURCEMANAGER_USER=fedora
        +   export YARN_NODEMANAGER_USER=fedora


    ssh master01 \
        '
        start-yarn.sh
        '

    >   Starting resourcemanager
    >   ERROR: Cannot set priority of resourcemanager process 17834
    >   Starting nodemanagers
    >   worker04: ERROR: Cannot set priority of nodemanager process 13165
    >   worker03: ERROR: Cannot set priority of nodemanager process 13165
    >   worker02: ERROR: Cannot set priority of nodemanager process 13178
    >   worker01: ERROR: Cannot set priority of nodemanager process 13187


# -----------------------------------------------------
# Found another clue ..
#[root@ansibler]

    ssh master01 \
        '
        cat /var/local/hadoop/logs/hadoop-fedora-resourcemanager-$(hostname).out
        '

    >   Error occurred during initialization of boot layer
    >   java.lang.module.FindException: Module java.activation not found
    >   ....
    >   ....
    
    #
    # Matches this blog post ..
    # https://medium.com/@gautambangalore/resolved-error-occurred-during-initialization-of-boot-layer-java-lang-module-findexception-fe75dc80ee7f


# -----------------------------------------------------
# Try installing the jar ..
#[root@ansibler]

    ssh master01 \
        '
        wget https://repo1.maven.org/maven2/javax/activation/activation/1.1/activation-1.1.jar

        pushd /opt/hadoop/share/hadoop/common/lib/
            ln -sf ${HOME}/activation-1.1.jar
        popd
        
        pushd /opt/hadoop/share/hadoop/yarn/lib/
            ln -sf ${HOME}/activation-1.1.jar
        popd

        pushd /opt/hadoop/share/hadoop/yarn/
            ln -sf ${HOME}/activation-1.1.jar
        popd


# -----------------------------------------------------
# Try again ....
#[root@ansibler]

    ssh master01 \
        '
        start-yarn.sh
        '

    >   Starting resourcemanager
    >   ERROR: Cannot set priority of resourcemanager process 18711
    >   Starting nodemanagers
    >   worker04: ERROR: Cannot set priority of nodemanager process 13316
    >   worker01: ERROR: Cannot set priority of nodemanager process 13334
    >   worker03: ERROR: Cannot set priority of nodemanager process 13315
    >   worker02: ERROR: Cannot set priority of nodemanager process 13326


# -----------------------------------------------------
# Found another clue ..
#[root@ansibler]

    ssh master01 \
        '
        cat /var/local/hadoop/logs/hadoop-fedora-resourcemanager-$(hostname).out
        '

    >   Error occurred during initialization of boot layer
    >   java.lang.module.FindException: Module java.activation not found
    >   ....

    #
    # OK - uphill fight here.
    # Why not downgrade to Java 8 and get it working that way ?
    #

# -----------------------------------------------------
# -----------------------------------------------------

    #
    # Change to Java 1.8.0


    vi experiments/zrq/ansible/10-install-java.yml    

        - name: "Install Java"
          become: true
          dnf:
    -       name:  'java-latest-openjdk-headless'
    +       name:  'java-1.8.0-openjdk-headless'
            state: present



# -----------------------------------------------------
# Run the initial part of our deplyment.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-01.yml"

    >   ....
    >   ....


# -----------------------------------------------------
# Run the Hadoop part of our deplyment.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-02.yml"

    >   ....
    >   ....


# -----------------------------------------------------
# Format the HDFS NameNode on master01.
#[root@ansibler]

    ssh master01 \
        '
        hdfs namenode -format
        '


    >   ....
    >   ....


# -----------------------------------------------------
# Start the YARN services.
#[root@ansibler]

    ssh master01 \
        '
        start-yarn.sh
        '

    >   Starting resourcemanager
    >   Starting nodemanagers


# -----------------------------------------------------
# -----------------------------------------------------
# Tail the logs (separate terminals).
#[user@desktop]


    podman exec -it $(
        podman ps --filter 'name=ansibler' --format "{{.ID}}"
        ) \
        ssh master01 \
            '
            tail -f /var/local/hadoop/logs/hadoop-fedora-resourcemanager-$(hostname).log
            '

    >   ....
    >   2020-04-12 03:02:42,923 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node worker01(cmPort: 42645 httpPort: 8042) registered with capability: <memory:15000, vCores:8>, assigned nodeId worker01:42645
    >   2020-04-12 03:02:42,926 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node worker03(cmPort: 45429 httpPort: 8042) registered with capability: <memory:15000, vCores:8>, assigned nodeId worker03:45429
    >   2020-04-12 03:02:42,932 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: worker04:41869 Node Transitioned from NEW to RUNNING
    >   2020-04-12 03:02:42,932 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: worker02:46449 Node Transitioned from NEW to RUNNING
    >   2020-04-12 03:02:42,932 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: worker01:42645 Node Transitioned from NEW to RUNNING
    >   2020-04-12 03:02:42,932 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: worker03:45429 Node Transitioned from NEW to RUNNING
    >   2020-04-12 03:02:42,952 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node worker04:41869 clusterResource: <memory:15000, vCores:8>
    >   2020-04-12 03:02:42,954 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node worker02:46449 clusterResource: <memory:30000, vCores:16>
    >   2020-04-12 03:02:42,955 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node worker01:42645 clusterResource: <memory:45000, vCores:24>
    >   2020-04-12 03:02:42,957 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node worker03:45429 clusterResource: <memory:60000, vCores:32>
    >   ....


    podman exec -it $(
        podman ps --filter 'name=ansibler' --format "{{.ID}}"
        ) \
        ssh worker01 \
            '
            tail -f /var/local/hadoop/logs/hadoop-fedora-nodemanager-$(hostname).log
            '

    >   ....
    >   2020-04-12 03:02:42,628 INFO org.eclipse.jetty.server.Server: Started @2574ms
    >   2020-04-12 03:02:42,628 INFO org.apache.hadoop.yarn.webapp.WebApps: Web app node started at 8042
    >   2020-04-12 03:02:42,629 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : worker01:42645
    >   2020-04-12 03:02:42,630 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
    >   2020-04-12 03:02:42,634 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at master01/10.10.0.29:8031
    >   2020-04-12 03:02:42,667 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []
    >   2020-04-12 03:02:42,675 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]
    >   2020-04-12 03:02:42,940 INFO org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id -1702980134
    >   2020-04-12 03:02:42,941 INFO org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 890273289
    >   2020-04-12 03:02:42,942 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as worker01:42645 with total resource of <memory:15000, vCores:8>
    >   ....



# -----------------------------------------------------
# Start the HDFS services.
#[root@ansibler]

    ssh master01 \
        '
        start-dfs.sh
        '

    >   Starting namenodes on [master01]
    >   Starting datanodes
    >   Starting secondary namenodes [aglais-20200411-master01.novalocal]
    >   aglais-20200411-master01.novalocal: Warning: Permanently added 'aglais-20200411-master01.novalocal,fe80::f816:3eff:fedb:dfd8%eth0' (ECDSA) to the list of known hosts.

    #
    # TODO - push the secondary namenode onto a different host.
    #


# -----------------------------------------------------
# -----------------------------------------------------
# Tail the logs (separate terminals).
#[user@desktop]


    podman exec -it $(
        podman ps --filter 'name=ansibler' --format "{{.ID}}"
        ) \
        ssh master01 \
            '
            tail -f /var/local/hadoop/logs/hadoop-fedora-namenode-$(hostname).log
            '

    >   ....
    >   2020-04-12 03:05:32,795 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-dc40c137-4e3f-4391-88e7-d9f97163f99f for DN 10.10.0.13:9866
    >   2020-04-12 03:05:32,795 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-5a20b877-25c3-456f-9915-d37d6762fef5 for DN 10.10.0.14:9866
    >   2020-04-12 03:05:32,833 INFO BlockStateChange: BLOCK* processReport 0xd13d17294b2323ff: Processing first storage report for DS-5a20b877-25c3-456f-9915-d37d6762fef5 from datanode 5792cd01-f6d6-4e2d-b64e-1286f53179f6
    >   2020-04-12 03:05:32,834 INFO BlockStateChange: BLOCK* processReport 0xd13d17294b2323ff: from storage DS-5a20b877-25c3-456f-9915-d37d6762fef5 node DatanodeRegistration(10.10.0.14:9866, datanodeUuid=5792cd01-f6d6-4e2d-b64e-1286f53179f6, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-e39db86d-0e91-4d8c-8616-93af3394d503;nsid=786528158;c=1586660450249), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
    >   2020-04-12 03:05:32,834 INFO BlockStateChange: BLOCK* processReport 0x6edc8d8a19692e51: Processing first storage report for DS-bb1e4c10-3502-4da1-9b87-c16cb62985e2 from datanode 025d03bb-3df5-43c2-b3ec-5991401b190c
    >   2020-04-12 03:05:32,834 INFO BlockStateChange: BLOCK* processReport 0x6edc8d8a19692e51: from storage DS-bb1e4c10-3502-4da1-9b87-c16cb62985e2 node DatanodeRegistration(10.10.0.21:9866, datanodeUuid=025d03bb-3df5-43c2-b3ec-5991401b190c, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-e39db86d-0e91-4d8c-8616-93af3394d503;nsid=786528158;c=1586660450249), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
    >   2020-04-12 03:05:32,843 INFO BlockStateChange: BLOCK* processReport 0x32f9e010aa743653: Processing first storage report for DS-dc40c137-4e3f-4391-88e7-d9f97163f99f from datanode 5f1f7069-3a13-44e3-b1a8-c94c594a8951
    >   2020-04-12 03:05:32,843 INFO BlockStateChange: BLOCK* processReport 0x32f9e010aa743653: from storage DS-dc40c137-4e3f-4391-88e7-d9f97163f99f node DatanodeRegistration(10.10.0.13:9866, datanodeUuid=5f1f7069-3a13-44e3-b1a8-c94c594a8951, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-e39db86d-0e91-4d8c-8616-93af3394d503;nsid=786528158;c=1586660450249), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
    >   2020-04-12 03:05:32,850 INFO BlockStateChange: BLOCK* processReport 0xf5e0816a773a483c: Processing first storage report for DS-5b10550a-200b-4d43-afff-a81b04f7000b from datanode 6dbbee6d-1089-4867-80e3-e6f4c4e6e18c
    >   2020-04-12 03:05:32,850 INFO BlockStateChange: BLOCK* processReport 0xf5e0816a773a483c: from storage DS-5b10550a-200b-4d43-afff-a81b04f7000b node DatanodeRegistration(10.10.0.25:9866, datanodeUuid=6dbbee6d-1089-4867-80e3-e6f4c4e6e18c, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-e39db86d-0e91-4d8c-8616-93af3394d503;nsid=786528158;c=1586660450249), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
    >   ....


    podman exec -it $(
        podman ps --filter 'name=ansibler' --format "{{.ID}}"
        ) \
        ssh worker02 \
            '
            tail -f /var/local/hadoop/logs/hadoop-fedora-datanode-$(hostname).log
            '

    >   ....
    >   2020-04-12 03:05:32,678 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-296307707-10.10.0.29-1586660450249: 2ms
    >   2020-04-12 03:05:32,679 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-296307707-10.10.0.29-1586660450249 on volume /data-01/hdfs/data
    >   2020-04-12 03:05:32,681 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/data-01/hdfs/data, DS-dc40c137-4e3f-4391-88e7-d9f97163f99f): finished scanning block pool BP-296307707-10.10.0.29-1586660450249
    >   2020-04-12 03:05:32,694 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 4/12/20 3:21 AM with interval of 21600000ms
    >   2020-04-12 03:05:32,696 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/data-01/hdfs/data, DS-dc40c137-4e3f-4391-88e7-d9f97163f99f): no suitable block pools found to scan.  Waiting 1814399983 ms.
    >   2020-04-12 03:05:32,699 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-296307707-10.10.0.29-1586660450249 (Datanode Uuid 5f1f7069-3a13-44e3-b1a8-c94c594a8951) service to master01/10.10.0.29:9000 beginning handshake with NN
    >   2020-04-12 03:05:32,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-296307707-10.10.0.29-1586660450249 (Datanode Uuid 5f1f7069-3a13-44e3-b1a8-c94c594a8951) service to master01/10.10.0.29:9000 successfully registered with NN
    >   2020-04-12 03:05:32,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode master01/10.10.0.29:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
    >   2020-04-12 03:05:32,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x32f9e010aa743653,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 69 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
    >   2020-04-12 03:05:32,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-296307707-10.10.0.29-1586660450249
    >   ....


# -----------------------------------------------------


