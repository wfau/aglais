#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    Target:

        Run the Spark and Zeppelin deploy.

    Result:



# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --env "clouduser=${AGLAIS_USER:?}" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --env "ANSIBLE_CODE=/mnt/ansible" \
        --volume "${AGLAIS_CODE:?}/experiments/hadoop-yarn/ansible:/mnt/ansible:ro,z" \
        atolmis/ansible-client:latest \
        bash


# -----------------------------------------------------
# Create our Ansible include vars file.
#[root@ansibler]

    cat > /tmp/ansible-vars.yml << EOF
buildtag:  'aglais-$(date '+%Y%m%d')'
cloudname: '${cloudname}'
clouduser: '${clouduser}'
EOF


# -----------------------------------------------------
# Run the scripts from the ansible directory.
#[root@ansibler]

    pushd "${ANSIBLE_CODE:?}"

# -----------------------------------------------------
# Create our client ssh config.
#[root@ansibler]

    ansible-playbook \
        --inventory 'hosts.yml' \
        '05-config-ssh.yml'

    ansible-playbook \
        --inventory 'hosts.yml' \
        '08-ping-test.yml'


# -----------------------------------------------------
# Run the Spark and Zeppelin deploy.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-02.yml"

    >   ....
    >   ....
    >   PLAY RECAP ..
    >   localhost                  : ok=39   changed=23   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   master01                   : ok=24   changed=18   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   master02                   : ok=19   changed=13   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker01                   : ok=22   changed=19   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker02                   : ok=22   changed=19   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker03                   : ok=22   changed=19   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker04                   : ok=22   changed=19   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker05                   : ok=22   changed=19   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker06                   : ok=22   changed=19   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker07                   : ok=22   changed=19   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker08                   : ok=22   changed=19   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0


    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-03.yml"

    >   ....
    >   ....
    >   PLAY RECAP ..
    >   localhost                  : ok=6    changed=6    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   master01                   : ok=8    changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   master02                   : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker01                   : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker02                   : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker03                   : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker04                   : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker05                   : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker06                   : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker07                   : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker08                   : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0


# -----------------------------------------------------
# Run a PySpark example ....
# https://www.tutorialspoint.com/pyspark/pyspark_sparkcontext.htm
#[root@ansibler]

    ssh master01

        pyspark

    >   Python 3.7.3 (default, Mar 27 2019, 13:36:35)
    >   [GCC 9.0.1 20190227 (Red Hat 9.0.1-0.8)] on linux
    >   Type "help", "copyright", "credits" or "license" for more information.
    >   2020-11-16 17:53:11,621 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    >   Setting default log level to "WARN".
    >   To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
    >   ....
    >   ....

    # Hangs ...
    # So probably not healthy ...
    # Ctrl^C to interrupt.

    >   ....
    >   ....
    >   ^CTraceback (most recent call last):
    >     File "/opt/spark/python/pyspark/shell.py", line 41, in <module>
    >       spark = SparkSession._create_shell_session()
    >     File "/opt/spark/python/pyspark/sql/session.py", line 482, in _create_shell_session
    >       return SparkSession.builder\
    >     File "/opt/spark/python/pyspark/sql/session.py", line 186, in getOrCreate
    >       sc = SparkContext.getOrCreate(sparkConf)
    >     File "/opt/spark/python/pyspark/context.py", line 376, in getOrCreate
    >       SparkContext(conf=conf or SparkConf())
    >     File "/opt/spark/python/pyspark/context.py", line 136, in __init__
    >       conf, jsc, profiler_cls)
    >     File "/opt/spark/python/pyspark/context.py", line 198, in _do_init
    >       self._jsc = jsc or self._initialize_context(self._conf._jconf)
    >     File "/opt/spark/python/pyspark/context.py", line 315, in _initialize_context
    >       return self._jvm.JavaSparkContext(jconf)
    >     File "/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1567, in __call__
    >     File "/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1033, in send_command
    >     File "/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1200, in send_command
    >     File "/usr/lib64/python3.7/socket.py", line 589, in readinto
    >       return self._sock.recv_into(b)
    >   KeyboardInterrupt
    >   >>>

    #
    # Are the problems due to updating the Spark version, or the deployment?
    # Does the extra router mess up the networking ?
    # TODO need to test this further.
    #
    # Update : Not a problem, just needed to start the Yarn services
    #


