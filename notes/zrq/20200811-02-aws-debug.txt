#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#





# -----------------------------------------------------

    # Deleted old cluster.
    # notes/zrq/20200807-04-openstack-delete.txt

    # Created new cluster.
    # notes/zrq/20200807-05-terraform-create.txt

    # Installed NGINX ingress controller.
    # notes/zrq/20200807-06-nginx-ingress.txt

    # Installed Dashboard
    # notes/zrq/20200807-07-dashboard.txt

    # Installed Zeppelin
    # notes/zrq/20200807-08-zeppelin-deploy.txt

# -----------------------------------------------------


# -----------------------------------------------------
# Delete the original jars.
#[user@desktop]

    pushd '/var/local/cache/maven'

        rm -rf com/amazonaws/

    popd


# -----------------------------------------------------
# Edit the Amazon code to disable the XML mangling.
#[user@desktop]

    source "${HOME}/aglais.env"
    pushd "${AGLAIS_HOME}"
        pushd 'external'
            pushd 'amazon'
                pushd 'aws-sdk-java'
                    pushd 'aws-java-sdk-s3'

                        gedit 'src/main/java/com/amazonaws/services/s3/model/transform/XmlResponsesSaxParser.java' &

                        -   private boolean sanitizeXmlDocument = true;
                        +   private boolean sanitizeXmlDocument = false;

                        git diff

                        mvn clean install -D skipTests=true

                    popd
                popd
            popd
        popd
    popd


    >   diff --git a/aws-java-sdk-s3/src/main/java/com/amazonaws/services/s3/model/transform/XmlResponsesSaxParser.java b/aws-java-sdk-s3/src/main/java/com/amazonaws/services/s3/model/transform/XmlResponsesSaxParser.java
    >   index 00a32cb8019..143278335ea 100644
    >   --- a/aws-java-sdk-s3/src/main/java/com/amazonaws/services/s3/model/transform/XmlResponsesSaxParser.java
    >   +++ b/aws-java-sdk-s3/src/main/java/com/amazonaws/services/s3/model/transform/XmlResponsesSaxParser.java
    >   @@ -107,7 +107,7 @@ public class XmlResponsesSaxParser {
    >   
    >        private XMLReader xr = null;
    >   
    >   -    private boolean sanitizeXmlDocument = true;
    >   +    private boolean sanitizeXmlDocument = false;
    >   
    >        /**
    >         * Constructs the XML SAX parser.


    >   ....
    >   ....
    >   [INFO] ------------------------------------------------------------------------
    >   [INFO] BUILD SUCCESS
    >   [INFO] ------------------------------------------------------------------------
    >   [INFO] Total time: 8.685 s
    >   [INFO] Finished at: 2020-08-11T05:09:47+01:00
    >   [INFO] ------------------------------------------------------------------------


# -----------------------------------------------------
# Edit the Amazon code to change the version.
#[user@desktop]

    oldversion=1.11.835
    newversion=1.11.835-aglais

    source "${HOME}/aglais.env"
    pushd "${AGLAIS_HOME}"
        pushd 'external'
            pushd 'amazon'
                pushd 'aws-sdk-java'

                    for pom in $(find . -name 'pom.xml')
                    do
                        sed -i "
                            s/<version>${oldversion:?}<\/version>/<version artifact='aws-sdk-java'>${newversion:?}<\/version>/
                            " "${pom:?}"
                    done

                    git diff

                    mvn clean install -D skipTests=true
                    mvn eclipse:eclipse

                popd
            popd
        popd
    popd


    >   diff --git a/aws-java-sdk-accessanalyzer/pom.xml b/aws-java-sdk-accessanalyzer/pom.xml
    >   index 92d524fae09..602d3add677 100644
    >   --- a/aws-java-sdk-accessanalyzer/pom.xml
    >   +++ b/aws-java-sdk-accessanalyzer/pom.xml
    >   @@ -5,7 +5,7 @@
    >      <parent>
    >        <groupId>com.amazonaws</groupId>
    >        <artifactId>aws-java-sdk-pom</artifactId>
    >   -    <version>1.11.835</version>
    >   +    <version artifact='aws-sdk-java'>1.11.835-aglais</version>
    >      </parent>
    >      <groupId>com.amazonaws</groupId>
    >      <artifactId>aws-java-sdk-accessanalyzer</artifactId>
    >   ....
    >   ....
    >   ....
    >   ....
    >   diff --git a/pom.xml b/pom.xml
    >   index 620f368cf55..57ec904d356 100644
    >   --- a/pom.xml
    >   +++ b/pom.xml
    >   @@ -4,7 +4,7 @@
    >      <modelVersion>4.0.0</modelVersion>
    >      <groupId>com.amazonaws</groupId>
    >      <artifactId>aws-java-sdk-pom</artifactId>
    >   -  <version>1.11.835</version>
    >   +  <version artifact='aws-sdk-java'>1.11.835-aglais</version>
    >      <packaging>pom</packaging>
    >      <name>AWS SDK for Java</name>
    >      <description>The Amazon Web Services SDK for Java provides Java APIs


    >   ....
    >   ....
    >   [INFO] ------------------------------------------------------------------------
    >   [INFO] BUILD SUCCESS
    >   [INFO] ------------------------------------------------------------------------
    >   [INFO] Total time: 08:23 min
    >   [INFO] Finished at: 2020-08-11T05:35:56+01:00
    >   [INFO] ------------------------------------------------------------------------


# -----------------------------------------------------
# Run our tests with the original jars.
#[user@desktop]

    sdkversion=${oldversion:?}

    source "${HOME}/aglais.env"
    pushd "${AGLAIS_CODE}"

        pushd 'experiments/zrq/java/spark-tools'

            sed -i "
                s/<aws-java-sdk.version>.*<\/aws-java-sdk.version>/<aws-java-sdk.version>${sdkversion:?}<\/aws-java-sdk.version>/
                " pom.xml

            mvn clean test
            mvn eclipse:eclipse

        popd
    popd


    >   [INFO] ------------------------------------------------------------------------
    >   [INFO] BUILD SUCCESS
    >   [INFO] ------------------------------------------------------------------------
    >   [INFO] Total time: 9.373 s
    >   [INFO] Finished at: 2020-08-11T05:50:50+01:00
    >   [INFO] ------------------------------------------------------------------------


# -----------------------------------------------------
# Run our tests with the modified jars.
#[user@desktop]

    sdkversion=${newversion:?}

    source "${HOME}/aglais.env"
    pushd "${AGLAIS_CODE}"

        pushd 'experiments/zrq/java/spark-tools'

            sed -i "
                s/<aws-java-sdk.version>.*<\/aws-java-sdk.version>/<aws-java-sdk.version>${sdkversion:?}<\/aws-java-sdk.version>/
                " pom.xml

            mvn clean test
            mvn eclipse:eclipse

        popd
    popd


    >   [INFO] ------------------------------------------------------------------------
    >   [INFO] BUILD SUCCESS
    >   [INFO] ------------------------------------------------------------------------
    >   [INFO] Total time: 10.257 s
    >   [INFO] Finished at: 2020-08-11T05:54:06+01:00
    >   [INFO] ------------------------------------------------------------------------

    #
    # Test passes both ways.
    # DONE Run through in Eclipse to check it is skipping the XML sanitize step.
    #


# -----------------------------------------------------
# Update the version of AWS SDK in our Spark Dockerfile
# to use a local modified version.
#[user@desktop]

    source "${HOME}/aglais.env"
    gedit "${AGLAIS_CODE}/experiments/zrq/spark/Dockermod"

    -   ADD https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.835/aws-java-sdk-bundle-1.11.835.jar /opt/spark/jars/
    +   ADD aws-java-sdk-bundle-1.11.835-aglais.jar /opt/spark/jars/

    -   RUN chmod a+r /opt/spark/jars/aws-java-sdk-bundle-1.11.835.jar
    +   RUN chmod a+r /opt/spark/jars/aws-java-sdk-bundle-1.11.835-aglais.jar

# -----------------------------------------------------
# Update the version of AWS SDK in our Spark Dockerfile
# to use a local modified version.
#[user@desktop]

    source "${HOME}/aglais.env"
    gedit "${AGLAIS_CODE}/experiments/zrq/pyspark/Dockermod"

    -   ADD https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.835/aws-java-sdk-bundle-1.11.835.jar /opt/spark/jars/
    +   ADD aws-java-sdk-bundle-1.11.835-aglais.jar /opt/spark/jars/

    -   RUN chmod a+r /opt/spark/jars/aws-java-sdk-bundle-1.11.835.jar
    +   RUN chmod a+r /opt/spark/jars/aws-java-sdk-bundle-1.11.835-aglais.jar


# -----------------------------------------------------
# Copy the modified version of the WS SDK.
#[user@desktop]

    mvncache=/var/local/cache/maven
    awsversion=1.11.835-aglais

    builddir=$(mktemp -d)

    cp ${mvncache:?}/com/amazonaws/aws-java-sdk-bundle/${awsversion:?}/aws-java-sdk-bundle-${awsversion:?}.jar \
       ${builddir:?}


# -----------------------------------------------------
# Build our modified Spark image.
#[user@desktop]

    source "${HOME}/aglais.env"

    buildsrc=2020.07.22
    buildtag=$(date '+%Y.%m.%d')
    buildtime=$(date '+%Y-%m-%dT%H:%M:%S')

    buildah bud \
        --format docker \
        --tag aglais/spark-mod:latest \
        --tag aglais/spark-mod:${buildtag:?} \
        --build-arg "buildsrc=${buildsrc:?}" \
        --build-arg "buildtag=${buildtag:?}" \
        --build-arg "buildtime=${buildtime:?}" \
        --file "${AGLAIS_CODE}/experiments/zrq/spark/Dockermod" \
        "${builddir:?}"

    >   ....
    >   ....
    >   Copying blob 04032846f584 done
    >   Copying config 75af5a6099 done
    >   Writing manifest to image destination
    >   Storing signatures
    >   --> 75af5a60991
    >   75af5a609910fcdba95c81384c7e715b72d0b74d1eeba1831ae1465f3d42f4de


    buildah bud \
        --format docker \
        --tag aglais/pyspark-mod:latest \
        --tag aglais/pyspark-mod:${buildtag:?} \
        --build-arg "buildsrc=${buildsrc:?}" \
        --build-arg "buildtag=${buildtag:?}" \
        --build-arg "buildtime=${buildtime:?}" \
        --file "${AGLAIS_CODE}/experiments/zrq/pyspark/Dockermod" \
        "${builddir:?}"

    >   ....
    >   ....
    >   Copying blob c1e3b7f9dae2 done
    >   Copying config d13077b24b done
    >   Writing manifest to image destination
    >   Storing signatures
    >   --> d13077b24bc
    >   d13077b24bc888a0e97a78ef4eb85a47419ababc136ece9871e4df4ac2b0799f


# -----------------------------------------------------
# Login to the Docker registry.
#[user@desktop]

    podman login \
        --username $(secret docker.io.user) \
        --password $(secret docker.io.pass) \
        registry-1.docker.io

    >   Login Succeeded!


# -----------------------------------------------------
# Push our images to Docker hub.
#[user@desktop]

--- TODO ---

    podman push "aglais/spark-mod:${buildtag:?}"

    podman push "aglais/spark-mod:latest"

    podman push "aglais/pyspark-mod:${buildtag:?}"

    podman push "aglais/pyspark-mod:latest"


    # TODO Update our Spark image to use the modified version.
    # TODO Deploy and retest with DR2.
    # TODO Update our 'fix' to use a config property.


# -----------------------------------------------------
# -----------------------------------------------------
# Test the PySpark interpreter with small and large data sets.
#[user@zeppelin]

    # Small container test
    df = sqlContext.read.parquet(
        "s3a://albert/"
        )

    print "DF count: ", df.count()
    print "DF partitions: ", df.rdd.getNumPartitions()

    >   DF count:  621626
    >   DF partitions:  10


    # Large container test
    df = sqlContext.read.parquet(
        "s3a://gaia-dr2-parquet/"
        )

    print "DF count: ", df.count()
    print "DF partitions: ", df.rdd.getNumPartitions()

    >   Py4JJavaError: An error occurred while calling o212.parquet.
    >   : java.io.EOFException: listObjects() on s3a://gaia-dr2-parquet/: ....
    >   	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:181)
    >   	at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)
    >   	....
    >       ....
    >       Caused by: java.lang.OutOfMemoryError: Java heap space


# -----------------------------------------------------
# Reload the PySpark interpreter and run our test.
#[user@zeppelin]

    # Small container test
    df = sqlContext.read.parquet(
        "s3a://albert/"
        )

    print "DF count: ", df.count()
    print "DF partitions: ", df.rdd.getNumPartitions()

    #
    # Lots of different errors.
    #

    >   : java.lang.OutOfMemoryError: Java heap space


    >   Exception in thread "dispatcher-CoarseGrainedScheduler" java.lang.BootstrapMethodError: call site initialization exception
    >   	at java.lang.invoke.CallSite.makeSite(CallSite.java:341)
    >   	at java.lang.invoke.MethodHandleNatives.linkCallSiteImpl(MethodHandleNatives.java:307)
    >   	at java.lang.invoke.MethodHandleNatives.linkCallSite(MethodHandleNatives.java:297)
    >   	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$removeExecutor$1(TaskSchedulerImpl.scala:823)
    >   	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$removeExecutor$1$adapted(TaskSchedulerImpl.scala:818)
    >   	at scala.Option.foreach(Option.scala:407)
    >   	at org.apache.spark.scheduler.TaskSchedulerImpl.removeExecutor(TaskSchedulerImpl.scala:818)
    >   	at org.apache.spark.scheduler.TaskSchedulerImpl.executorLost(TaskSchedulerImpl.scala:756)
    >   	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.disableExecutor(CoarseGrainedSchedulerBackend.scala:428)
    >   	at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint.$anonfun$onDisconnected$1(KubernetesClusterSchedulerBackend.scala:195)
    >   	at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint.$anonfun$onDisconnected$1$adapted(KubernetesClusterSchedulerBackend.scala:195)
    >   	at scala.Option.foreach(Option.scala:407)
    >   	at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint.onDisconnected(KubernetesClusterSchedulerBackend.scala:195)
    >   	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:141)
    >   	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:203)
    >   	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
    >   	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
    >   	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
    >   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
    >   Exception in thread "dispatcher-CoarseGrainedScheduler" java.lang.BootstrapMethodError: call site initialization exception
    >   	at java.lang.invoke.CallSite.makeSite(CallSite.java:341)
    >   	at java.lang.invoke.MethodHandleNatives.linkCallSiteImpl(MethodHandleNatives.java:307)
    >   	at java.lang.invoke.MethodHandleNatives.linkCallSite(MethodHandleNatives.java:297)
    >   	at org.apache.spark.scheduler.TaskSchedulerImpl.removeExecutor(TaskSchedulerImpl.scala:843)
    >   	at org.apache.spark.scheduler.TaskSchedulerImpl.executorLost(TaskSchedulerImpl.scala:756)
    >   	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.disableExecutor(CoarseGrainedSchedulerBackend.scala:428)
    >   	at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint.$anonfun$onDisconnected$1(KubernetesClusterSchedulerBackend.scala:195)
    >   	at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint.$anonfun$onDisconnected$1$adapted(KubernetesClusterSchedulerBackend.scala:195)
    >   	at scala.Option.foreach(Option.scala:407)
    >   	at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint.onDisconnected(KubernetesClusterSchedulerBackend.scala:195)
    >   	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:141)
    >   	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:203)
    >   	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
    >   	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
    >   	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
    >   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    >   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
    >   Exception in thread "kubernetes-executor-snapshots-subscribers-1" java.lang.OutOfMemoryError: GC overhead limit exceeded
    >   Exception in thread "netty-rpc-env-timeout" java.lang.OutOfMemoryError: GC overhead limit exceeded
    >   Exception in thread "kubernetes-dispatcher-0" java.lang.OutOfMemoryError: GC overhead limit exceeded
    >   Exception in thread "spark-listener-group-appStatus" java.lang.OutOfMemoryError: GC overhead limit exceeded
    >   Py4JJavaError: An error occurred while calling o118.parquet.
    >   : java.lang.OutOfMemoryError: GC overhead limit exceeded





