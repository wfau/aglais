#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    JUnit tests for S3 are working using the following properties:

        configuration.set("fs.s3a.endpoint", "https://cumulus.openstack.hpc.cam.ac.uk:6780");
        configuration.set("fs.s3a.path.style.access", "true");
        configuration.set("fs.s3a.list.version", "2");
        configuration.set("fs.s3a.bucket.probe", "0");
        configuration.set("fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider");


    Zeppelin notebooks fail using the same properties:

        %spark.pyspark
        sc._jsc.hadoopConfiguration().set(
            "fs.s3a.endpoint", "cumulus.openstack.hpc.cam.ac.uk:6780/"
            )
        sc._jsc.hadoopConfiguration().set(
            "fs.s3a.path.style.access", "true"
            )
        sc._jsc.hadoopConfiguration().set(
            "fs.s3a.list.version", "2"
            )
        sc._jsc.hadoopConfiguration().set(
            "fs.s3a.bucket.probe", "0"
            )
        sc._jsc.hadoopConfiguration().set(
            "fs.s3a.aws.credentials.provider",
            "org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider"
            )

    We used to get an XML parser error:

        Py4JJavaError: An error occurred while calling o524.parquet.
            org.apache.hadoop.fs.s3a.AWSClientIOException:
                getFileStatus on s3a://gaia-dr2-parquet/:
                    com.amazonaws.SdkClientException:
                        Failed to parse XML document with handler class com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListObjectsV2Handler:
                            Failed to parse XML document with handler class com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListObjectsV2Handler


    Now we get an EOF exception:

        java.io.EOFException:
            listObjects() on s3a://gaia-dr2-parquet/:
                com.amazonaws.SdkClientException:
                    Failed to sanitize XML document destined for handler class com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListObjectsV2Handler
	                at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:181)
	                at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)
	                at org.apache.hadoop.fs.s3a.Listing$ObjectListingIterator.next(Listing.java:604)
	                at org.apache.hadoop.fs.s3a.Listing$FileStatusListingIterator.requestNextBatch(Listing.java:421)
	                at org.apache.hadoop.fs.s3a.Listing$FileStatusListingIterator.sourceHasNext(Listing.java:373)
	                at org.apache.hadoop.fs.s3a.Listing$FileStatusListingIterator.hasNext(Listing.java:368)
	                at org.apache.hadoop.fs.s3a.S3AFileSystem.innerListStatus(S3AFileSystem.java:1924)
	                at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listStatus$9(S3AFileSystem.java:1882)
	                at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
	                at org.apache.hadoop.fs.s3a.S3AFileSystem.listStatus(S3AFileSystem.java:1882)
	                ....
            Caused by: com.amazonaws.SdkClientException:
                Failed to sanitize XML document destined for handler class com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListObjectsV2Handler
	                at com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.sanitizeXmlDocument(XmlResponsesSaxParser.java:224)
	                at com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.parseListObjectsV2Response(XmlResponsesSaxParser.java:339)
	                at com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsV2Unmarshaller.unmarshall(Unmarshallers.java:128)
	                at com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsV2Unmarshaller.unmarshall(Unmarshallers.java:117)
	                at com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:62)
	                at com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:31)
	                ....
            Caused by: java.lang.OutOfMemoryError: Java heap space



# -----------------------------------------------------

    In the mean time ... the Kubernetes cluster is no longer healthy.

# -----------------------------------------------------
# Get the connection details for our cluster.
#[user@kubernator]

    kubectl \
        cluster-info

    >   Unable to connect to the server: dial tcp 128.232.227.148:6443: i/o timeout

    kubectl \
        cluster-info \
            dump

    >   Unable to connect to the server: dial tcp 128.232.227.148:6443: i/o timeout





