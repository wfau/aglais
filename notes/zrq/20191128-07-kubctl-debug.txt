#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2019, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    #
    # Started to see consistent delays of ~1min30sec in kubectl calls.
    #

    #
    # Advice from Bharat Kunwar on IRIS Slack <bharat@stackhpc.com>
    # Created a 'jump host' to acces the cluster from inside the private network.
    #

    #
    # Created a new virtual machine instance, assigned to the same private network as the K8 cluster.
    # Added a floating IP address on the public network.
    # Added security rules to allow SSH.
    #


# -----------------------------------------------------
# Login to the jump host using public IP address.
#[user@desktop]

    ssh fedora@128.232.227.140


# -----------------------------------------------------
# Install Docker
#[fedora@virtual]

    sudo dnf info docker

    >   Fedora 30 - x86_64                              170 kB/s |  24 kB     00:00
    >   Last metadata expiration check: 0:00:14 ago on Thu 28 Nov 2019 11:44:35 UTC.
    >   Available Packages
    >   Name         : docker
    >   Epoch        : 2
    >   Version      : 1.13.1
    >   Release      : 68.git47e2230.fc30
    >   Architecture : x86_64
    >   Size         : 26 M
    >   Source       : docker-1.13.1-68.git47e2230.fc30.src.rpm
    >   Repository   : updates
    >   Summary      : Automates deployment of containerized applications
    >   URL          : https://github.com/projectatomic/docker
    >   License      : ASL 2.0
    >   Description  : Docker is an open-source engine that automates the deployment of any
    >                : application as a lightweight, portable, self-sufficient container that will
    >                : run virtually anywhere.
    >                :
    >                : Docker containers can encapsulate any payload, and will run consistently on
    >                : and between virtually any server. The same container that a developer builds
    >                : and tests on a laptop will run at scale, in production*, on VMs, bare-metal
    >                : servers, OpenStack clusters, public instances, or combinations of the above.


    sudo dnf install docker

    >   ....
    >   Installed:
    >       docker-2:1.13.1-68.git47e2230.fc30.x86_64
    >       docker-common-2:1.13.1-68.git47e2230.fc30.x86_64
    >       docker-rhel-push-plugin-2:1.13.1-68.git47e2230.fc30.x86_64
    >       ....


# -----------------------------------------------------
# Run our OpenStack client.
#[fedora@virtual]

    docker run \
        --rm \
        --tty \
        --user "$(id -u)" \
        --interactive \
        --hostname openstacker \
        --volume "${HOME}/clouds.yaml:/etc/openstack/clouds.yaml" \
        atolmis/openstack-client \
        bash

    >   /usr/bin/docker-current:
    >       Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock:
    >           Post http://%2Fvar%2Frun%2Fdocker.sock/v1.26/containers/create: dial unix /var/run/docker.sock: connect:
    >               permission denied.


    ls -al /var/run/docker.sock

    >   srw-rw----. 1 root root 0 Nov 28 11:47 /var/run/docker.sock


    sudo chmod a+rw /var/run/docker.sock

    ls -al /var/run/docker.sock

    >   srw-rw-rw-. 1 root root 0 Nov 28 11:47 /var/run/docker.sock


# -----------------------------------------------------
# Run our OpenStack client.
#[fedora@virtual]

    docker run \
        --rm \
        --tty \
        --user "$(id -u)" \
        --interactive \
        --hostname openstacker \
        --volume "${HOME}/clouds.yaml:/etc/openstack/clouds.yaml" \
        atolmis/openstack-client \
        bash

    >   Unable to find image 'atolmis/openstack-client:latest' locally
    >   Trying to pull repository docker.io/atolmis/openstack-client ...
    >   sha256:09ef00ed423b4a46a6f7e090f193bbbab640d12065999947de4a4f55f6385e28: Pulling from docker.io/atolmis/openstack-client
    >   d318c91bf2a8: Pull complete
    >   d50090d4a6e9: Pull complete
    >   ....
    >   ....
    >   bc0ddb06c25d: Pull complete
    >   58c4eb1fb3ac: Pull complete
    >   Digest: sha256:09ef00ed423b4a46a6f7e090f193bbbab640d12065999947de4a4f55f6385e28
    >   Status: Downloaded newer image for docker.io/atolmis/openstack-client:latest


# -----------------------------------------------------
# Check our cloud config.
#[user@openstacker]

    cat /etc/openstack/clouds.yaml

    >   clouds:
    >
    >     gaia-dev:
    >       auth:
    >         auth_url: https://cumulus.openstack.hpc.cam.ac.uk:5000/v3
    >         application_credential_id:     '597b9c075db341f8a70c56d23dd30917'
    >         application_credential_secret: 'Voh7nei3.eeVee1Jo.Eep6zei2.Ohgh8eng'
    >       region_name: "RegionOne"
    >       interface: "public"
    >       identity_api_version: 3
    >       auth_type: "v3applicationcredential"
    >
    >     gaia-dev-super:
    >       auth:
    >         auth_url: https://cumulus.openstack.hpc.cam.ac.uk:5000/v3
    >         application_credential_id:     '446d162546ef45ffa41da8275e28df77'
    >         application_credential_secret: 'Oosub7oo.Agu9gee4.Beesh8Wi.OovaeN1o'
    >       region_name: "RegionOne"
    >       interface: "public"
    >       identity_api_version: 3
    >       auth_type: "v3applicationcredential"


# -----------------------------------------------------
# Get our cluster UUID.
#[user@openstacker]

    clusteruuid=$(
        openstack \
            --os-cloud gaia-dev \
                coe cluster list \
                    --format json \
        | jq -r '.[] | select(.name | test("albert")) | .uuid'
        )

    echo "Cluster uuid [${clusteruuid:?}]"

    >   Cluster uuid [222859e8-82d5-4298-a072-ae2e21705ef9]


# -----------------------------------------------------
# Get our cluster details.
#[user@openstacker]

    openstack \
        --os-cloud gaia-dev \
            coe cluster show \
                "${clusteruuid}"

    >   +---------------------+---------------------------------------------------------------+
    >   | Field               | Value                                                         |
    >   +---------------------+---------------------------------------------------------------+
    >   | status              | CREATE_COMPLETE                                               |
    >   | cluster_template_id | 41437926-8461-4fdf-ac1b-ff97325a79f8                          |
    >   | node_addresses      | ['10.0.0.5', '10.0.0.10', '10.0.0.20', '10.0.0.6']            |
    >   | uuid                | 222859e8-82d5-4298-a072-ae2e21705ef9                          |
    >   | stack_id            | c3f714fa-6f8c-4fc0-8479-4c0501750c01                          |
    >   | status_reason       | Stack CREATE completed successfully                           |
    >   | created_at          | 2019-11-28T10:06:03+00:00                                     |
    >   | updated_at          | 2019-11-28T10:13:18+00:00                                     |
    >   | coe_version         | v1.11.6                                                       |
    >   | labels              | { .... }                                                      |
    >   | faults              |                                                               |
    >   | keypair             | zrq-gaia-keypair                                              |
    >   | api_address         | https://128.232.227.124:6443                                  |
    >   | master_addresses    | ['10.0.0.30', '10.0.0.17']                                    |
    >   | create_timeout      | 60                                                            |
    >   | node_count          | 4                                                             |
    >   | discovery_url       | https://discovery.etcd.io/744f11b396d52af40a207953ff12648b    |
    >   | master_count        | 2                                                             |
    >   | container_version   | 1.12.6                                                        |
    >   | name                | albert                                                        |
    >   | master_flavor_id    | 20061eba-9e88-494c-95a3-41ed77721244                          |
    >   | flavor_id           | 20061eba-9e88-494c-95a3-41ed77721244                          |
    >   +---------------------+---------------------------------------------------------------+


# -----------------------------------------------------
# Time the API call.
#[user@openstacker]

    time kubectl --kubeconfig "${confdir}/config" cluster-info

    >   Kubernetes master is running at https://128.232.227.124:6443
    >   Heapster is running at https://128.232.227.124:6443/api/v1/namespaces/kube-system/services/heapster/proxy
    >   CoreDNS is running at https://128.232.227.124:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
    >
    >   To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
    >
    >   real    0m0.130s
    >   user    0m0.111s
    >   sys     0m0.019s


    time kubectl --kubeconfig "${confdir}/config" cluster-info

    >   Kubernetes master is running at https://128.232.227.124:6443
    >   Heapster is running at https://128.232.227.124:6443/api/v1/namespaces/kube-system/services/heapster/proxy
    >   CoreDNS is running at https://128.232.227.124:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
    >
    >   To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
    >
    >   real    1m30.115s
    >   user    0m0.106s
    >   sys     0m0.018s


    time kubectl --kubeconfig "${confdir}/config" cluster-info

    >   Kubernetes master is running at https://128.232.227.124:6443
    >   Heapster is running at https://128.232.227.124:6443/api/v1/namespaces/kube-system/services/heapster/proxy
    >   CoreDNS is running at https://128.232.227.124:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
    >
    >   To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
    >
    >   real    1m30.108s
    >   user    0m0.093s
    >   sys     0m0.026s


    time kubectl --kubeconfig "${confdir}/config" cluster-info

    >   Kubernetes master is running at https://128.232.227.124:6443
    >   Heapster is running at https://128.232.227.124:6443/api/v1/namespaces/kube-system/services/heapster/proxy
    >   CoreDNS is running at https://128.232.227.124:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
    >
    >   To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
    >
    >   real    0m0.131s
    >   user    0m0.095s
    >   sys     0m0.022s

# -----------------------------------------------------
# Advice from Bharat Kunwar to replace dns-autoscaler.
#[user@openstacker]

    kubectl \
        --kubeconfig "${confdir}/config"  \
            set image \
                deploy/kube-dns-autoscaler \
                    autoscaler=gcr.io/google_containers/cluster-proportional-autoscaler-amd64:1.1.2 \
                    -n kube-system

    >   deployment.extensions/kube-dns-autoscaler image updated


# -----------------------------------------------------
# Time the API call, still getting delays.
#[user@openstacker]

    time kubectl --kubeconfig "${confdir}/config" cluster-info

    >   Kubernetes master is running at https://128.232.227.124:6443
    >   Heapster is running at https://128.232.227.124:6443/api/v1/namespaces/kube-system/services/heapster/proxy
    >   CoreDNS is running at https://128.232.227.124:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
    >
    >   To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
    >
    >   real	1m30.114s
    >   user	0m0.095s
    >   sys	0m0.029s


# -----------------------------------------------------
# Edit the endpoint to use internal 10.0.0.0 address.
#[user@openstacker]

    vi "${confdir}/config"

        apiVersion: v1
        clusters:
        - cluster:
            certificate-authority-data: LS0tLS1C .... RS0tLS0t
    -       server: https://128.232.227.124:6443
    +       server: https://10.0.0.7:6443
          name: albert
        contexts:
        - context:
            cluster: albert
            user: admin
          name: default
        current-context: default
        kind: Config
        preferences: {}
        users:
        - name: admin
          user:
            client-certificate-data: LS0tLS1C .... LS0tLS0=
            client-key-data: LS0tLS1C .... LS0tLQo=


# -----------------------------------------------------
# Time the API call, still getting delays.
#[user@openstacker]

    time kubectl --kubeconfig "${confdir}/config" cluster-info

    >   Kubernetes master is running at https://10.0.0.7:6443
    >   Heapster is running at https://10.0.0.7:6443/api/v1/namespaces/kube-system/services/heapster/proxy
    >   CoreDNS is running at https://10.0.0.7:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
    >
    >   To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
    >
    >   real	1m30.114s
    >   user	0m0.092s
    >   sys	0m0.023s


# -----------------------------------------------------
# Check the pods are running ...
#[user@openstacker]

    time kubectl --kubeconfig "${confdir}/config" get all -A

    >   NAMESPACE       NAME                                                          READY   STATUS      RESTARTS   AGE
    >   kube-system     pod/cluster-autoscaler-67c9f4fc54-67t7x                       1/1     Running     0          171m
    >   kube-system     pod/coredns-865bd969f-sbnvp                                   1/1     Running     0          171m
    >   kube-system     pod/coredns-865bd969f-v2b6h                                   1/1     Running     0          51m
    >   kube-system     pod/heapster-7bf5794cc7-zm8rx                                 1/1     Running     0          171m
    >   kube-system     pod/k8s-keystone-auth-jr7f5                                   1/1     Running     0          171m
    >   kube-system     pod/k8s-keystone-auth-pqvvl                                   1/1     Running     0          171m
    >   kube-system     pod/kube-dns-autoscaler-6c6b4b7cf7-fq8vv                      1/1     Running     0          51m
    >   kube-system     pod/kube-flannel-ds-amd64-4vsqh                               1/1     Running     0          171m
    >   kube-system     pod/kube-flannel-ds-amd64-6g5hq                               1/1     Running     0          171m
    >   kube-system     pod/kube-flannel-ds-amd64-k6k68                               1/1     Running     0          171m
    >   kube-system     pod/kube-flannel-ds-amd64-t7ptt                               1/1     Running     0          171m
    >   kube-system     pod/kube-flannel-ds-amd64-tg8rp                               1/1     Running     0          171m
    >   kube-system     pod/kube-flannel-ds-amd64-z4nhd                               1/1     Running     0          171m
    >   kube-system     pod/kubernetes-dashboard-d48c76949-jrqz6                      1/1     Running     0          171m
    >   kube-system     pod/metrics-server-dcbc75646-vsz75                            1/1     Running     0          170m
    >   kube-system     pod/npd-6k8gh                                                 1/1     Running     0          171m
    >   kube-system     pod/npd-k8kgd                                                 1/1     Running     0          171m
    >   kube-system     pod/npd-t8nlc                                                 1/1     Running     0          171m
    >   kube-system     pod/npd-xbb28                                                 1/1     Running     0          171m
    >   kube-system     pod/octavia-ingress-controller-0                              1/1     Running     0          171m
    >   kube-system     pod/openstack-cloud-controller-manager-8rrhg                  1/1     Running     0          171m
    >   kube-system     pod/openstack-cloud-controller-manager-xfj6w                  1/1     Running     0          171m
    >   magnum-tiller   pod/install-metrics-server-job-lz5jz                          0/1     Completed   0          171m
    >   magnum-tiller   pod/install-prometheus-operator-job-vgfmh                     0/1     Completed   0          171m
    >   magnum-tiller   pod/tiller-deploy-74f5b67459-kt49p                            1/1     Running     0          171m
    >   monitoring      pod/alertmanager-prometheus-alertmanager-0                    2/2     Running     0          169m
    >   monitoring      pod/prometheus-operator-787f64f8d7-fp28m                      1/1     Running     0          169m
    >   monitoring      pod/prometheus-operator-grafana-5598b9dbf9-84cc8              3/3     Running     6          169m
    >   monitoring      pod/prometheus-operator-kube-state-metrics-79457f8c5f-qs797   1/1     Running     0          169m
    >   monitoring      pod/prometheus-operator-prometheus-node-exporter-49xcw        1/1     Running     0          169m
    >   monitoring      pod/prometheus-operator-prometheus-node-exporter-684mg        1/1     Running     0          169m
    >   monitoring      pod/prometheus-operator-prometheus-node-exporter-9c49j        1/1     Running     0          169m
    >   monitoring      pod/prometheus-operator-prometheus-node-exporter-b8m4r        1/1     Running     0          169m
    >   monitoring      pod/prometheus-operator-prometheus-node-exporter-gztmq        1/1     Running     0          169m
    >   monitoring      pod/prometheus-operator-prometheus-node-exporter-xkgbd        1/1     Running     0          169m
    >   monitoring      pod/prometheus-prometheus-prometheus-0                        3/3     Running     1          169m
    >
    >
    >   NAMESPACE       NAME                                                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE
    >   default         service/cassandra                                      ClusterIP   None             <none>        9042/TCP                 110m
    >   default         service/kubernetes                                     ClusterIP   10.254.0.1       <none>        443/TCP                  171m
    >   kube-system     service/heapster                                       ClusterIP   10.254.177.234   <none>        80/TCP                   171m
    >   kube-system     service/kube-dns                                       ClusterIP   10.254.0.10      <none>        53/UDP,53/TCP,9153/TCP   171m
    >   kube-system     service/kubernetes-dashboard                           ClusterIP   10.254.121.24    <none>        443/TCP                  171m
    >   kube-system     service/metrics-server                                 ClusterIP   10.254.100.193   <none>        443/TCP                  170m
    >   kube-system     service/prometheus-coredns                             ClusterIP   None             <none>        9153/TCP                 169m
    >   kube-system     service/prometheus-kube-controller-manager             ClusterIP   None             <none>        10252/TCP                169m
    >   kube-system     service/prometheus-kube-etcd                           ClusterIP   None             <none>        4001/TCP                 169m
    >   kube-system     service/prometheus-kube-scheduler                      ClusterIP   None             <none>        10251/TCP                169m
    >   kube-system     service/prometheus-kubelet                             ClusterIP   None             <none>        10250/TCP                166m
    >   magnum-tiller   service/tiller-deploy                                  ClusterIP   10.254.45.73     <none>        44134/TCP                171m
    >   monitoring      service/alertmanager-operated                          ClusterIP   None             <none>        9093/TCP,6783/TCP        169m
    >   monitoring      service/prometheus-alertmanager                        ClusterIP   10.254.253.57    <none>        9093/TCP                 169m
    >   monitoring      service/prometheus-operated                            ClusterIP   None             <none>        9090/TCP                 169m
    >   monitoring      service/prometheus-operator                            ClusterIP   None             <none>        8080/TCP                 169m
    >   monitoring      service/prometheus-operator-grafana                    ClusterIP   10.254.73.175    <none>        80/TCP                   169m
    >   monitoring      service/prometheus-operator-kube-state-metrics         ClusterIP   10.254.236.157   <none>        8080/TCP                 169m
    >   monitoring      service/prometheus-operator-prometheus-node-exporter   ClusterIP   10.254.109.88    <none>        9100/TCP                 169m
    >   monitoring      service/prometheus-prometheus                          ClusterIP   10.254.172.129   <none>        9090/TCP                 169m
    >
    >   NAMESPACE     NAME                                                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                     AGE
    >   kube-system   daemonset.apps/k8s-keystone-auth                              2         2         2       2            2           node-role.kubernetes.io/master=   171m
    >   kube-system   daemonset.apps/kube-flannel-ds-amd64                          6         6         6       6            6           beta.kubernetes.io/arch=amd64     171m
    >   kube-system   daemonset.apps/npd                                            4         4         4       4            4           <none>                            171m
    >   kube-system   daemonset.apps/openstack-cloud-controller-manager             2         2         2       2            2           node-role.kubernetes.io/master=   171m
    >   monitoring    daemonset.apps/prometheus-operator-prometheus-node-exporter   6         6         6       6            6           <none>                            169m
    >
    >   NAMESPACE       NAME                                                     READY   UP-TO-DATE   AVAILABLE   AGE
    >   kube-system     deployment.apps/cluster-autoscaler                       1/1     1            1           171m
    >   kube-system     deployment.apps/coredns                                  2/2     2            2           171m
    >   kube-system     deployment.apps/heapster                                 1/1     1            1           171m
    >   kube-system     deployment.apps/kube-dns-autoscaler                      1/1     1            1           171m
    >   kube-system     deployment.apps/kubernetes-dashboard                     1/1     1            1           171m
    >   kube-system     deployment.apps/metrics-server                           1/1     1            1           170m
    >   magnum-tiller   deployment.apps/tiller-deploy                            1/1     1            1           171m
    >   monitoring      deployment.apps/prometheus-operator                      1/1     1            1           169m
    >   monitoring      deployment.apps/prometheus-operator-grafana              1/1     1            1           169m
    >   monitoring      deployment.apps/prometheus-operator-kube-state-metrics   1/1     1            1           169m
    >
    >   NAMESPACE       NAME                                                                DESIRED   CURRENT   READY   AGE
    >   kube-system     replicaset.apps/cluster-autoscaler-67c9f4fc54                       1         1         1       171m
    >   kube-system     replicaset.apps/coredns-865bd969f                                   2         2         2       171m
    >   kube-system     replicaset.apps/heapster-7bf5794cc7                                 1         1         1       171m
    >   kube-system     replicaset.apps/kube-dns-autoscaler-57bd7f54d5                      0         0         0       171m
    >   kube-system     replicaset.apps/kube-dns-autoscaler-6c6b4b7cf7                      1         1         1       51m
    >   kube-system     replicaset.apps/kubernetes-dashboard-d48c76949                      1         1         1       171m
    >   kube-system     replicaset.apps/metrics-server-dcbc75646                            1         1         1       170m
    >   magnum-tiller   replicaset.apps/tiller-deploy-74f5b67459                            1         1         1       171m
    >   monitoring      replicaset.apps/prometheus-operator-787f64f8d7                      1         1         1       169m
    >   monitoring      replicaset.apps/prometheus-operator-grafana-5598b9dbf9              1         1         1       169m
    >   monitoring      replicaset.apps/prometheus-operator-kube-state-metrics-79457f8c5f   1         1         1       169m
    >
    >   NAMESPACE     NAME                                                    READY   AGE
    >   kube-system   statefulset.apps/octavia-ingress-controller             1/1     171m
    >   monitoring    statefulset.apps/alertmanager-prometheus-alertmanager   1/1     169m
    >   monitoring    statefulset.apps/prometheus-prometheus-prometheus       1/1     169m
    >
    >
    >   NAMESPACE       NAME                                        COMPLETIONS   DURATION   AGE
    >   magnum-tiller   job.batch/install-metrics-server-job        1/1           67s        171m
    >   magnum-tiller   job.batch/install-prometheus-operator-job   1/1           2m53s      171m
    >
    >
    >   real	6m0.226s
    >   user	0m0.178s
    >   sys	0m0.061s


# -----------------------------------------------------
# Check both masters are up ..
#[user@openstacker]

    time kubectl --kubeconfig "${confdir}/config" get nodes -o wide

    >   NAME                           STATUS   ROLES    AGE    VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                       KERNEL-VERSION          CONTAINER-RUNTIME
    >   albert-5xkt67cs5bgh-master-0   Ready    master   179m   v1.14.6   10.0.0.30     <none>        Debian GNU/Linux 9 (stretch)   5.3.6-100.fc29.x86_64   docker://1.13.1
    >   albert-5xkt67cs5bgh-master-1   Ready    master   179m   v1.14.6   10.0.0.17     <none>        Debian GNU/Linux 9 (stretch)   5.3.6-100.fc29.x86_64   docker://1.13.1
    >   albert-5xkt67cs5bgh-minion-0   Ready    <none>   178m   v1.14.6   10.0.0.5      <none>        Debian GNU/Linux 9 (stretch)   5.3.6-100.fc29.x86_64   docker://1.13.1
    >   albert-5xkt67cs5bgh-minion-1   Ready    <none>   178m   v1.14.6   10.0.0.10     <none>        Debian GNU/Linux 9 (stretch)   5.3.6-100.fc29.x86_64   docker://1.13.1
    >   albert-5xkt67cs5bgh-minion-2   Ready    <none>   178m   v1.14.6   10.0.0.20     <none>        Debian GNU/Linux 9 (stretch)   5.3.6-100.fc29.x86_64   docker://1.13.1
    >   albert-5xkt67cs5bgh-minion-3   Ready    <none>   178m   v1.14.6   10.0.0.6      <none>        Debian GNU/Linux 9 (stretch)   5.3.6-100.fc29.x86_64   docker://1.13.1
    >
    >   real	1m30.111s
    >   user	0m0.098s
    >   sys	0m0.022s






