#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2018, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-indent
#zrq-notes-ansible
#

    Based on what we know of the GDAF hardware
    Physical machines:

        2 x Intel™ Xeon™ E5-2640v3 8 cpu-core, 16 cpu-threads
        = 32 cpu-threads (HyperThreading enabled)
        https://ark.intel.com/content/www/us/en/ark/products/83359/intel-xeon-processor-e5-2640-v3-20m-cache-2-60-ghz.html
        8 x 8GB DDR4 2133MHz ECC REG
        = 64G RAM (low?)

        2 x 128GB SSD PCIe 6Gb/s (probably SATA not PCIe)
        = 256G local SSD

        12 x 1Tbyte disc SATA 6Gb/s 7200rmp
        = 12Tbyte disc

    Six machine cluster:

        = 192 cpu-threads  (96 cpu-core)
        = 384G RAM (low?)
        = 1.5G SSD
        = 72 TB disc

    ---- ---- ---- ----

    Example analysis Mor-2019 paper.
    https://arxiv.org/abs/1901.07564

    "Each complete and robust inference of the full set of parameters
    requires 2*10^4CPU hours in the Spark environment of the Big Data
    platform at the University of Barcelona."

    If we assume HyperThreading enabled (single user, no security issues).

        2×10⁴ cpu-hours / 192 cpu-threads = 104 hours
                                          = 4.34 days per iteration

    Quote from the paper:
    "We used 10^5 hours of CPU"

        (1×10⁵)÷(2×10⁴) ~= 5 iterations

        1×10⁵ cpu-hours / 192 cpu-threads = 520.83 hours
                                          =  21.70 days total

    ---- ---- ---- ----

    Current allocation on Cambridge Cumulus system:
    OpenStack platform wih HyperThreading disabled due to security issues.

    Physical machines:

        2x Intel™ Xeon™ Gold 6142 16 cpu-core, 32 cpu-threads
        = 64 cpu-threads (HyperThreading enabled)
        = 32 cpu-threads (HyperThreading disabled)
        https://ark.intel.com/content/www/us/en/ark/products/120487/intel-xeon-gold-6142-processor-22m-cache-2-60-ghz.html

        192G RAM
        400G local SSD

    Cluster total:

        Initially deployed with 8 machines
        Expanding to 40 machines in 2019/2020

    OpenStack allocation:

         20 virtual machines
        200 vcpus (HyperThreading disabled)

         1TB volume storage
           0 object storage

    GDAF hardware is descibed in "Gaia Data Analytics Framework (GDAF) technical configuration"
    [GAIA-C9-TN-UB-FJL-002][2016-11-23]
    [https://gaia.esac.esa.int/dpacsvn/DPAC/CU9/docs/WP970_Science_Enabling_Apps/nonECSS/TechNotes/GAIA-C9-SP-UB-FJL-002/]

    Assuming GDAF platform hasn't been expended since 2016, then Cambridge OpenStack
    allocation of 200 vcpu is roughly equivalent to the (original) GDAF hardware.
    (*) big assumption ?

    If we assume 1:1 equivalence between GDAF cpu-threads and OpenStack vcpus

        2×10⁴ cpu-hours / 200 vcpus = 100 hours
                                    = 4.16 days per iteration

    (*) GDAF has HyperThreading enabled
    (*) Cambridge Cumulus system has HyperThreading disabled

    ---- ---- ---- ----




