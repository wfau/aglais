#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2018, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    Based on what we know of the GDAF hardware
    Physical machines:

        2 x Intel™ Xeon™ E5-2640v3 8 cpu-core, 16 cpu-threads
        = 32 cpu-threads (HyperThreading enabled)
        https://ark.intel.com/content/www/us/en/ark/products/83359/intel-xeon-processor-e5-2640-v3-20m-cache-2-60-ghz.html
        8 x 8GB DDR4 2133MHz ECC REG
        = 64G RAM (low?)

        2 x 128GB SSD PCIe 6Gb/s (probably SATA not PCIe)
        = 256G local SSD

        12 x 1Tbyte disc SATA 6Gb/s 7200rmp
        = 12Tbyte disc

    Six machine cluster:

        = 192 cpu-threads  (96 cpu-core)
        = 384G RAM (low?)
        = 1.5G SSD
        = 72 TB disc

---- ---- ---- ----

    Example analysis Mor-2019 paper.
    https://arxiv.org/abs/1901.07564

    "Each complete and robust inference of the full set of parameters
    requires 2*10^4CPU hours in the Spark environment of the Big Data
    platform at the University of Barcelona."

    If we assume HyperThreading enabled (single user, no security issues).

        2×10⁴ cpu-hours / 192 cpu-threads = 104 hours
                                          = 4.34 days per iteration

    Quote from the paper:
    "We used 10^5 hours of CPU"

        (1×10⁵)÷(2×10⁴) ~= 5 iterations

        1×10⁵ cpu-hours / 192 cpu-threads = 520.83 hours
                                          =  21.70 days total

---- ---- ---- ----

    Current allocation on Cambridge Cumulus system:
    OpenStack platform wih HyperThreading disabled due to security issues.

    Physical machines:

        2x Intel™ Xeon™ Gold 6142 16 cpu-core, 32 cpu-threads
        = 64 cpu-threads (HyperThreading enabled)
        = 32 cpu-threads (HyperThreading disabled)
        https://ark.intel.com/content/www/us/en/ark/products/120487/intel-xeon-gold-6142-processor-22m-cache-2-60-ghz.html

        192G RAM
        400G local SSD

    Cluster total:

        Initially deployed with 8 machines
        Expanding to 40 machines in 2019/2020

    OpenStack allocation:

         20 virtual machines
        200 vcpus (HyperThreading disabled)

         1TB volume storage
           0 object storage

    GDAF hardware is descibed in "Gaia Data Analytics Framework (GDAF) technical configuration"
    [GAIA-C9-TN-UB-FJL-002][2016-11-23]
    [https://gaia.esac.esa.int/dpacsvn/DPAC/CU9/docs/WP970_Science_Enabling_Apps/nonECSS/TechNotes/GAIA-C9-SP-UB-FJL-002/]

    Assuming GDAF platform hasn't been expended since 2016, then Cambridge OpenStack
    allocation of 200 vcpu is roughly equivalent to the (original) GDAF hardware.
    (*) big assumption ?

    If we assume 1:1 equivalence between GDAF cpu-threads and OpenStack vcpus

        2×10⁴ cpu-hours / 200 vcpus = 100 hours
                                    = 4.16 days per iteration

    (*) GDAF has HyperThreading enabled
    (*) Cambridge Cumulus system has HyperThreading disabled

---- ---- ---- ----

Resources on Cambridge OpenStack system

Current allocation of 200 vcpu on 20 instances is (roughly) equivalent
to one GDAF 2016 instance.

Likleyhood is that the GDAF platform itself has evolved in the last 3 years,
so the GDAF platform used for the Mor-2019 paper is not the same as the
GDAF platform described in the Gaia documents in 2016.

There will always be some level of performance overhead in using virtualised
cloud based resources compared to physical hardware specificially configured
to run Hadoop and Spark.

We need to factor this in when requesting resources to create a cloud compute
system with equivalent performance to the GDAF hardware platform.

---- ---- ---- ----

Test system at Edinburgh

For comparison, at ROE we have the following testbed machines:

    LSST:UK worker nodes (x4)

        2x Intel(R) Xeon(R) Gold 6132 CPU @ 2.60GHz
        28 core (56 hyper-thread)
        512Gbyte RAM

    Gaia worker nodes (x2)(+2 in Jan)

        2x AMD EPYC 7451 24-Core Processor
        48 core (96 hyper-thread)
        256Gbyte RAM

    Total test hardware at ROE by end January 2020

        (4*28)+(4*48)   = 304 cpu cores (604 hyper-threaded)
        (4*512)+(4*256) = 3Tbyte RAM

---- ---- ---- ----

Resource request for 2020

Increase the total resource allocation from 200 vcpu on 20 instances and 500Gbyte of memory to 600 vcpu on 60 instances and 3Tbyte of memory, spread across three OpenStack projects.

Each project would get 200 vcpu on 20 instances and 1Tbyte of memory.

Increase the disc space from 1Tbyte of volume storage to 12Tbyte of volume storage and 12Tbyte of object storage, spread across three OpenStack projects.

Each of which would get 4Tbyte of volume storage and 4Tbyte of object storage

The total allocation for 2020 :

    iris-gaia-dev   200 vcpu on 20 instances, 1Tbyte RAM, 4Tbyte volume storage, 4Tbyte object storage
    iris-gaia-test  200 vcpu on 20 instances, 1Tbyte RAM, 4Tbyte volume storage, 4Tbyte object storage
    iris-gaia-live  200 vcpu on 20 instances, 1Tbyte RAM, 4Tbyte volume storage, 4Tbyte object storage

The test and live systems can come online in January or Feburary next year, starting with lower resource allocations and buiuld up to the total allocatation over Q1..Q2 2020.

However we need the volume and object storage on the dev project as soon as possible
if we want to have something up and running in time for the CU9 plenary meeting in January.




