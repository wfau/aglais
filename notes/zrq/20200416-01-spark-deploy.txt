#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

# -----------------------------------------------------
# Create our project config file.
#[user@desktop]

    cat > "${HOME:?}/aglais.env" << 'EOF'

source "${HOME:?}/projects.env"
AGLAIS_REPO='git@github.com:Zarquan/aglais.git'
AGLAIS_HOME="${PROJECTS_ROOT:?}/WFAU/aglais"
AGLAIS_CODE="${AGLAIS_HOME:?}/github-zrq"
AGLAIS_CLOUD=gaia-prod
AGLAIS_USER=albert

EOF

# -----------------------------------------------------
# Create our cloud config file.
#[user@desktop]

    cat > "${HOME:?}/clouds.yaml" << EOF

clouds:

  gaia-dev:
    auth:
      auth_url: https://cumulus.openstack.hpc.cam.ac.uk:5000/v3
      application_credential_id:     '$(secret 'zrq-gaia-dev.APP_CREDENTIAL_ID')'
      application_credential_secret: '$(secret 'zrq-gaia-dev.APP_CREDENTIAL_SECRET')'
    region_name: "RegionOne"
    interface: "public"
    identity_api_version: 3
    auth_type: "v3applicationcredential"

  gaia-dev-super:
    auth:
      auth_url: https://cumulus.openstack.hpc.cam.ac.uk:5000/v3
      application_credential_id:     '$(secret 'zrq-gaia-dev.APX_CREDENTIAL_ID')'
      application_credential_secret: '$(secret 'zrq-gaia-dev.APX_CREDENTIAL_SECRET')'
    region_name: "RegionOne"
    interface: "public"
    identity_api_version: 3
    auth_type: "v3applicationcredential"

  gaia-prod:
    auth:
      auth_url: https://cumulus.openstack.hpc.cam.ac.uk:5000/v3
      application_credential_id:     '$(secret 'zrq-gaia-prod.APP_CREDENTIAL_ID')'
      application_credential_secret: '$(secret 'zrq-gaia-prod.APP_CREDENTIAL_SECRET')'
    region_name: "RegionOne"
    interface: "public"
    identity_api_version: 3
    auth_type: "v3applicationcredential"

  gaia-prod-super:
    auth:
      auth_url: https://cumulus.openstack.hpc.cam.ac.uk:5000/v3
      application_credential_id:     '$(secret 'zrq-gaia-prod.APX_CREDENTIAL_ID')'
      application_credential_secret: '$(secret 'zrq-gaia-prod.APX_CREDENTIAL_SECRET')'
    region_name: "RegionOne"
    interface: "public"
    identity_api_version: 3
    auth_type: "v3applicationcredential"

EOF


# -----------------------------------------------------
# Create a container to work with.
# https://podman.readthedocs.io/en/latest/markdown/podman-run.1.html
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --env "clouduser=${AGLAIS_USER:?}" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --env "ANSIBLE_CODE=/mnt/ansible" \
        --volume "${AGLAIS_CODE:?}/experiments/zrq/ansible:/mnt/ansible:ro,z" \
        atolmis/ansible-client:latest \
        bash

    >   ....
    >   ....


# -----------------------------------------------------
# Create our Ansible include vars file.
#[root@ansibler]

    cat > /tmp/ansible-vars.yml << EOF
buildtag:  'aglais-$(date '+%Y%m%d')'
cloudname: '${cloudname}'
clouduser: '${clouduser}'
EOF


# -----------------------------------------------------
# Run the scripts from the ansible directory.
#[root@ansibler]

    # TODO Make this the working directory in the container ?
    # --env ANSIBLE_CODE=/mnt/ansible

    cd "${ANSIBLE_CODE:?}"


# -----------------------------------------------------
# Run the initial part of our deplyment.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-01.yml"

    >   ....
    >   PLAY RECAP ..
    >   gateway                    : ok=8    changed=6    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   localhost                  : ok=25   changed=18   unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   
    >   master01                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   master02                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker01                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker02                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker03                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker04                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker05                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker06                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker07                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker08                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   


# -----------------------------------------------------
# Run the Hadoop part of our deplyment.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-02.yml"

    >   ....
    >   PLAY RECAP ..
    >   localhost                  : ok=39   changed=31   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   master01                   : ok=24   changed=18   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   master02                   : ok=19   changed=13   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker01                   : ok=22   changed=19   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker02                   : ok=22   changed=19   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker03                   : ok=22   changed=19   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker04                   : ok=22   changed=19   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker05                   : ok=22   changed=19   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker06                   : ok=22   changed=19   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker07                   : ok=22   changed=19   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker08                   : ok=22   changed=19   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   


# -----------------------------------------------------
# Format the HDFS NameNode on master01.
#[root@ansibler]

    ssh master01 \
        '
        hdfs namenode -format
        '

    >   2020-04-17 01:30:41,970 INFO namenode.NameNode: STARTUP_MSG: 
    >   /..
    >   STARTUP_MSG: Starting NameNode
    >   STARTUP_MSG:   host = master01/10.10.0.5
    >   STARTUP_MSG:   args = [-format]
    >   STARTUP_MSG:   version = 3.2.1
    >   STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-text-1.4.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.19.jar:/opt/hadoop/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.11.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/common/lib/jetty-servlet-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-2.3.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.10.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.6.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.5.jar:/opt/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.18.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop/share/hadoop/common/lib/asm-5.0.4.jar:/opt/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang3-3.7.jar:/opt/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-2.9.8.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.19.jar:/opt/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/hadoop/common/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/accessors-smart-1.2.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.6.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.19.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-kms-3.2.1.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-3.2.1.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.2.1.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-text-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/json-smart-2.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/asm-5.0.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-2.9.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-net-3.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.jar:/opt/hadoop/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.8.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/opt/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.8.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.8.jar:/opt/hadoop/share/hadoop/yarn/lib/objenesis-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.2.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.2.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.2.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.2.1.jar
    >   STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z
    >   STARTUP_MSG:   java = 1.8.0_242
    >   ************************************************************/
    >   2020-04-17 01:30:41,979 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
    >   2020-04-17 01:30:42,057 INFO namenode.NameNode: createNameNode [-format]
    >   2020-04-17 01:30:42,419 INFO common.Util: Assuming 'file' scheme for path /var/local/hadoop/namenode/fsimage in configuration.
    >   Formatting using clusterid: CID-b6702a67-afa7-46f6-8607-99d2a2037588
    >   2020-04-17 01:30:42,420 INFO common.Util: Assuming 'file' scheme for path /var/local/hadoop/namenode/fsimage in configuration.
    >   2020-04-17 01:30:42,459 INFO namenode.FSEditLog: Edit logging is async:true
    >   2020-04-17 01:30:42,474 INFO namenode.FSNamesystem: KeyProvider: null
    >   2020-04-17 01:30:42,475 INFO namenode.FSNamesystem: fsLock is fair: true
    >   2020-04-17 01:30:42,476 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
    >   2020-04-17 01:30:42,481 INFO namenode.FSNamesystem: fsOwner             = fedora (auth:SIMPLE)
    >   2020-04-17 01:30:42,481 INFO namenode.FSNamesystem: supergroup          = supergroup
    >   2020-04-17 01:30:42,481 INFO namenode.FSNamesystem: isPermissionEnabled = true
    >   2020-04-17 01:30:42,481 INFO namenode.FSNamesystem: HA Enabled: false
    >   2020-04-17 01:30:42,530 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
    >   2020-04-17 01:30:42,549 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
    >   2020-04-17 01:30:42,549 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
    >   2020-04-17 01:30:42,554 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
    >   2020-04-17 01:30:42,554 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Apr 17 01:30:42
    >   2020-04-17 01:30:42,556 INFO util.GSet: Computing capacity for map BlocksMap
    >   2020-04-17 01:30:42,556 INFO util.GSet: VM type       = 64-bit
    >   2020-04-17 01:30:42,557 INFO util.GSet: 2.0% max memory 1.3 GB = 26.5 MB
    >   2020-04-17 01:30:42,557 INFO util.GSet: capacity      = 2^22 = 4194304 entries
    >   2020-04-17 01:30:42,584 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
    >   2020-04-17 01:30:42,584 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
    >   2020-04-17 01:30:42,591 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
    >   2020-04-17 01:30:42,591 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
    >   2020-04-17 01:30:42,591 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
    >   2020-04-17 01:30:42,591 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
    >   2020-04-17 01:30:42,591 INFO blockmanagement.BlockManager: defaultReplication         = 2
    >   2020-04-17 01:30:42,591 INFO blockmanagement.BlockManager: maxReplication             = 512
    >   2020-04-17 01:30:42,591 INFO blockmanagement.BlockManager: minReplication             = 1
    >   2020-04-17 01:30:42,591 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
    >   2020-04-17 01:30:42,592 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
    >   2020-04-17 01:30:42,592 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
    >   2020-04-17 01:30:42,592 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
    >   2020-04-17 01:30:42,617 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
    >   2020-04-17 01:30:42,617 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
    >   2020-04-17 01:30:42,617 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
    >   2020-04-17 01:30:42,617 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
    >   2020-04-17 01:30:42,630 INFO util.GSet: Computing capacity for map INodeMap
    >   2020-04-17 01:30:42,630 INFO util.GSet: VM type       = 64-bit
    >   2020-04-17 01:30:42,630 INFO util.GSet: 1.0% max memory 1.3 GB = 13.2 MB
    >   2020-04-17 01:30:42,630 INFO util.GSet: capacity      = 2^21 = 2097152 entries
    >   2020-04-17 01:30:42,633 INFO namenode.FSDirectory: ACLs enabled? false
    >   2020-04-17 01:30:42,633 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
    >   2020-04-17 01:30:42,633 INFO namenode.FSDirectory: XAttrs enabled? true
    >   2020-04-17 01:30:42,633 INFO namenode.NameNode: Caching file names occurring more than 10 times
    >   2020-04-17 01:30:42,637 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
    >   2020-04-17 01:30:42,638 INFO snapshot.SnapshotManager: SkipList is disabled
    >   2020-04-17 01:30:42,642 INFO util.GSet: Computing capacity for map cachedBlocks
    >   2020-04-17 01:30:42,642 INFO util.GSet: VM type       = 64-bit
    >   2020-04-17 01:30:42,643 INFO util.GSet: 0.25% max memory 1.3 GB = 3.3 MB
    >   2020-04-17 01:30:42,643 INFO util.GSet: capacity      = 2^19 = 524288 entries
    >   2020-04-17 01:30:42,650 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
    >   2020-04-17 01:30:42,650 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
    >   2020-04-17 01:30:42,650 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
    >   2020-04-17 01:30:42,654 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
    >   2020-04-17 01:30:42,654 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
    >   2020-04-17 01:30:42,655 INFO util.GSet: Computing capacity for map NameNodeRetryCache
    >   2020-04-17 01:30:42,655 INFO util.GSet: VM type       = 64-bit
    >   2020-04-17 01:30:42,655 INFO util.GSet: 0.029999999329447746% max memory 1.3 GB = 406.9 KB
    >   2020-04-17 01:30:42,655 INFO util.GSet: capacity      = 2^16 = 65536 entries
    >   2020-04-17 01:30:42,684 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1499846343-10.10.0.5-1587087042677
    >   2020-04-17 01:30:42,702 INFO common.Storage: Storage directory /var/local/hadoop/namenode/fsimage has been successfully formatted.
    >   2020-04-17 01:30:42,790 INFO namenode.FSImageFormatProtobuf: Saving image file /var/local/hadoop/namenode/fsimage/current/fsimage.ckpt_0000000000000000000 using no compression
    >   2020-04-17 01:30:42,869 INFO namenode.FSImageFormatProtobuf: Image file /var/local/hadoop/namenode/fsimage/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved in 0 seconds .
    >   2020-04-17 01:30:42,876 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
    >   2020-04-17 01:30:42,884 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
    >   2020-04-17 01:30:42,885 INFO namenode.NameNode: SHUTDOWN_MSG: 
    >   /..
    >   SHUTDOWN_MSG: Shutting down NameNode at master01/10.10.0.5
    >   ************************************************************/


# -----------------------------------------------------
# Start the HDFS services.
#[root@ansibler]

    ssh master01 \
        '
        start-dfs.sh
        '

    >   Starting namenodes on [master01]
    >   Starting datanodes
    >   Starting secondary namenodes [aglais-20200417-master01.novalocal]


# -----------------------------------------------------
# Check the HDFS status.
#[root@ansibler]

    ssh master01 \
        '
        hdfs dfsadmin -report
        '

    >   Configured Capacity: 4398046511104 (4 TB)
    >   Present Capacity: 4380721971200 (3.98 TB)
    >   DFS Remaining: 4380721938432 (3.98 TB)
    >   DFS Used: 32768 (32 KB)
    >   DFS Used%: 0.00%
    >   Replicated Blocks:
    >   	Under replicated blocks: 0
    >   	Blocks with corrupt replicas: 0
    >   	Missing blocks: 0
    >   	Missing blocks (with replication factor 1): 0
    >   	Low redundancy blocks with highest priority to recover: 0
    >   	Pending deletion blocks: 0
    >   Erasure Coded Block Groups: 
    >   	Low redundancy block groups: 0
    >   	Block groups with corrupt internal blocks: 0
    >   	Missing block groups: 0
    >   	Low redundancy blocks with highest priority to recover: 0
    >   	Pending deletion blocks: 0
    >   
    >   -------------------------------------------------
    >   Live datanodes (8):
    >   
    >   Name: 10.10.0.10:9866 (worker01)
    >   Hostname: worker01
    >   Decommission Status : Normal
    >   Configured Capacity: 549755813888 (512 GB)
    >   DFS Used: 4096 (4 KB)
    >   Non DFS Used: 17297408 (16.50 MB)
    >   DFS Remaining: 547590242304 (509.98 GB)
    >   DFS Used%: 0.00%
    >   DFS Remaining%: 99.61%
    >   Configured Cache Capacity: 0 (0 B)
    >   Cache Used: 0 (0 B)
    >   Cache Remaining: 0 (0 B)
    >   Cache Used%: 100.00%
    >   Cache Remaining%: 0.00%
    >   Xceivers: 1
    >   Last contact: Fri Apr 17 01:32:28 UTC 2020
    >   Last Block Report: Fri Apr 17 01:32:07 UTC 2020
    >   Num of Blocks: 0
    >   
    >   
    >   Name: 10.10.0.14:9866 (worker04)
    >   Hostname: worker04
    >   Decommission Status : Normal
    >   Configured Capacity: 549755813888 (512 GB)
    >   DFS Used: 4096 (4 KB)
    >   Non DFS Used: 17297408 (16.50 MB)
    >   DFS Remaining: 547590242304 (509.98 GB)
    >   DFS Used%: 0.00%
    >   DFS Remaining%: 99.61%
    >   Configured Cache Capacity: 0 (0 B)
    >   Cache Used: 0 (0 B)
    >   Cache Remaining: 0 (0 B)
    >   Cache Used%: 100.00%
    >   Cache Remaining%: 0.00%
    >   Xceivers: 1
    >   Last contact: Fri Apr 17 01:32:28 UTC 2020
    >   Last Block Report: Fri Apr 17 01:32:07 UTC 2020
    >   Num of Blocks: 0
    >   
    >   
    >   Name: 10.10.0.17:9866 (worker07)
    >   Hostname: worker07
    >   Decommission Status : Normal
    >   Configured Capacity: 549755813888 (512 GB)
    >   DFS Used: 4096 (4 KB)
    >   Non DFS Used: 17297408 (16.50 MB)
    >   DFS Remaining: 547590242304 (509.98 GB)
    >   DFS Used%: 0.00%
    >   DFS Remaining%: 99.61%
    >   Configured Cache Capacity: 0 (0 B)
    >   Cache Used: 0 (0 B)
    >   Cache Remaining: 0 (0 B)
    >   Cache Used%: 100.00%
    >   Cache Remaining%: 0.00%
    >   Xceivers: 1
    >   Last contact: Fri Apr 17 01:32:28 UTC 2020
    >   Last Block Report: Fri Apr 17 01:32:07 UTC 2020
    >   Num of Blocks: 0
    >   
    >   
    >   Name: 10.10.0.22:9866 (worker03)
    >   Hostname: worker03
    >   Decommission Status : Normal
    >   Configured Capacity: 549755813888 (512 GB)
    >   DFS Used: 4096 (4 KB)
    >   Non DFS Used: 17297408 (16.50 MB)
    >   DFS Remaining: 547590242304 (509.98 GB)
    >   DFS Used%: 0.00%
    >   DFS Remaining%: 99.61%
    >   Configured Cache Capacity: 0 (0 B)
    >   Cache Used: 0 (0 B)
    >   Cache Remaining: 0 (0 B)
    >   Cache Used%: 100.00%
    >   Cache Remaining%: 0.00%
    >   Xceivers: 1
    >   Last contact: Fri Apr 17 01:32:28 UTC 2020
    >   Last Block Report: Fri Apr 17 01:32:07 UTC 2020
    >   Num of Blocks: 0
    >   
    >   
    >   Name: 10.10.0.4:9866 (worker08)
    >   Hostname: worker08
    >   Decommission Status : Normal
    >   Configured Capacity: 549755813888 (512 GB)
    >   DFS Used: 4096 (4 KB)
    >   Non DFS Used: 17297408 (16.50 MB)
    >   DFS Remaining: 547590242304 (509.98 GB)
    >   DFS Used%: 0.00%
    >   DFS Remaining%: 99.61%
    >   Configured Cache Capacity: 0 (0 B)
    >   Cache Used: 0 (0 B)
    >   Cache Remaining: 0 (0 B)
    >   Cache Used%: 100.00%
    >   Cache Remaining%: 0.00%
    >   Xceivers: 1
    >   Last contact: Fri Apr 17 01:32:28 UTC 2020
    >   Last Block Report: Fri Apr 17 01:32:07 UTC 2020
    >   Num of Blocks: 0
    >   
    >   
    >   Name: 10.10.0.7:9866 (worker02)
    >   Hostname: worker02
    >   Decommission Status : Normal
    >   Configured Capacity: 549755813888 (512 GB)
    >   DFS Used: 4096 (4 KB)
    >   Non DFS Used: 17297408 (16.50 MB)
    >   DFS Remaining: 547590242304 (509.98 GB)
    >   DFS Used%: 0.00%
    >   DFS Remaining%: 99.61%
    >   Configured Cache Capacity: 0 (0 B)
    >   Cache Used: 0 (0 B)
    >   Cache Remaining: 0 (0 B)
    >   Cache Used%: 100.00%
    >   Cache Remaining%: 0.00%
    >   Xceivers: 1
    >   Last contact: Fri Apr 17 01:32:28 UTC 2020
    >   Last Block Report: Fri Apr 17 01:32:07 UTC 2020
    >   Num of Blocks: 0
    >   
    >   
    >   Name: 10.10.0.8:9866 (worker06)
    >   Hostname: worker06
    >   Decommission Status : Normal
    >   Configured Capacity: 549755813888 (512 GB)
    >   DFS Used: 4096 (4 KB)
    >   Non DFS Used: 17297408 (16.50 MB)
    >   DFS Remaining: 547590242304 (509.98 GB)
    >   DFS Used%: 0.00%
    >   DFS Remaining%: 99.61%
    >   Configured Cache Capacity: 0 (0 B)
    >   Cache Used: 0 (0 B)
    >   Cache Remaining: 0 (0 B)
    >   Cache Used%: 100.00%
    >   Cache Remaining%: 0.00%
    >   Xceivers: 1
    >   Last contact: Fri Apr 17 01:32:28 UTC 2020
    >   Last Block Report: Fri Apr 17 01:32:07 UTC 2020
    >   Num of Blocks: 0
    >   
    >   
    >   Name: 10.10.0.9:9866 (worker05)
    >   Hostname: worker05
    >   Decommission Status : Normal
    >   Configured Capacity: 549755813888 (512 GB)
    >   DFS Used: 4096 (4 KB)
    >   Non DFS Used: 17297408 (16.50 MB)
    >   DFS Remaining: 547590242304 (509.98 GB)
    >   DFS Used%: 0.00%
    >   DFS Remaining%: 99.61%
    >   Configured Cache Capacity: 0 (0 B)
    >   Cache Used: 0 (0 B)
    >   Cache Remaining: 0 (0 B)
    >   Cache Used%: 100.00%
    >   Cache Remaining%: 0.00%
    >   Xceivers: 1
    >   Last contact: Fri Apr 17 01:32:28 UTC 2020
    >   Last Block Report: Fri Apr 17 01:32:07 UTC 2020
    >   Num of Blocks: 0


# -----------------------------------------------------
# Start the YARN services.
#[root@ansibler]

    ssh master01 \
        '
        start-yarn.sh
        '

    >   Starting resourcemanager
    >   Starting nodemanagers


# -----------------------------------------------------
# Install the Spark binaries.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "20-install-spark.yml"

    >   ....
    >   PLAY RECAP ..
    >   master01                   : ok=3    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   


# -----------------------------------------------------
# Add the security rules for Spark.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "21-config-spark-security.yml"

    >   ....
    >   PLAY RECAP ..
    >   localhost                  : ok=6    changed=6    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   


# -----------------------------------------------------
# Create our Spark configuration.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "22-config-spark-master.yml"

    >   ....
    >   PLAY RECAP ..
    >   master01                   : ok=2    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   


# -----------------------------------------------------
# Create our HDFS log directory.
#[root@ansibler]

    ssh master01 \
        '
        hdfs dfs -mkdir /spark-log
        '


# -----------------------------------------------------
# Run the SparkPi example from the Spark install instructtions.
# https://spark.apache.org/docs/3.0.0-preview2/running-on-yarn.html#launching-spark-on-yarn
#[root@ansibler]

    ssh master01 \
        '
        cd "${SPARK_HOME:?}"

        spark-submit \
            --class org.apache.spark.examples.SparkPi \
            --master yarn \
            --deploy-mode cluster \
            --driver-memory 1g \
            --executor-memory 1g \
            --executor-cores 1 \
            examples/jars/spark-examples*.jar \
                10
        '

    >   2020-04-17 01:35:17,763 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    >   2020-04-17 01:35:17,820 INFO client.RMProxy: Connecting to ResourceManager at master01/10.10.0.5:8032
    >   2020-04-17 01:35:18,105 INFO yarn.Client: Requesting a new application from cluster with 8 NodeManagers
    >   2020-04-17 01:35:18,569 INFO conf.Configuration: resource-types.xml not found
    >   2020-04-17 01:35:18,570 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
    >   2020-04-17 01:35:18,587 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
    >   2020-04-17 01:35:18,588 INFO yarn.Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead
    >   2020-04-17 01:35:18,588 INFO yarn.Client: Setting up container launch context for our AM
    >   2020-04-17 01:35:18,589 INFO yarn.Client: Setting up the launch environment for our AM container
    >   2020-04-17 01:35:18,609 INFO yarn.Client: Preparing resources for our AM container
    >   2020-04-17 01:35:18,662 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
    >   2020-04-17 01:35:21,025 INFO yarn.Client: Uploading resource file:/tmp/spark-ac53ceeb-ba42-4e85-abed-98a8ee3105ad/__spark_libs__8384341613453426356.zip -> hdfs://master01:9000/user/fedora/.sparkStaging/application_1587087175431_0001/__spark_libs__8384341613453426356.zip
    >   2020-04-17 01:35:22,552 INFO yarn.Client: Uploading resource file:/opt/spark-3.0.0-preview2-bin-hadoop3.2/examples/jars/spark-examples_2.12-3.0.0-preview2.jar -> hdfs://master01:9000/user/fedora/.sparkStaging/application_1587087175431_0001/spark-examples_2.12-3.0.0-preview2.jar
    >   2020-04-17 01:35:22,961 INFO yarn.Client: Uploading resource file:/tmp/spark-ac53ceeb-ba42-4e85-abed-98a8ee3105ad/__spark_conf__8841125858989044572.zip -> hdfs://master01:9000/user/fedora/.sparkStaging/application_1587087175431_0001/__spark_conf__.zip
    >   2020-04-17 01:35:23,161 INFO spark.SecurityManager: Changing view acls to: fedora
    >   2020-04-17 01:35:23,162 INFO spark.SecurityManager: Changing modify acls to: fedora
    >   2020-04-17 01:35:23,162 INFO spark.SecurityManager: Changing view acls groups to: 
    >   2020-04-17 01:35:23,163 INFO spark.SecurityManager: Changing modify acls groups to: 
    >   2020-04-17 01:35:23,163 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fedora); groups with view permissions: Set(); users  with modify permissions: Set(fedora); groups with modify permissions: Set()
    >   2020-04-17 01:35:23,216 INFO yarn.Client: Submitting application application_1587087175431_0001 to ResourceManager
    >   2020-04-17 01:35:23,448 INFO impl.YarnClientImpl: Submitted application application_1587087175431_0001
    >   2020-04-17 01:35:24,454 INFO yarn.Client: Application report for application_1587087175431_0001 (state: ACCEPTED)
    >   2020-04-17 01:35:24,457 INFO yarn.Client: 
    >   	 client token: N/A
    >   	 diagnostics: [Fri Apr 17 01:35:24 +0000 2020] Scheduler has assigned a container for AM, waiting for AM container to be launched
    >   	 ApplicationMaster host: N/A
    >   	 ApplicationMaster RPC port: -1
    >   	 queue: default
    >   	 start time: 1587087323338
    >   	 final status: UNDEFINED
    >   	 tracking URL: http://master01:8088/proxy/application_1587087175431_0001/
    >   	 user: fedora
    >   2020-04-17 01:35:25,460 INFO yarn.Client: Application report for application_1587087175431_0001 (state: ACCEPTED)
    >   2020-04-17 01:35:26,463 INFO yarn.Client: Application report for application_1587087175431_0001 (state: ACCEPTED)
    >   2020-04-17 01:35:27,466 INFO yarn.Client: Application report for application_1587087175431_0001 (state: ACCEPTED)
    >   2020-04-17 01:35:28,470 INFO yarn.Client: Application report for application_1587087175431_0001 (state: ACCEPTED)
    >   2020-04-17 01:35:29,473 INFO yarn.Client: Application report for application_1587087175431_0001 (state: ACCEPTED)
    >   2020-04-17 01:35:30,476 INFO yarn.Client: Application report for application_1587087175431_0001 (state: ACCEPTED)
    >   2020-04-17 01:35:31,480 INFO yarn.Client: Application report for application_1587087175431_0001 (state: ACCEPTED)
    >   2020-04-17 01:35:32,483 INFO yarn.Client: Application report for application_1587087175431_0001 (state: RUNNING)
    >   2020-04-17 01:35:32,484 INFO yarn.Client: 
    >   	 client token: N/A
    >   	 diagnostics: N/A
    >   	 ApplicationMaster host: worker06
    >   	 ApplicationMaster RPC port: 33877
    >   	 queue: default
    >   	 start time: 1587087323338
    >   	 final status: UNDEFINED
    >   	 tracking URL: http://master01:8088/proxy/application_1587087175431_0001/
    >   	 user: fedora
    >   2020-04-17 01:35:33,487 INFO yarn.Client: Application report for application_1587087175431_0001 (state: RUNNING)
    >   2020-04-17 01:35:34,490 INFO yarn.Client: Application report for application_1587087175431_0001 (state: RUNNING)
    >   2020-04-17 01:35:35,493 INFO yarn.Client: Application report for application_1587087175431_0001 (state: RUNNING)
    >   2020-04-17 01:35:36,496 INFO yarn.Client: Application report for application_1587087175431_0001 (state: RUNNING)
    >   2020-04-17 01:35:37,498 INFO yarn.Client: Application report for application_1587087175431_0001 (state: RUNNING)
    >   2020-04-17 01:35:38,504 INFO yarn.Client: Application report for application_1587087175431_0001 (state: RUNNING)
    >   2020-04-17 01:35:39,508 INFO yarn.Client: Application report for application_1587087175431_0001 (state: RUNNING)
    >   2020-04-17 01:35:40,510 INFO yarn.Client: Application report for application_1587087175431_0001 (state: FINISHED)
    >   2020-04-17 01:35:40,511 INFO yarn.Client: 
    >   	 client token: N/A
    >   	 diagnostics: N/A
    >   	 ApplicationMaster host: worker06
    >   	 ApplicationMaster RPC port: 33877
    >   	 queue: default
    >   	 start time: 1587087323338
    >   	 final status: SUCCEEDED
    >   	 tracking URL: http://master01:8088/proxy/application_1587087175431_0001/
    >   	 user: fedora
    >   2020-04-17 01:35:40,525 INFO util.ShutdownHookManager: Shutdown hook called
    >   2020-04-17 01:35:40,525 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-7e5e8c44-bed1-4d68-bc54-099fcca768d6
    >   2020-04-17 01:35:40,535 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-ac53ceeb-ba42-4e85-abed-98a8ee3105ad


# -----------------------------------------------------
# Install Python.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "23-install-python.yml"

    >   ....
    >   PLAY RECAP ..
    >   master01                   : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   master02                   : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker01                   : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker02                   : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker03                   : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker04                   : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker05                   : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker06                   : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker07                   : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
    >   worker08                   : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   


# -----------------------------------------------------
# Install PySpark.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "24-install-pyspark.yml"

    >   ....
    >   PLAY RECAP ..
    >   master01                   : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   


# -----------------------------------------------------
# Run a PySpark example ....
# https://www.tutorialspoint.com/pyspark/pyspark_sparkcontext.htm
#[root@ansibler]

    ssh master01

        pyspark
        

    >   Python 3.7.3 (default, Mar 27 2019, 13:36:35) 
    >   [GCC 9.0.1 20190227 (Red Hat 9.0.1-0.8)] on linux
    >   Type "help", "copyright", "credits" or "license" for more information.
    >   2020-04-17 01:37:49,520 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    >   Setting default log level to "WARN".
    >   To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
    >   2020-04-17 01:37:52,076 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
    >   Welcome to
    >         ____              __
    >        / __/__  ___ _____/ /__
    >       _\ \/ _ \/ _ `/ __/  '_/
    >      /__ / .__/\_,_/_/ /_/\_\   version 3.0.0-preview2
    >         /_/
    >   
    >   Using Python version 3.7.3 (default, Mar 27 2019 13:36:35)
    >   SparkSession available as 'spark'.
    >   >>> 


        logFile = "file:///opt/hadoop/README.txt"
        logData = sc.textFile(logFile).cache()
        numAs = logData.filter(lambda s: 'a' in s).count()
        numBs = logData.filter(lambda s: 'b' in s).count()
        print(
            'Lines with a: {}, lines with b: {}'.format(
                numAs,
                numBs
                )
            )

    >   Lines with a: 25, lines with b: 7

    exit


# -----------------------------------------------------
# Run a PySpark example ....
# https://www.tutorialspoint.com/pyspark/pyspark_sparkcontext.htm
#[root@ansibler]

    ssh master01


        cat > firstapp.py << 'EOF'
from pyspark import SparkContext
sc = SparkContext("local", "first app")
logFile = "file:///opt/hadoop/README.txt"
logData = sc.textFile(logFile).cache()
numAs = logData.filter(lambda s: 'a' in s).count()
numBs = logData.filter(lambda s: 'b' in s).count()
print(
    'Lines with a: {}, lines with b: {}'.format(
        numAs,
        numBs
        )
    )
EOF


    spark-submit firstapp.py

    >   2020-04-17 01:39:15,458 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    >   2020-04-17 01:39:16,090 INFO spark.SparkContext: Running Spark version 3.0.0-preview2
    >   2020-04-17 01:39:16,126 INFO resource.ResourceUtils: ==============================================================
    >   2020-04-17 01:39:16,127 INFO resource.ResourceUtils: Resources for spark.driver:
    >   
    >   2020-04-17 01:39:16,127 INFO resource.ResourceUtils: ==============================================================
    >   2020-04-17 01:39:16,128 INFO spark.SparkContext: Submitted application: first app
    >   2020-04-17 01:39:16,183 INFO spark.SecurityManager: Changing view acls to: fedora
    >   2020-04-17 01:39:16,184 INFO spark.SecurityManager: Changing modify acls to: fedora
    >   2020-04-17 01:39:16,184 INFO spark.SecurityManager: Changing view acls groups to: 
    >   2020-04-17 01:39:16,184 INFO spark.SecurityManager: Changing modify acls groups to: 
    >   2020-04-17 01:39:16,184 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fedora); groups with view permissions: Set(); users  with modify permissions: Set(fedora); groups with modify permissions: Set()
    >   2020-04-17 01:39:16,413 INFO util.Utils: Successfully started service 'sparkDriver' on port 34561.
    >   2020-04-17 01:39:16,450 INFO spark.SparkEnv: Registering MapOutputTracker
    >   2020-04-17 01:39:16,484 INFO spark.SparkEnv: Registering BlockManagerMaster
    >   2020-04-17 01:39:16,501 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
    >   2020-04-17 01:39:16,502 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
    >   2020-04-17 01:39:16,510 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat
    >   2020-04-17 01:39:16,525 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-8b887944-7a8d-4f08-8651-520e36944ad9
    >   2020-04-17 01:39:16,551 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MiB
    >   2020-04-17 01:39:16,569 INFO spark.SparkEnv: Registering OutputCommitCoordinator
    >   2020-04-17 01:39:16,672 INFO util.log: Logging initialized @2439ms to org.sparkproject.jetty.util.log.Slf4jLog
    >   2020-04-17 01:39:16,776 INFO server.Server: jetty-9.4.z-SNAPSHOT; built: 2019-04-29T20:42:08.989Z; git: e1bc35120a6617ee3df052294e433f3a25ce7097; jvm 1.8.0_242-b08
    >   2020-04-17 01:39:16,797 INFO server.Server: Started @2565ms
    >   2020-04-17 01:39:16,825 INFO server.AbstractConnector: Started ServerConnector@6778c5f1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
    >   2020-04-17 01:39:16,825 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
    >   2020-04-17 01:39:16,849 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@42f81e7d{/jobs,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,851 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@783a8189{/jobs/json,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,851 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c614dec{/jobs/job,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,854 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@aa234a5{/jobs/job/json,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,855 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@715d6aa7{/stages,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,855 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@261079eb{/stages/json,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,856 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2bbe571a{/stages/stage,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,857 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58455e29{/stages/stage/json,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,858 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a4eca12{/stages/pool,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,858 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@fb9f3bb{/stages/pool/json,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,859 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10b4aae{/storage,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,860 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46e6dd72{/storage/json,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,860 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@81525ea{/storage/rdd,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,861 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@68ebb6b5{/storage/rdd/json,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,862 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18e900ae{/environment,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,862 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@297daca{/environment/json,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,863 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79b251d8{/executors,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,864 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1684f413{/executors/json,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,864 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2243ed6f{/executors/threadDump,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,865 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@701c4aaf{/executors/threadDump/json,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,873 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26bedfce{/static,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,873 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d929897{/,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,874 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5483ac38{/api,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,875 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60c1d0a9{/jobs/job/kill,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,876 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36e74e4f{/stages/stage/kill,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:16,878 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://master01:4040
    >   2020-04-17 01:39:17,098 INFO executor.Executor: Starting executor ID driver on host master01
    >   2020-04-17 01:39:17,126 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39945.
    >   2020-04-17 01:39:17,126 INFO netty.NettyBlockTransferService: Server created on master01:39945
    >   2020-04-17 01:39:17,128 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
    >   2020-04-17 01:39:17,134 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, master01, 39945, None)
    >   2020-04-17 01:39:17,138 INFO storage.BlockManagerMasterEndpoint: Registering block manager master01:39945 with 366.3 MiB RAM, BlockManagerId(driver, master01, 39945, None)
    >   2020-04-17 01:39:17,141 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, master01, 39945, None)
    >   2020-04-17 01:39:17,142 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, master01, 39945, None)
    >   2020-04-17 01:39:17,295 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26b831c7{/metrics/json,null,AVAILABLE,@Spark}
    >   2020-04-17 01:39:17,964 INFO history.SingleEventLogFileWriter: Logging events to hdfs://master01:9000/spark-log/local-1587087556970.inprogress
    >   2020-04-17 01:39:18,729 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 294.3 KiB, free 366.0 MiB)
    >   2020-04-17 01:39:18,778 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 366.0 MiB)
    >   2020-04-17 01:39:18,780 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on master01:39945 (size: 27.3 KiB, free: 366.3 MiB)
    >   2020-04-17 01:39:18,786 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0
    >   2020-04-17 01:39:18,973 INFO mapred.FileInputFormat: Total input files to process : 1
    >   2020-04-17 01:39:19,086 INFO spark.SparkContext: Starting job: count at /home/fedora/firstapp.py:5
    >   2020-04-17 01:39:19,101 INFO scheduler.DAGScheduler: Got job 0 (count at /home/fedora/firstapp.py:5) with 1 output partitions
    >   2020-04-17 01:39:19,102 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (count at /home/fedora/firstapp.py:5)
    >   2020-04-17 01:39:19,102 INFO scheduler.DAGScheduler: Parents of final stage: List()
    >   2020-04-17 01:39:19,112 INFO scheduler.DAGScheduler: Missing parents: List()
    >   2020-04-17 01:39:19,238 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /home/fedora/firstapp.py:5), which has no missing parents
    >   2020-04-17 01:39:19,290 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.2 KiB, free 366.0 MiB)
    >   2020-04-17 01:39:19,300 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 366.0 MiB)
    >   2020-04-17 01:39:19,303 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on master01:39945 (size: 4.8 KiB, free: 366.3 MiB)
    >   2020-04-17 01:39:19,303 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1206
    >   2020-04-17 01:39:19,333 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at count at /home/fedora/firstapp.py:5) (first 15 tasks are for partitions Vector(0))
    >   2020-04-17 01:39:19,344 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
    >   2020-04-17 01:39:19,404 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, master01, executor driver, partition 0, PROCESS_LOCAL, 7368 bytes)
    >   2020-04-17 01:39:19,428 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
    >   2020-04-17 01:39:19,712 INFO rdd.HadoopRDD: Input split: file:/opt/hadoop/README.txt:0+1361
    >   2020-04-17 01:39:19,976 INFO memory.MemoryStore: Block rdd_1_0 stored as values in memory (estimated size 1133.0 B, free 366.0 MiB)
    >   2020-04-17 01:39:19,976 INFO storage.BlockManagerInfo: Added rdd_1_0 in memory on master01:39945 (size: 1133.0 B, free: 366.3 MiB)
    >   2020-04-17 01:39:20,357 INFO python.PythonRunner: Times: total = 365, boot = 311, init = 54, finish = 0
    >   2020-04-17 01:39:20,386 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1592 bytes result sent to driver
    >   2020-04-17 01:39:20,396 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1008 ms on master01 (executor driver) (1/1)
    >   2020-04-17 01:39:20,398 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
    >   2020-04-17 01:39:20,402 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 58461
    >   2020-04-17 01:39:20,422 INFO scheduler.DAGScheduler: ResultStage 0 (count at /home/fedora/firstapp.py:5) finished in 1.145 s
    >   2020-04-17 01:39:20,435 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
    >   2020-04-17 01:39:20,436 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
    >   2020-04-17 01:39:20,441 INFO scheduler.DAGScheduler: Job 0 finished: count at /home/fedora/firstapp.py:5, took 1.353835 s
    >   2020-04-17 01:39:20,484 INFO spark.SparkContext: Starting job: count at /home/fedora/firstapp.py:6
    >   2020-04-17 01:39:20,485 INFO scheduler.DAGScheduler: Got job 1 (count at /home/fedora/firstapp.py:6) with 1 output partitions
    >   2020-04-17 01:39:20,485 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (count at /home/fedora/firstapp.py:6)
    >   2020-04-17 01:39:20,486 INFO scheduler.DAGScheduler: Parents of final stage: List()
    >   2020-04-17 01:39:20,489 INFO scheduler.DAGScheduler: Missing parents: List()
    >   2020-04-17 01:39:20,493 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (PythonRDD[3] at count at /home/fedora/firstapp.py:6), which has no missing parents
    >   2020-04-17 01:39:20,499 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.2 KiB, free 366.0 MiB)
    >   2020-04-17 01:39:20,502 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 366.0 MiB)
    >   2020-04-17 01:39:20,504 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on master01:39945 (size: 4.8 KiB, free: 366.3 MiB)
    >   2020-04-17 01:39:20,506 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1206
    >   2020-04-17 01:39:20,508 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[3] at count at /home/fedora/firstapp.py:6) (first 15 tasks are for partitions Vector(0))
    >   2020-04-17 01:39:20,508 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
    >   2020-04-17 01:39:20,514 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, master01, executor driver, partition 0, PROCESS_LOCAL, 7368 bytes)
    >   2020-04-17 01:39:20,514 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
    >   2020-04-17 01:39:20,520 INFO storage.BlockManager: Found block rdd_1_0 locally
    >   2020-04-17 01:39:20,568 INFO python.PythonRunner: Times: total = 42, boot = -153, init = 195, finish = 0
    >   2020-04-17 01:39:20,570 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 1592 bytes result sent to driver
    >   2020-04-17 01:39:20,576 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 63 ms on master01 (executor driver) (1/1)
    >   2020-04-17 01:39:20,576 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
    >   2020-04-17 01:39:20,578 INFO scheduler.DAGScheduler: ResultStage 1 (count at /home/fedora/firstapp.py:6) finished in 0.082 s
    >   2020-04-17 01:39:20,578 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
    >   2020-04-17 01:39:20,578 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
    >   2020-04-17 01:39:20,579 INFO scheduler.DAGScheduler: Job 1 finished: count at /home/fedora/firstapp.py:6, took 0.094801 s
    >   Lines with a: 25, lines with b: 7
    >   2020-04-17 01:39:20,620 INFO spark.SparkContext: Invoking stop() from shutdown hook
    >   2020-04-17 01:39:20,630 INFO server.AbstractConnector: Stopped Spark@6778c5f1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
    >   2020-04-17 01:39:20,633 INFO ui.SparkUI: Stopped Spark web UI at http://master01:4040
    >   2020-04-17 01:39:20,689 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
    >   2020-04-17 01:39:20,716 INFO memory.MemoryStore: MemoryStore cleared
    >   2020-04-17 01:39:20,717 INFO storage.BlockManager: BlockManager stopped
    >   2020-04-17 01:39:20,725 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
    >   2020-04-17 01:39:20,728 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
    >   2020-04-17 01:39:20,735 INFO spark.SparkContext: Successfully stopped SparkContext
    >   2020-04-17 01:39:20,735 INFO util.ShutdownHookManager: Shutdown hook called
    >   2020-04-17 01:39:20,736 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-17b811c0-b900-4dfc-8af7-0ef020f26e36
    >   2020-04-17 01:39:20,739 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-a0f4d281-99be-4b11-b0f4-e436deace326
    >   2020-04-17 01:39:20,741 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-17b811c0-b900-4dfc-8af7-0ef020f26e36/pyspark-cebf4e93-db9f-4608-8d45-ce97d35806a1


    exit



