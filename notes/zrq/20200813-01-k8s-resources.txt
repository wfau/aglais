#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#


    Running low on resources.

    Current live system (gaia-dev)

        8x Spark worker  [{small} 6 cores, 22Gi memory]

        1x Storage node  [{small} 6 cores, 22Gi memory]

        1x Spark master  [{medium} 14 cores, 45Gi memory]

        1x Zeppelin node [{medium} 14 cores, 45Gi memory]

        1x Gateway node [{tiny} 2 cores, 6Gi memory]

            84/200 cpu cores
           294/500 Gi memory

    Spark

        8x Spark worker  [{small} 6 cores, 22Gi memory]
        1x Spark master  [{medium} 14 cores, 45Gi memory]

            (8*6) + 14  = 62 cores
            (8*22) + 45 = 221 Gi memory

    Zeppelin

        1x Zeppelin node [{medium} 14 cores, 45Gi memory]
        1x Storage node  [{small} 6 cores, 22Gi memory]

            14 + 6  = 20 cores
            45 + 22 = 67 Gi memory

    If we want to allocate a similar sized system using K8s.
    The K8s cluster nodes must be large enough to accomodate the largest worker node.

    Magnum auto scale works, kind of, but is wayy too slow for an interactive responsive system.

        If we are expecting to hit the resource limits, then auto-scale works against us.
        If we allocate a small system and reply on auto-scale, we can't guarantee the system will work.
        If someone else has used all the space, auto-scale when we tyr to use it.
        If we allocate the full quota of cluster nodes at the start, we know we have enough.

    To be able to support medium sized nodes, the K8s cluster nodes need to be larger.

    Current system:

            62 + 20  = 82 cores
            221 + 67 = 288 Gi memory

    Cluster equivalent:

            Medium node = 14 cores, 45Gi memory

                82/14  = 5.8 medium nodes
                288/45 = 6.4 medium nodes

            Large node = 28 cores, 90Gi memory

                82/28  = 2.9 large nodes
                288/90 = 3.2 large nodes

    Current deploy has K8s cluster made up of 6x medium nodes.

            6*14 = 84 cores
            6*45 = 270 Gi memory

    We can use the Zeppelin interpreter properties to set the Spark node sizes.

        zeppelin.cores  10
        zeppelin.memory 40

        spark.driver.cores  10
        spark.driver.memory 20

        spark.executor.cores  4
        spark.executor.memory 8
        spark.executor.instances 8


    In theory we should have enough resources.
    In practice K8s claims insufficient cpu and memory

        0/6 nodes are available: 1 Insufficient cpu, 6 Insufficient memory.


# -----------------------------------------------------
# -----------------------------------------------------
# List the resources.
#[user@openstacker]

    cloudname=gaia-dev

    openstack \
        --os-cloud "${cloudname:?}" \
        server list

    >   +--------------------------------------+------------------+--------+--------------------------------------------+---------------+-------------------+
    >   | ID                                   | Name             | Status | Networks                                   | Image         | Flavor            |
    >   +--------------------------------------+------------------+--------+--------------------------------------------+---------------+-------------------+
    >   | 2f3a89ea-d61b-4980-967e-135e43cde77a | stv-dev-worker-8 | ACTIVE | stv-dev-network=10.0.0.5                   |               | general.v1.small  |
    >   | e2a1cb6d-50ea-4969-ab22-3b14f0a8c1d7 | stv-dev-worker-7 | ACTIVE | stv-dev-network=10.0.0.13                  |               | general.v1.small  |
    >   | cf99039a-9c37-48ae-9dfe-586bcad4c742 | stv-dev-storage  | ACTIVE | stv-dev-network=10.0.0.17                  |               | general.v1.tiny   |
    >   | e949f4ad-f388-4a2c-b93e-b29cd62015d3 | stv-dev-worker-6 | ACTIVE | stv-dev-network=10.0.0.29                  |               | general.v1.small  |
    >   | 165b4c28-9e81-4614-8f67-35bdef29bade | stv-dev-worker-2 | ACTIVE | stv-dev-network=10.0.0.4                   |               | general.v1.small  |
    >   | 254902a8-7e08-493d-9565-79c96d111316 | stv-dev-worker-4 | ACTIVE | stv-dev-network=10.0.0.33                  |               | general.v1.small  |
    >   | cb8fa8b7-a62a-4b8f-838e-f2909e06ce80 | stv-dev-worker-5 | ACTIVE | stv-dev-network=10.0.0.28                  |               | general.v1.small  |
    >   | e76726db-4129-4e17-b365-797a0e049ee2 | stv-dev-worker-3 | ACTIVE | stv-dev-network=10.0.0.6                   |               | general.v1.small  |
    >   | eaca1f6a-4fbc-44ca-9c86-1026e42dab9f | stv-dev-worker-1 | ACTIVE | stv-dev-network=10.0.0.16                  |               | general.v1.small  |
    >   | 34103538-0ef6-4454-a6e9-65c035f0805a | stv-dev-zeppelin | ACTIVE | stv-dev-network=10.0.0.27, 128.232.224.69  | Fedora-30-1.2 | general.v1.medium |
    >   | 46a5ec53-dd9b-4718-80e5-a33e571b8eac | stv-dev-master   | ACTIVE | stv-dev-network=10.0.0.14                  | Fedora-30-1.2 | general.v1.medium |
    >   | a829b57b-32ba-4708-aa65-7ad643ed5fc6 | stv-dev-gateway  | ACTIVE | stv-dev-network=10.0.0.20, 128.232.227.134 | Fedora-30-1.2 | general.v1.tiny   |
    >   +--------------------------------------+------------------+--------+--------------------------------------------+---------------+-------------------+


    openstack \
        --os-cloud "${cloudname:?}" \
        volume list

    >   +--------------------------------------+------+--------+------+-------------------------------------------+
    >   | ID                                   | Name | Status | Size | Attached to                               |
    >   +--------------------------------------+------+--------+------+-------------------------------------------+
    >   | ee5605f2-2f16-4b0b-8d61-2bf0e7f0bb6e |      | in-use |  500 | Attached to stv-dev-worker-8 on /dev/vda  |
    >   | f4319224-c790-4db6-85a5-e69b8202009a |      | in-use |  500 | Attached to stv-dev-worker-7 on /dev/vda  |
    >   | 49b92bc1-fa6a-4628-9826-2478421d221f |      | in-use | 1000 | Attached to stv-dev-storage on /dev/vda   |
    >   | 5d6d4c9b-335b-443c-a901-34be64cae447 |      | in-use |  500 | Attached to stv-dev-worker-3 on /dev/vda  |
    >   | 33bca8ab-a430-4c24-81b3-f1915bc16231 |      | in-use |  500 | Attached to stv-dev-worker-5 on /dev/vda  |
    >   | 2189a41d-b05d-4fa1-8ecd-896ccb780814 |      | in-use |  500 | Attached to stv-dev-worker-6 on /dev/vda  |
    >   | 1079eba6-2c72-4bd1-b566-b53a83c55afd |      | in-use |  500 | Attached to stv-dev-worker-4 on /dev/vda  |
    >   | 780a8382-b7f6-4765-b389-fa57541fe5f8 |      | in-use |  500 | Attached to stv-dev-worker-1 on /dev/vda  |
    >   | 0690b0bc-705b-4cf7-9ad0-65d0c47ee1ff |      | in-use |  500 | Attached to stv-dev-worker-2 on /dev/vda  |
    >   +--------------------------------------+------+--------+------+-------------------------------------------+


    openstack \
        --os-cloud "${cloudname:?}" \
        flavor list

    >   +--------------------------------------+-------------------+--------+------+-----------+-------+-----------+
    >   | ID                                   | Name              |    RAM | Disk | Ephemeral | VCPUs | Is Public |
    >   +--------------------------------------+-------------------+--------+------+-----------+-------+-----------+
    >   | 20061eba-9e88-494c-95a3-41ed77721244 | general.v1.small  |  22528 |   20 |         0 |     6 | True      |
    >   | 406a17e0-afd0-47d3-a6ad-8b19198bdd97 | general.v1.tiny   |   6144 |   12 |         0 |     2 | True      |
    >   | 8a821ef8-20b8-4bbb-990b-91198745e7a7 | general.v1.xlarge | 184320 |   20 |       340 |    28 | True      |
    >   | 996c1c8c-c934-411c-9631-b74eb2829631 | general.v1.medium |  46080 |   20 |        60 |    14 | True      |
    >   | c4c07f5a-260a-4f22-9530-a09a19aa490a | general.v1.large  |  92160 |   20 |       160 |    28 | True      |
    >   +--------------------------------------+-------------------+--------+------+-----------+-------+-----------+



