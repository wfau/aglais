#7
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    # Spent too long trying to get the K8s Manila driver to work.
    # The in-tree K8s Manila driver has been replaced by the Manila CSI plugin.
    # All of the documentation, source code and issues point to the CSI plugin.
    # Worth jumping ahead and trying out the CSI plugin.

    Start again ...

    Cluster delete
        20200827-01-cluster-delete.txt
        Explicitly delete the CephFS router
            Delete the route
            Delete the port
            Delete the router

    Magnum cluster
        20200828-01-magnum-cluster.txt
        Using Openstack directly, not Terraform.

    Nginx-controller
        20200807-06-nginx-ingress.txt
            Helm based deploy.

    Dashboard
        20200807-07-dashboard.txt
            Helm based deploy, followed by kubectl additions.

    CephFS-router
        20200820-05-cephfs-router.txt
            Openstack calls to get configuration.
            Terraform deploy.



# -----------------------------------------------------
# Checkout a copy of the Cloud Provider OpenStack repo.
#[user@desktop]

    source "${HOME}/aglais.env"

    pushd "${AGLAIS_HOME}/external"

        mkdir Kubernetes
        pushd Kubernetes

            git clone 'https://github.com/kubernetes/cloud-provider-openstack.git'

        popd
    popd


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --hostname kubernator \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --env "clustername=${CLUSTER_NAME:?}" \
        --volume "${HOME}/clouds.yaml:/etc/openstack/clouds.yaml:z" \
        --volume "${AGLAIS_CODE}/experiments/zrq/kubernetes:/kubernetes:z" \
        --volume "${AGLAIS_HOME}/external/Kubernetes/cloud-provider-openstack:/cloud-provider-openstack:z" \
        atolmis/openstack-client \
        bash

# -----------------------------------------------------
# Get the connection details for our cluster.
#[user@kubernator]

    mkdir -p "${HOME}/.kube"
    openstack \
        --os-cloud "${cloudname:?}" \
        coe cluster config \
            "${clustername:?}" \
                --force \
                --dir "${HOME}/.kube"


    kubectl \
        cluster-info

    >   Kubernetes master is running at https://....
    >   Heapster is running at https://....
    >   CoreDNS is running at https://....


# -----------------------------------------------------
# Install config editing tools.
#[user@kubernator]

    # TODO - add this to the kubernator image
    mkdir "${HOME:?}/bin"
    wget -O "${HOME:?}/bin/yq" https://github.com/mikefarah/yq/releases/download/3.3.2/yq_linux_amd64
    chmod a+x "${HOME:?}/bin/yq"


# -----------------------------------------------------
# Install the CSI plugin using Helm.
# https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-manila-csi-plugin.md#kubernetes-115
#[user@kubernator]


    cat > "/tmp/csi-manila-cephfs-helm-values.yaml" << EOF
# Enabled Manila share protocols
shareProtocols:
   - protocolSelector: CEPHFS
     fwdNodePluginEndpoint:
       dir: /var/lib/kubelet/plugins/cephfs.csi.ceph.com
       sockFile: csi.sock

nameOverride: csi-manila-cephfs
EOF


    helm install \
        csi-manila-cephfs \
        /cloud-provider-openstack/charts/manila-csi-plugin \
        --values "/tmp/csi-manila-cephfs-helm-values.yaml"

    >   NAME: csi-manila-cephfs
    >   LAST DEPLOYED: Mon Aug 31 03:44:43 2020
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 1
    >   TEST SUITE: None


# -----------------------------------------------------
# Check all the parts are in place.
# https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-manila-csi-plugin.md#verifying-the-deployment
#[user@kubernator]

    kubectl get all

    >   NAME                                                           READY   STATUS    RESTARTS   AGE
    >   pod/csi-manila-cephfs-controllerplugin-0                       3/3     Running   0          28s
    >   pod/csi-manila-cephfs-nodeplugin-bs7zt                         2/2     Running   0          28s
    >   pod/csi-manila-cephfs-nodeplugin-rjr78                         2/2     Running   0          28s
    >   pod/csi-manila-cephfs-nodeplugin-wfpkx                         2/2     Running   0          28s
    >   pod/csi-manila-cephfs-nodeplugin-zkzwg                         2/2     Running   0          28s
    >   ....
    >
    >   NAME                                                          TYPE           CLUSTER-IP       EXTERNAL-IP       PORT(S)                      AGE
    >   service/csi-manila-cephfs-controllerplugin                    ClusterIP      10.254.110.103   <none>            12345/TCP                    28s
    >   ....
    >
    >   NAME                                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
    >   daemonset.apps/csi-manila-cephfs-nodeplugin   4         4         4       4            4           <none>          28s
    >   ....
    >
    >   NAME                                                  READY   AGE
    >   statefulset.apps/csi-manila-cephfs-controllerplugin   1/1     28s
    >   ....


# -----------------------------------------------------
# Create our secrets.
#[user@kubernator]

    authurl=$(
        yq r /etc/openstack/clouds.yaml \
            'clouds.gaia-prod.auth.auth_url'
        )

    region=$(
        yq r /etc/openstack/clouds.yaml \
            'clouds.gaia-prod.region_name'
        )

    credid=$(
        yq r /etc/openstack/clouds.yaml \
            'clouds.gaia-prod.auth.application_credential_id'
        )

    credsecret=$(
        yq r /etc/openstack/clouds.yaml \
            'clouds.gaia-prod.auth.application_credential_id'
        )


    cat > "/tmp/csi-manila-cephfs-secrets.yaml" << EOF
apiVersion: v1
kind: Secret
metadata:
  name: csi-manila-cephfs-secrets
  namespace: default
stringData:
  # Mandatory
  os-authURL: "${authurl:?}"
  os-region: "${region:?}"

  # Authentication using user credentials
  os-applicationCredentialID: "${credid:?}"
  os-applicationCredentialSecret: "${credsecret:?}"

EOF

    kubectl apply \
        --filename "/tmp/csi-manila-cephfs-secrets.yaml"

    >   secret/csi-manila-cephfs-secrets created


    kubectl describe \
        secret \
            csi-manila-cephfs-secrets

    >   Name:         csi-manila-cephfs-secrets
    >   Namespace:    default
    >   Labels:       <none>
    >   Annotations:
    >   Type:         Opaque
    >
    >   Data
    >   ====
    >   os-applicationCredentialSecret:  32 bytes
    >   os-authURL:                      47 bytes
    >   os-region:                       9 bytes
    >   os-applicationCredentialID:      32 bytes


# -----------------------------------------------------
# Set the Manila API version.
# https://stackoverflow.com/a/58806536
#[user@kubernator]

    export OS_SHARE_API_VERSION=2.51

# -----------------------------------------------------
# List the available shares.
#[user@kubernator]

    openstack \
        --os-cloud "${cloudname:?}" \
            share list

    >   +--------------------------------------+------------------------------------------+------+-------------+-----------+-----------+------------------+------+-------------------+
    >   | ID                                   | Name                                     | Size | Share Proto | Status    | Is Public | Share Type Name  | Host | Availability Zone |
    >   +--------------------------------------+------------------------------------------+------+-------------+-----------+-----------+------------------+------+-------------------+
    >   | ad1d9ca2-5b1c-4064-8c74-695286de6098 | gaia-dr2-share                           | 4399 | CEPHFS      | available | True      | cephfsnativetype |      | nova              |
    >   | a32ea61f-a922-4b9d-959b-a9d2d2e57c4b | magruela-share                           |    5 | CEPHFS      | available | True      | cephfsnativetype |      | nova              |
    >   | d71dae9a-b508-4a54-a5e9-7a91e8548b1e | pvc-41814ac5-00cc-4ba3-ae8b-6eddad01ef0c |    1 | CEPHFS      | available | False     | cephfsnativetype |      | nova              |
    >   +--------------------------------------+------------------------------------------+------+-------------+-----------+-----------+------------------+------+-------------------+


# -----------------------------------------------------
# List our current share access rules.
#[user@kubernator]

    shareid=a32ea61f-a922-4b9d-959b-a9d2d2e57c4b

    openstack \
        --os-cloud "${cloudname:?}" \
            share access list \
                "${shareid:?}"

    >   +--------------------------------------+-------------+----------------+--------------+--------+----------------+----------------------------+----------------------------+
    >   | id                                   | access_type | access_to      | access_level | state  | access_key     | created_at                 | updated_at                 |
    >   +--------------------------------------+-------------+----------------+--------------+--------+----------------+----------------------------+----------------------------+
    >   | f658ea33-daef-46da-8fff-82b23ced65de | cephx       | magruela-share | rw           | active | AQCE........== | 2020-08-31T04:24:36.000000 | 2020-08-31T04:24:36.000000 |
    >   +--------------------------------------+-------------+----------------+--------------+--------+----------------+----------------------------+----------------------------+

    shareaccessid=f658ea33-daef-46da-8fff-82b23ced65de


# -----------------------------------------------------
# Create our persistent volume.
#[user@kubernator]

    cat > "/tmp/magruela-volume.yaml" << EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: magruela-volume
  labels:
    name: magruela-volume
spec:
  accessModes:
  - ReadWriteMany
  capacity:
    storage: 4Gi
  csi:
    driver: cephfs.manila.csi.openstack.org
    volumeHandle: magruela-volume-handle
    nodeStageSecretRef:
      name: csi-manila-cephfs-secrets
      namespace: default
    nodePublishSecretRef:
      name: csi-manila-cephfs-secrets
      namespace: default
    volumeAttributes:
      shareID: ${shareid:?}
      shareAccessID: ${shareaccessid:?}
EOF

    kubectl apply \
        --filename "/tmp/magruela-volume.yaml"

    >   persistentvolume/magruela-volume created


    kubectl describe \
        persistentvolume \
            magruela-volume

    >   Name:            magruela-volume
    >   Labels:          name=magruela-volume
    >   Annotations:     kubectl.kubernetes.io/last-applied-configuration:
    >                      {"apiVersion":"v1","kind":"PersistentVolume","metadata":{"annotations":{},"labels":{"name":"magruela-volume"},"name":"magruela-volume"},"s...
    >   Finalizers:      [kubernetes.io/pv-protection]
    >   StorageClass:
    >   Status:          Available
    >   Claim:
    >   Reclaim Policy:  Retain
    >   Access Modes:    RWX
    >   VolumeMode:      Filesystem
    >   Capacity:        4Gi
    >   Node Affinity:   <none>
    >   Message:
    >   Source:
    >       Type:              CSI (a Container Storage Interface (CSI) volume source)
    >       Driver:            cephfs.manila.csi.openstack.org
    >       VolumeHandle:      magruela-volume-handle
    >       ReadOnly:          false
    >       VolumeAttributes:      shareAccessID=f658ea33-daef-46da-8fff-82b23ced65de
    >                              shareID=a32ea61f-a922-4b9d-959b-a9d2d2e57c4b
    >   Events:                <none>


# -----------------------------------------------------
# Create our persistent volume claim.
#[user@kubernator]

    cat > "/tmp/magruela-claim.yaml" << EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: magruela-claim
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 3Gi
  selector:
    matchExpressions:
    - key: name
      operator: In
      values: ["magruela-volume"]
EOF

    kubectl apply \
        --filename "/tmp/magruela-claim.yaml"

    >   persistentvolumeclaim/magruela-claim created


    kubectl describe \
        persistentvolumeclaim \
            magruela-claim

    >   Name:          magruela-claim
    >   Namespace:     default
    >   StorageClass:
    >   Status:        Bound
    >   Volume:        magruela-volume
    >   Labels:        <none>
    >   Annotations:   kubectl.kubernetes.io/last-applied-configuration:
    >                    {"apiVersion":"v1","kind":"PersistentVolumeClaim","metadata":{"annotations":{},"name":"magruela-claim","namespace":"default"},"spec":{"acc...
    >                  pv.kubernetes.io/bind-completed: yes
    >                  pv.kubernetes.io/bound-by-controller: yes
    >   Finalizers:    [kubernetes.io/pvc-protection]
    >   Capacity:      4Gi
    >   Access Modes:  RWX
    >   VolumeMode:    Filesystem
    >   Mounted By:    <none>
    >   Events:        <none>


# -----------------------------------------------------
# Create a Pod that mounts the volume ....
#[user@kubernator]

    cat > /tmp/magruela-pod.yaml << EOF
kind: Pod
apiVersion: v1
metadata:
  name: magruela-pod
  namespace: default
spec:
  volumes:
    - name: magruela-data
      persistentVolumeClaim:
        claimName: magruela-claim
    - name: local-data
      emptyDir: {}
  containers:
    - name: magruela-container
      image: 'fedora:latest'
      volumeMounts:
        - name: magruela-data
          mountPath: /magruela-data
        - name: local-data
          mountPath: /local-data
      command: ["/bin/sh"]
      args:
        - "-c"
        - >-
          while true; do
          date >> /local-data/date-log.txt;
          sleep 1;
          done
EOF

    kubectl \
        apply \
            --filename /tmp/magruela-pod.yaml

    >   pod/magruela-pod created


    kubectl \
        describe pod \
            magruela-pod


    >   ....
    >   ....
    >   Events:
    >     Type     Reason       Age               From                                            Message
    >     ----     ------       ----              ----                                            -------
    >     Normal   Scheduled    <unknown>         default-scheduler                               Successfully assigned default/magruela-pod to tiberius-20200831-cnt5rv7mw7ov-node-1
    >     Warning  FailedMount  7s (x7 over 38s)  kubelet, tiberius-20200831-cnt5rv7mw7ov-node-1  MountVolume.MountDevice failed for volume "magruela-volume" : kubernetes.io/csi: ....

    >   MountVolume.MountDevice failed for volume "magruela-volume" :
    >       kubernetes.io/csi: attacher.MountDevice failed to create newCsiDriverClient:
    >           driver name cephfs.manila.csi.openstack.org not found in the list of registered CSI drivers

    #
    # Nothing is easy on this project.
    #


    kubectl get CSIDriver

    >   NAME                              CREATED AT
    >   cephfs.manila.csi.openstack.org   2020-08-31T03:44:44Z


    kubectl describe \
        CSIDriver \
            'cephfs.manila.csi.openstack.org'

    >   Name:         cephfs.manila.csi.openstack.org
    >   Namespace:
    >   Labels:       app.kubernetes.io/managed-by=Helm
    >   Annotations:  meta.helm.sh/release-name: csi-manila-cephfs
    >                 meta.helm.sh/release-namespace: default
    >   API Version:  storage.k8s.io/v1beta1
    >   Kind:         CSIDriver
    >   Metadata:
    >     Creation Timestamp:  2020-08-31T03:44:44Z
    >     Resource Version:    16054
    >     Self Link:           /apis/storage.k8s.io/v1beta1/csidrivers/cephfs.manila.csi.openstack.org
    >     UID:                 d9900a10-dab7-40b3-a36c-2a778d7a0240
    >   Spec:
    >     Attach Required:    false
    >     Pod Info On Mount:  false
    >     Volume Lifecycle Modes:
    >       Persistent
    >   Events:  <none>


    #
    # The Manila CSI driver documentation talks about being a proxy driver,
    # and passing filesystem (CephFS or NFS) operations down to a lower
    # level driver.
    #
    # It kind of implies we need to install the CephFS driver ?
    # Or should everything we need be installed by our Helm install ?
    #
    # The error message says 'cephfs.manila.csi.openstack.org' is missing,
    # not 'ceph-csi-cephfs'.
    #
    # There was a name length error whenm I installed, and I had
    # to uninstall and install a second time.
    # Is this a broken deployment ?
    #

    #
    # Nuke from orbit and create from start ?
    # Tried that, didn't work :-(
    #


# -----------------------------------------------------
# The only thing I changed from the xample was the 'volume handle',
# changed from 'magruela-volume' to 'magruela-volume-handle'.
# Try changing it ..
#[user@kubernator]

    vi /tmp/magruela-volume.yaml

          ....
          csi:
            driver: cephfs.manila.csi.openstack.org
    -       volumeHandle: magruela-volume-handle
    +       volumeHandle: magruela-volume-handle
          ....



    kubectl apply \
        --filename "/tmp/magruela-volume.yaml"

    >   The PersistentVolume "magruela-volume" is invalid: spec.persistentvolumesource: Forbidden: is immutable after creation

# -----------------------------------------------------
# Try delete, edit and re-apply the templates.
#[user@kubernator]

    kubectl delete \
        pod \
            magruela-pod

    kubectl delete \
        persistentvolumeclaim \
            magruela-claim

    kubectl delete \
        persistentvolume \
            magruela-volume


    kubectl apply \
        --filename "/tmp/magruela-volume.yaml"

    kubectl apply \
        --filename "/tmp/magruela-claim.yaml"

    kubectl apply \
        --filename "/tmp/magruela-pod.yaml"

    kubectl \
        describe pod \
            magruela-pod

    >   ....
    >   ....
    >   Events:
    >     Type     Reason       Age               From                                            Message
    >     ----     ------       ----              ----                                            -------
    >     Normal   Scheduled    <unknown>         default-scheduler                               Successfully assigned default/magruela-pod to tiberius-20200831-wq5fv3lbgfco-node-0
    >     Warning  FailedMount  4s (x6 over 20s)  kubelet, tiberius-20200831-wq5fv3lbgfco-node-0  MountVolume.MountDevice failed for volume "magruela-volume" : kubernetes.io/csi: attacher.MountDevice failed to create newCsiDriverClient: driver name cephfs.manila.csi.openstack.org not found in the list of registered CSI drivers


# -----------------------------------------------------


    >   MountVolume.MountDevice failed for volume "magruela-volume" :
    >       kubernetes.io/csi: attacher.MountDevice failed to create newCsiDriverClient:
    >           driver name cephfs.manila.csi.openstack.org not found in the list of registered CSI drivers

    Implies we need to register a CSI driver.

    Browsing the K8s dashboard for clues ...
    Looking at the event messages for csi-manila-cephfs-nodeplugin:

    >   Successfully assigned default/csi-manila-cephfs-nodeplugin-vk9pv to tiberius-20200831-wq5fv3lbgfco-node-3
    >   default-scheduler
    >
    >   Pulling image "quay.io/k8scsi/csi-node-driver-registrar:v1.1.0"
    >   kubelet tiberius-20200831-wq5fv3lbgfco-node-3
    >   spec.containers{cephfs-registrar}
    >
    >   Successfully pulled image "quay.io/k8scsi/csi-node-driver-registrar:v1.1.0"
    >   kubelet tiberius-20200831-wq5fv3lbgfco-node-3
    >   spec.containers{cephfs-registrar}
    >
    >   Created container cephfs-registrar
    >   kubelet tiberius-20200831-wq5fv3lbgfco-node-3
    >   spec.containers{cephfs-registrar}
    >
    >   Started container cephfs-registrar
    >   kubelet tiberius-20200831-wq5fv3lbgfco-node-3
    >   spec.containers{cephfs-registrar}
    >
    >   Pulling image "k8scloudprovider/manila-csi-plugin:latest"
    >   kubelet tiberius-20200831-wq5fv3lbgfco-node-3
    >   spec.containers{cephfs-nodeplugin}
    >
    >   Successfully pulled image "k8scloudprovider/manila-csi-plugin:latest"
    >   kubelet tiberius-20200831-wq5fv3lbgfco-node-3
    >   spec.containers{cephfs-nodeplugin}
    >
    >   Created container cephfs-nodeplugin
    >   kubelet tiberius-20200831-wq5fv3lbgfco-node-3
    >   spec.containers{cephfs-nodeplugin}
    >
    >   Started container cephfs-nodeplugin
    >   kubelet tiberius-20200831-wq5fv3lbgfco-node-3
    >   spec.containers{cephfs-nodeplugin}

    That looks like it loaded its own registrar side car,
    so registration is probably done.

    On the othet hand, chack the logs for one of the csi-manila-cephfs-nodeplugin instances.
    (there is one instance on each node)

    kubectl logs \
        csi-manila-cephfs-nodeplugin-4tzpr \
        --container cephfs-nodeplugin

    >   I0831 13:59:24.372814       1 driver.go:124] Driver: cephfs.manila.csi.openstack.org
    >   I0831 13:59:24.372856       1 driver.go:125] Driver version: 0.9.0@latest
    >   I0831 13:59:24.372860       1 driver.go:126] CSI spec version: 1.2.0
    >   I0831 13:59:24.372863       1 driver.go:129] Operating on CEPHFS shares
    >   I0831 13:59:24.372869       1 driver.go:134] Topology awareness disabled
    >   I0831 13:59:24.372873       1 driver.go:197] Enabling controller service capability: CREATE_DELETE_VOLUME
    >   I0831 13:59:24.372878       1 driver.go:197] Enabling controller service capability: CREATE_DELETE_SNAPSHOT
    >   I0831 13:59:24.372884       1 driver.go:216] Enabling volume access mode: MULTI_NODE_MULTI_WRITER
    >   I0831 13:59:24.372898       1 driver.go:216] Enabling volume access mode: MULTI_NODE_SINGLE_WRITER
    >   I0831 13:59:24.372901       1 driver.go:216] Enabling volume access mode: MULTI_NODE_READER_ONLY
    >   I0831 13:59:24.372904       1 driver.go:216] Enabling volume access mode: SINGLE_NODE_WRITER
    >   I0831 13:59:24.372906       1 driver.go:216] Enabling volume access mode: SINGLE_NODE_READER_ONLY
    >   W0831 13:59:25.373318       1 builder.go:82] still connecting to unix:///var/lib/kubelet/plugins/cephfs.csi.ceph.com/csi.sock
    >   W0831 13:59:26.373231       1 builder.go:82] still connecting to unix:///var/lib/kubelet/plugins/cephfs.csi.ceph.com/csi.sock
    >   W0831 13:59:27.373290       1 builder.go:82] still connecting to unix:///var/lib/kubelet/plugins/cephfs.csi.ceph.com/csi.sock
    >   W0831 13:59:28.373241       1 builder.go:82] still connecting to unix:///var/lib/kubelet/plugins/cephfs.csi.ceph.com/csi.sock
    >   ....
    >   ....
    >   W0831 14:51:22.373019       1 builder.go:82] still connecting to unix:///var/lib/kubelet/plugins/cephfs.csi.ceph.com/csi.sock
    >   W0831 14:51:23.373201       1 builder.go:82] still connecting to unix:///var/lib/kubelet/plugins/cephfs.csi.ceph.com/csi.sock
    >   W0831 14:51:24.373168       1 builder.go:82] still connecting to unix:///var/lib/kubelet/plugins/cephfs.csi.ceph.com/csi.sock
    >   W0831 14:51:25.373230       1 builder.go:82] still connecting to unix:///var/lib/kubelet/plugins/cephfs.csi.ceph.com/csi.sock

    #
    # Which suggests we do need to install the CephFS CSI driver.
    #


    Ceph CSI plugins
    https://github.com/ceph/ceph-csi

        Ceph CSI plugins implement an interface between CSI enabled Container Orchestrator (CO) and Ceph cluster.
        It allows dynamically provisioning Ceph volumes and attaching them to workloads.

    Helm charts
    https://github.com/ceph/ceph-csi/blob/master/charts/ceph-csi-cephfs/README.md#ceph-csi-cephfs

# -----------------------------------------------------
# Install the chart repo.
#[user@kubernator]

    helm repo add \
        ceph-csi \
            'https://ceph.github.io/csi-charts'

    >   "ceph-csi" has been added to your repositories


    kubectl create \
        namespace \
            ceph-csi-cephfs

    >   namespace/ceph-csi-cephfs created


    helm install \
        --namespace "ceph-csi-cephfs" \
        "ceph-csi-cephfs" \
            ceph-csi/ceph-csi-cephfs

    >   NAME: ceph-csi-cephfs
    >   LAST DEPLOYED: Mon Aug 31 14:57:41 2020
    >   NAMESPACE: ceph-csi-cephfs
    >   STATUS: deployed
    >   REVISION: 1
    >   TEST SUITE: None
    >   NOTES:
    >   Examples on how to configure a storage class and start using the driver are here:
    >   https://github.com/ceph/ceph-csi/tree/v3.1.0/examples/cephfs


# -----------------------------------------------------
# Follow the logs on our cephfs-nodeplugin instance.
#[user@kubernator]

    kubectl logs \
        csi-manila-cephfs-nodeplugin-4tzpr \
        --container cephfs-nodeplugin

    >   ....
    >   ....
    >   W0831 14:58:10.373183       1 builder.go:82] still connecting to unix:///var/lib/kubelet/plugins/cephfs.csi.ceph.com/csi.sock
    >   W0831 14:58:11.373224       1 builder.go:82] still connecting to unix:///var/lib/kubelet/plugins/cephfs.csi.ceph.com/csi.sock
    >   W0831 14:58:12.373034       1 builder.go:82] still connecting to unix:///var/lib/kubelet/plugins/cephfs.csi.ceph.com/csi.sock
    >   W0831 14:58:13.373054       1 builder.go:82] still connecting to unix:///var/lib/kubelet/plugins/cephfs.csi.ceph.com/csi.sock
    >   I0831 14:58:13.513978       1 connection.go:261] Probing CSI driver for readiness
    >   I0831 14:58:13.514062       1 builder.go:39] [ID:1] FWD GRPC call: /csi.v1.Identity/Probe
    >   I0831 14:58:13.514090       1 builder.go:40] [ID:1] FWD GRPC request: {}
    >   I0831 14:58:13.518666       1 builder.go:46] [ID:1] FWD GRPC response: {}
    >   I0831 14:58:13.519317       1 builder.go:39] [ID:2] FWD GRPC call: /csi.v1.Identity/GetPluginInfo
    >   I0831 14:58:13.519335       1 builder.go:40] [ID:2] FWD GRPC request: {}
    >   I0831 14:58:13.522387       1 builder.go:46] [ID:2] FWD GRPC response: {"name":"cephfs.csi.ceph.com","vendor_version":"v3.1.0"}
    >   I0831 14:58:13.522870       1 driver.go:262] proxying CSI driver cephfs.csi.ceph.com version v3.1.0
    >   I0831 14:58:13.522908       1 builder.go:39] [ID:3] FWD GRPC call: /csi.v1.Node/NodeGetCapabilities
    >   I0831 14:58:13.522914       1 builder.go:40] [ID:3] FWD GRPC request: {}
    >   I0831 14:58:13.525301       1 builder.go:46] [ID:3] FWD GRPC response: {"capabilities":[{"Type":{"Rpc":{"type":1}}},{"Type":{"Rpc":{"type":2}}}]}
    >   I0831 14:58:13.526754       1 driver.go:227] Enabling node service capability: STAGE_UNSTAGE_VOLUME
    >   I0831 14:58:13.526782       1 driver.go:227] Enabling node service capability: GET_VOLUME_STATS
    >   I0831 14:58:13.527243       1 driver.go:326] listening for connections on &net.UnixAddr{Name:"/var/lib/kubelet/plugins/cephfs.manila.csi.openstack.org/csi.sock", Net:"unix"}
    >   I0831 14:58:13.920318       1 driver.go:309] [ID:1] GRPC call: /csi.v1.Identity/GetPluginInfo
    >   I0831 14:58:13.920343       1 driver.go:310] [ID:1] GRPC request: {}
    >   I0831 14:58:13.922426       1 driver.go:315] [ID:1] GRPC response: {"name":"cephfs.manila.csi.openstack.org","vendor_version":"0.9.0@latest"}
    >   I0831 14:58:14.898513       1 driver.go:309] [ID:2] GRPC call: /csi.v1.Node/NodeGetInfo
    >   I0831 14:58:14.898537       1 driver.go:310] [ID:2] GRPC request: {}
    >   I0831 14:58:14.898984       1 driver.go:315] [ID:2] GRPC response: {"node_id":"tiberius-20200831-wq5fv3lbgfco-node-1"}


# -----------------------------------------------------
# Check the status on our test Pod.
#[user@kubernator]

    kubectl \
        describe pod \
            magruela-pod

    >   ....
    >   ....
    >   Events:
    >     Type     Reason       Age                  From                                            Message
    >     ----     ------       ----                 ----                                            -------
    >     Normal   Scheduled    <unknown>            default-scheduler                               Successfully assigned default/magruela-pod to tiberius-20200831-wq5fv3lbgfco-node-0
    >     Warning  FailedMount  44m (x2 over 46m)    kubelet, tiberius-20200831-wq5fv3lbgfco-node-0  Unable to attach or mount volumes: unmounted volumes=[magruela-data], unattached volumes=[default-token-z2wxb magruela-data local-data]: timed out waiting for the condition
    >     Warning  FailedMount  20m (x23 over 50m)   kubelet, tiberius-20200831-wq5fv3lbgfco-node-0  MountVolume.MountDevice failed for volume "magruela-volume" : kubernetes.io/csi: attacher.MountDevice failed to create newCsiDriverClient: driver name cephfs.manila.csi.openstack.org not found in the list of registered CSI drivers
    >     Warning  FailedMount  10m (x10 over 48m)   kubelet, tiberius-20200831-wq5fv3lbgfco-node-0  Unable to attach or mount volumes: unmounted volumes=[magruela-data], unattached volumes=[magruela-data local-data default-token-z2wxb]: timed out waiting for the condition
    >     Warning  FailedMount  5m49s (x6 over 33m)  kubelet, tiberius-20200831-wq5fv3lbgfco-node-0  Unable to attach or mount volumes: unmounted volumes=[magruela-data], unattached volumes=[local-data default-token-z2wxb magruela-data]: timed out waiting for the condition

    #
    # That could just mean they gave up before we fixed the CSI plugin.
    #

# -----------------------------------------------------
# Delete and re-create the Pod.
#[user@kubernator]

    kubectl \
        delete pod \
            magruela-pod

    >   pod "magruela-pod" deleted


    kubectl apply \
        --filename "/tmp/magruela-pod.yaml"

    >   pod/magruela-pod created


    kubectl \
        describe pod \
            magruela-pod

    >   ....
    >   ....
    >   Events:
    >     Type     Reason       Age              From                                            Message
    >     ----     ------       ----             ----                                            -------
    >     Normal   Scheduled    <unknown>        default-scheduler                               Successfully assigned default/magruela-pod to tiberius-20200831-wq5fv3lbgfco-node-2
    >     Warning  FailedMount  1s (x5 over 9s)  kubelet, tiberius-20200831-wq5fv3lbgfco-node-2  MountVolume.MountDevice failed for volume "magruela-volume" : rpc error: code = InvalidArgument desc = invalid OpenStack secrets: parameter 'os-authURL' requires exactly one of [os-password os-trustID] parameters

    #
    # Yay - progress.
    #

    So this message :

        "driver name 'cephfs.manila.csi.openstack.org' not found in the list of registered CSI drivers

    Actually means :

        "driver 'cephfs.manila.csi.openstack.org' can't find driver 'cephfs.csi.ceph.com'"

    aaauuuugh!

    #
    # Back to where we are.
    # Not ther yet, it found the driver, tried the mount but failed at the authentication step.
    #

    >   MountVolume.MountDevice failed for volume "magruela-volume" :
    >       rpc error:
    >           code = InvalidArgument
    >           desc = invalid OpenStack secrets:
    >               parameter 'os-authURL' requires exactly one of [os-password os-trustID] parameters

    #
    # That suggests the plugin doesn't support applicationCredentialID and applicationCredentialSecret ?
    # Which would be a bad thing.
    #

    #
    # Documentation suggests it should:#
    # https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-manila-csi-plugin.md#secrets-authentication

        Mandatory secrets for application credential authentication:

            os-applicationCredentialID or os-applicationCredentialName
            os-applicationCredentialSecret

# -----------------------------------------------------
# Check the secrets.
#[user@kubernator]

    kubectl \
        get persistentvolume \
            --output json \
                magruela-volume


    >   {
    >       "apiVersion": "v1",
    >       "kind": "PersistentVolume",
    >       "metadata": {
    >           "annotations": {
    >               "kubectl.kubernetes.io/last-applied-configuration": "{\"apiVersion\":\"v1\",\"kind\":\"PersistentVolume\",\"metadata\":{\"annotations\":{},\"labels\":{\"name\":\"magruela-volume\"},\"name\":\"magruela-volume\"},\"spec\":{\"accessModes\":[\"ReadWriteMany\"],\"capacity\":{\"storage\":\"4Gi\"},\"csi\":{\"driver\":\"cephfs.manila.csi.openstack.org\",\"nodePublishSecretRef\":{\"name\":\"csi-manila-cephfs-secrets\",\"namespace\":\"default\"},\"nodeStageSecretRef\":{\"name\":\"csi-manila-cephfs-secrets\",\"namespace\":\"default\"},\"volumeAttributes\":{\"shareAccessID\":\"f658ea33-daef-46da-8fff-82b23ced65de\",\"shareID\":\"a32ea61f-a922-4b9d-959b-a9d2d2e57c4b\"},\"volumeHandle\":\"magruela-volume\"}}}\n",
    >               "pv.kubernetes.io/bound-by-controller": "yes"
    >           },
    >           "creationTimestamp": "2020-08-31T14:09:56Z",
    >           "finalizers": [
    >               "kubernetes.io/pv-protection"
    >           ],
    >           "labels": {
    >               "name": "magruela-volume"
    >           },
    >           "name": "magruela-volume",
    >           "resourceVersion": "9992",
    >           "selfLink": "/api/v1/persistentvolumes/magruela-volume",
    >           "uid": "9a0b9e54-e9c3-4bbd-b718-86d2264ba550"
    >       },
    >       "spec": {
    >           "accessModes": [
    >               "ReadWriteMany"
    >           ],
    >           "capacity": {
    >               "storage": "4Gi"
    >           },
    >           "claimRef": {
    >               "apiVersion": "v1",
    >               "kind": "PersistentVolumeClaim",
    >               "name": "magruela-claim",
    >               "namespace": "default",
    >               "resourceVersion": "9989",
    >               "uid": "d118dfb3-1d3d-4b2f-b071-a91cb47839f9"
    >           },
    >           "csi": {
    >               "driver": "cephfs.manila.csi.openstack.org",
    >               "nodePublishSecretRef": {
    >                   "name": "csi-manila-cephfs-secrets",
    >                   "namespace": "default"
    >               },
    >               "nodeStageSecretRef": {
    >                   "name": "csi-manila-cephfs-secrets",
    >                   "namespace": "default"
    >               },
    >               "volumeAttributes": {
    >                   "shareAccessID": "f658ea33-daef-46da-8fff-82b23ced65de",
    >                   "shareID": "a32ea61f-a922-4b9d-959b-a9d2d2e57c4b"
    >               },
    >               "volumeHandle": "magruela-volume"
    >           },
    >           "persistentVolumeReclaimPolicy": "Retain",
    >           "volumeMode": "Filesystem"
    >       },
    >       "status": {
    >           "phase": "Bound"
    >       }
    >   }


    kubectl \
        get persistentvolume \
            --output json \
                magruela-volume \
    | jq '.spec.csi'

    >   {
    >     "driver": "cephfs.manila.csi.openstack.org",
    >     "nodePublishSecretRef": {
    >       "name": "csi-manila-cephfs-secrets",
    >       "namespace": "default"
    >     },
    >     "nodeStageSecretRef": {
    >       "name": "csi-manila-cephfs-secrets",
    >       "namespace": "default"
    >     },
    >     "volumeAttributes": {
    >       "shareAccessID": "f658ea33-daef-46da-8fff-82b23ced65de",
    >       "shareID": "a32ea61f-a922-4b9d-959b-a9d2d2e57c4b"
    >     },
    >     "volumeHandle": "magruela-volume"
    >   }


    kubectl \
        get secret \
            --output json \
                csi-manila-cephfs-secrets

    >   {
    >       "apiVersion": "v1",
    >       "data": {
    >           "os-applicationCredentialID": "NDFkZjEyNDJkODFhNGJkOTk3MGEyOTFjY2U4NGIwZGE=",
    >           "os-applicationCredentialSecret": "NDFkZjEyNDJkODFhNGJkOTk3MGEyOTFjY2U4NGIwZGE=",
    >           "os-authURL": "aHR0cHM6Ly9jdW11bHVzLm9wZW5zdGFjay5ocGMuY2FtLmFjLnVrOjUwMDAvdjM=",
    >           "os-region": "UmVnaW9uT25l"
    >       },
    >       "kind": "Secret",
    >       "metadata": {
    >           "annotations": {
    >               "kubectl.kubernetes.io/last-applied-configuration": "{\"apiVersion\":\"v1\",\"kind\":\"Secret\",\"metadata\":{\"annotations\":{},\"name\":\"csi-manila-cephfs-secrets\",\"namespace\":\"default\"},\"stringData\":{\"os-applicationCredentialID\":\"41df1242d81a4bd9970a291cce84b0da\",\"os-applicationCredentialSecret\":\"41df1242d81a4bd9970a291cce84b0da\",\"os-authURL\":\"https://cumulus.openstack.hpc.cam.ac.uk:5000/v3\",\"os-region\":\"RegionOne\"}}\n"
    >           },
    >           "creationTimestamp": "2020-08-31T13:59:56Z",
    >           "name": "csi-manila-cephfs-secrets",
    >           "namespace": "default",
    >           "resourceVersion": "7113",
    >           "selfLink": "/api/v1/namespaces/default/secrets/csi-manila-cephfs-secrets",
    >           "uid": "de90570b-7003-400c-8cdb-f8b179d7bbf5"
    >       },
    >       "type": "Opaque"
    >   }

    Two things to notice:
    1) The 'last-applied-configuration' annotation contains the RAW, UNENCRYPTED, values.
    2) os-applicationCredentialID and os-applicationCredentialSecret have the same value.

    Don't 'apply' secrets, always use 'create'.


# -----------------------------------------------------
# Create our secrets.
#[user@kubernator]

    authurl=$(
        yq r /etc/openstack/clouds.yaml \
            'clouds.gaia-prod.auth.auth_url'
        )

    region=$(
        yq r /etc/openstack/clouds.yaml \
            'clouds.gaia-prod.region_name'
        )

    credid=$(
        yq r /etc/openstack/clouds.yaml \
            'clouds.gaia-prod.auth.application_credential_id'
        )

    credsecret=$(
        yq r /etc/openstack/clouds.yaml \
            'clouds.gaia-prod.auth.application_credential_secret'
        )


    cat > "/tmp/csi-manila-cephfs-secrets.yaml" << EOF
apiVersion: v1
kind: Secret
metadata:
  name: csi-manila-cephfs-secrets
  namespace: default
stringData:
  # Mandatory
  os-authURL: "${authurl:?}"
  os-region: "${region:?}"

  # Authentication using user credentials
  os-applicationCredentialID: "${credid:?}"
  os-applicationCredentialSecret: "${credsecret:?}"

EOF

    kubectl delete \
        secret \
            csi-manila-cephfs-secrets

    kubectl create \
        --filename "/tmp/csi-manila-cephfs-secrets.yaml"

    >   secret/csi-manila-cephfs-secrets created


    kubectl describe \
        secret \
            csi-manila-cephfs-secrets

    >   Name:         csi-manila-cephfs-secrets
    >   Namespace:    default
    >   Labels:       <none>
    >   Annotations:  <none>
    >
    >   Type:  Opaque
    >
    >   Data
    >   ====
    >   os-applicationCredentialSecret:  35 bytes
    >   os-authURL:                      47 bytes
    >   os-region:                       9 bytes
    >   os-applicationCredentialID:      32 bytes


# -----------------------------------------------------
# Delete and re-create the Pod.
#[user@kubernator]

    kubectl \
        delete pod \
            magruela-pod

    >   pod "magruela-pod" deleted


    kubectl apply \
        --filename "/tmp/magruela-pod.yaml"

    >   pod/magruela-pod created


    kubectl \
        describe pod \
            magruela-pod


    >   ....
    >   ....
    >   Events:
    >     Type     Reason       Age              From                                            Message
    >     ----     ------       ----             ----                                            -------
    >     Normal   Scheduled    <unknown>        default-scheduler                               Successfully assigned default/magruela-pod to tiberius-20200831-wq5fv3lbgfco-node-2
    >     Warning  FailedMount  2s (x4 over 6s)  kubelet, tiberius-20200831-wq5fv3lbgfco-node-2  MountVolume.MountDevice failed for volume "magruela-volume" : rpc error: code = InvalidArgument desc = invalid OpenStack secrets: parameter 'os-authURL' requires exactly one of [os-password os-trustID] parameters

    #
    # Spent an interesting hour in the cloud-provider-openstack source code.
    # https://github.com/kubernetes/cloud-provider-openstack

    #
    # Code for handling application credentials is here:
    # https://github.com/gophercloud/gophercloud/tree/master/openstack/identity/v3/applicationcredentials

    #
    # Did find this :
    # https://github.com/kubernetes/cloud-provider-openstack/blob/master/pkg/cloudprovider/providers/openstack/openstack.go#L190

        type AuthOpts struct {
	        AuthURL          string `gcfg:"auth-url" mapstructure:"auth-url" name:"os-authURL" dependsOn:"os-password|os-trustID"`
	        UserID           string `gcfg:"user-id" mapstructure:"user-id" name:"os-userID" value:"optional" dependsOn:"os-password"`
	        Username         string `name:"os-userName" value:"optional" dependsOn:"os-password"`
            ....
            ....
	        ApplicationCredentialID     string `gcfg:"application-credential-id" mapstructure:"application-credential-id" name:"os-applicationCredentialID" value:"optional"`
	        ApplicationCredentialName   string `gcfg:"application-credential-name" mapstructure:"application-credential-name" name:"os-applicationCredentialName" value:"optional"`
	        ApplicationCredentialSecret string `gcfg:"application-credential-secret" mapstructure:"application-credential-secret" name:"os-applicationCredentialSecret" value:"optional"`
        }

    #
    # This bit seems to imply we must have either a password or a trustID :

        AuthURL ... dependsOn:"os-password|os-trustID"`

    #
    # That matches the error message we get :

        parameter 'os-authURL' requires exactly one of [os-password os-trustID] parameters

    #
    # But it might not be the cause ...
    # What if, the secret was loaded when the PersistentVolume was created, or when the Pod was created ?
    #


# -----------------------------------------------------
# Try delete and re-apply the templates.
#[user@kubernator]

    kubectl delete \
        pod \
            magruela-pod

    kubectl delete \
        persistentvolumeclaim \
            magruela-claim

    kubectl delete \
        persistentvolume \
            magruela-volume

    kubectl delete \
        secret \
            csi-manila-cephfs-secrets

    kubectl create \
        --filename "/tmp/csi-manila-cephfs-secrets.yaml"

    kubectl apply \
        --filename "/tmp/magruela-volume.yaml"

    kubectl apply \
        --filename "/tmp/magruela-claim.yaml"

    kubectl apply \
        --filename "/tmp/magruela-pod.yaml"

    kubectl \
        describe pod \
            magruela-pod

    >   ....
    >   ....
    >   Events:
    >     Type     Reason       Age        From                                            Message
    >     ----     ------       ----       ----                                            -------
    >     Normal   Scheduled    <unknown>  default-scheduler                               Successfully assigned default/magruela-pod to tiberius-20200831-wq5fv3lbgfco-node-2
    >     Warning  FailedMount  0s         kubelet, tiberius-20200831-wq5fv3lbgfco-node-2  MountVolume.MountDevice failed for volume "magruela-volume" : rpc error: code = InvalidArgument desc = invalid OpenStack secrets: parameter 'os-authURL' requires exactly one of [os-password os-trustID] parameters


# -----------------------------------------------------
# What if we give it a fake password ?
#[user@kubernator]

    cat > "/tmp/csi-manila-cephfs-secrets.yaml" << EOF
apiVersion: v1
kind: Secret
metadata:
  name: csi-manila-cephfs-secrets
  namespace: default
stringData:
  # Mandatory
  os-authURL: "${authurl:?}"
  os-region: "${region:?}"

  # Fake password for validation
  os-password: "not a password"

  # Authentication using user credentials
  os-applicationCredentialID: "${credid:?}"
  os-applicationCredentialSecret: "${credsecret:?}"

EOF

    ....
    ....

    >   ....
    >   ....
    >   .... invalid OpenStack secrets: parameter 'os-password' requires exactly one of [os-domainID os-domainName] parameters

    #
    # Moved the error on a step.
    # By giving it a password, it now wants [os-domainID os-domainName].

    #
    # From our openrc file :
    os-domainID: "default"
    os-domainName: "ee408625947942aa8b21f86831b39339"


# -----------------------------------------------------
# Add domainID ...
#[user@kubernator]

    cat > "/tmp/csi-manila-cephfs-secrets.yaml" << EOF
apiVersion: v1
kind: Secret
metadata:
  name: csi-manila-cephfs-secrets
  namespace: default
stringData:
  # Mandatory
  os-authURL: "${authurl:?}"
  os-region: "${region:?}"

  # Fake password for validation
  os-password: "not a password"
  os-domainID: "default"

  # Authentication using user credentials
  os-applicationCredentialID: "${credid:?}"
  os-applicationCredentialSecret: "${credsecret:?}"

EOF

    ....
    ....

    >   ....
    >   ....
    >   .... invalid OpenStack secrets: parameter 'os-password' requires exactly one of [os-projectID os-projectName] parameters


# -----------------------------------------------------
# Add projectID ..
#[user@kubernator]

    cat > "/tmp/csi-manila-cephfs-secrets.yaml" << EOF
apiVersion: v1
kind: Secret
metadata:
  name: csi-manila-cephfs-secrets
  namespace: default
stringData:
  # Mandatory
  os-authURL: "${authurl:?}"
  os-region: "${region:?}"

  # Fake password for validation
  os-password:    "not a password"

  # Extra stuff for validation
  os-domainID:    "default"
  #os-domainName:  "ee408625947942aa8b21f86831b39339"
  #os-projectID:   "21b4ae3a2ea44bc5a9c14005ed2963af"
  os-projectName: "iris-gaia-prod"

  # Authentication using user credentials
  os-applicationCredentialID: "${credid:?}"
  os-applicationCredentialSecret: "${credsecret:?}"

EOF

    ....
    ....

    >   ....
    >   ....
    >   .... invalid OpenStack secrets: parameter 'os-password' requires exactly one of [os-userID os-userName] parameters


# -----------------------------------------------------
# Add a fake userName ..
#[user@kubernator]

    cat > "/tmp/csi-manila-cephfs-secrets.yaml" << EOF
apiVersion: v1
kind: Secret
metadata:
  name: csi-manila-cephfs-secrets
  namespace: default
stringData:
  # Mandatory
  os-authURL: "${authurl:?}"
  os-region: "${region:?}"

  # Fake username and password for validation
  os-userName:    "not my name"
  os-password:    "not a password"

  # Extra stuff for validation
  os-domainID:    "default"
  #os-domainName:  "ee408625947942aa8b21f86831b39339"
  #os-projectID:   "21b4ae3a2ea44bc5a9c14005ed2963af"
  os-projectName: "iris-gaia-prod"

  # Authentication using user credentials
  os-applicationCredentialID: "${credid:?}"
  os-applicationCredentialSecret: "${credsecret:?}"

EOF


    >   ....
    >   ....
    >   .... failed to create Manila v2 client: failed to authenticate: Authentication failed

    #
    # Not a suprise.
    # Username and password are taken before application credentials.
    #

# -----------------------------------------------------
# Revert back to what it should be.
#[user@kubernator]

    cat > "/tmp/csi-manila-cephfs-secrets.yaml" << EOF
apiVersion: v1
kind: Secret
metadata:
  name: csi-manila-cephfs-secrets
  namespace: default
stringData:
  # Mandatory
  os-authURL: "${authurl:?}"
  os-region: "${region:?}"

  # Authentication using user credentials
  os-applicationCredentialID: "${credid:?}"
  os-applicationCredentialSecret: "${credsecret:?}"

EOF



    #
    # Do we need to build and deploy a modified version of the CSI plugin !?
    # https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/getting-started-provider-dev.md
    #

    #
    # Will this project never end ?
    #

    #
    # Before we go there ... is the share public ?
    #
    # Simple test, confirm the share is public ?
    # Share is set to public, still fails.
    #
    # Simple test, what if we remove the secrets from the volume spec ?

    vi /tmp/magruela-volume.yaml

        #   nodeStageSecretRef:
        #     name: csi-manila-cephfs-secrets
        #     namespace: default
        #   nodePublishSecretRef:
        #     name: csi-manila-cephfs-secrets
        #     namespace: default


    >   ....
    >   ....
    >     Warning  FailedMount
    >       kubelet, tiberius-20200831-wq5fv3lbgfco-node-2
    >           MountVolume.MountDevice failed for volume "magruela-volume"
    >               rpc error: code = InvalidArgument desc = stage secrets cannot be nil or empty

    #
    # Technically, shouldn't we be able to mount a public read-only share without credentials ?
    # That is another day ...
    #

    #
    # Before we go there ... can we use the os-trusteeID and os-trusteePassword option to test ?
    # https://superuser.openstack.org/articles/tutorial-create-self-trust-in-keystone/
    # Not a high enough level of confidence in the limited documentation.
    # Not totally clear how we create the values for os-trusteeID and os-trusteePassword.
    # Do we need to be an Openstack administrator to be able to see the listof roles ?
    # What roles do we choose ?
    # Can we do any of this without a user name and password ?
    #

    #
    # Can we give ourselves a password ?
    # https://cumulus.openstack.hpc.cam.ac.uk/settings/password/
    # Best try that day time when there are people to bail us out when we break things.
    #














# -----------------------------------------------------
# Delete and re-create from templates.
#[user@kubernator]

    kubectl delete \
        pod \
            magruela-pod

    kubectl delete \
        persistentvolumeclaim \
            magruela-claim

    kubectl delete \
        persistentvolume \
            magruela-volume

    kubectl delete \
        secret \
            csi-manila-cephfs-secrets

    kubectl create \
        --filename "/tmp/csi-manila-cephfs-secrets.yaml"

    kubectl apply \
        --filename "/tmp/magruela-volume.yaml"

    kubectl apply \
        --filename "/tmp/magruela-claim.yaml"

    kubectl apply \
        --filename "/tmp/magruela-pod.yaml"

    kubectl \
        describe pod \
            magruela-pod

