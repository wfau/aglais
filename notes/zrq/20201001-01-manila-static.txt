#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    Plan to make the static share deployments repeatable.
    Time to create our own Helm charts.

    Result - works for a 5G static share :-)


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --hostname kubernator \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --env "clustername=${CLUSTER_NAME:?}" \
        --volume "${HOME}/clouds.yaml:/etc/openstack/clouds.yaml:z" \
        --volume "${AGLAIS_CODE}/experiments/zrq/helm:/helm:z" \
        --volume "${AGLAIS_CODE}/experiments/zrq/kubernetes:/kubernetes:z" \
        atolmis/openstack-client \
        bash


# -----------------------------------------------------
# Set the deployment params.
#[user@kubernator]

    sharename=rutoria
    sharesize=5
    sharepublic=false


# -----------------------------------------------------
# Get the connection details for our cluster.
#[user@kubernator]

    mkdir -p "${HOME}/.kube"
    openstack \
        --os-cloud "${cloudname:?}" \
        coe cluster config \
            "${clustername:?}" \
                --force \
                --dir "${HOME}/.kube"


    kubectl \
        cluster-info

    >   Kubernetes master is running at https://....
    >   Heapster is running at https://....
    >   CoreDNS is running at https://....


# -----------------------------------------------------
# Set the Manila API version.
# https://stackoverflow.com/a/58806536
#[user@kubernator]

    export OS_SHARE_API_VERSION=2.51


# -----------------------------------------------------
# Install YQ.
# TODO - add this to the kubernator image
#[user@kubernator]

    mkdir     "${HOME:?}/bin"
    wget -O   "${HOME:?}/bin/yq" https://github.com/mikefarah/yq/releases/download/3.3.2/yq_linux_amd64
    chmod a+x "${HOME:?}/bin/yq"


# -----------------------------------------------------
# Create a new static share.
# https://docs.openstack.org/python-openstackclient/latest/cli/plugin-commands/manila.html#share-create
#[user@kubernator]

    openstack \
        --os-cloud "${cloudname:?}" \
        share create \
            --format json \
            --name   "${sharename:?}" \
            --public "${sharepublic:?}" \
            --share-type 'cephfsnativetype' \
            --availability-zone 'nova' \
            'CEPHFS' \
            "${sharesize:?}" \
    | tee "/tmp/${sharename:?}-share.json"

    shareid=$(
        jq -r '.id' "/tmp/${sharename:?}-share.json"
        )

    openstack \
        --os-cloud "${cloudname:?}" \
            share show \
                --format json \
                "${shareid:?}"

    >   {
    >     "access_rules_status": "active",
    >     "availability_zone": "nova",
    >     "create_share_from_snapshot_support": false,
    >     "created_at": "2020-10-01T03:57:22.000000",
    >     "description": null,
    >     "export_locations": "\npath = 10.206.1.5:6789,10.206.1.6:6789,10.206.1.7:6789:/volumes/_nogroup/922a9231-6dc7-48f9-95ab-e3fbd5d5612f\nid = 84efe246-8364-42ac-bdad-c0d4bdd15136\npreferred = False",
    >     "has_replicas": false,
    >     "id": "1e6ce46c-46e0-4002-89fe-3a06c436f824",
    >     "is_public": false,
    >     "mount_snapshot_support": false,
    >     "name": "rutoria",
    >     "project_id": "21b4ae3a2ea44bc5a9c14005ed2963af",
    >     "properties": {},
    >     "replication_type": null,
    >     "revert_to_snapshot_support": false,
    >     "share_group_id": null,
    >     "share_network_id": null,
    >     "share_proto": "CEPHFS",
    >     "share_type": "5d0f58c5-ed21-4e1f-91bb-fe1a49deb5d8",
    >     "share_type_name": "cephfsnativetype",
    >     "size": 5,
    >     "snapshot_id": null,
    >     "snapshot_support": false,
    >     "source_share_group_snapshot_member_id": null,
    >     "status": "available",
    >     "task_state": null,
    >     "user_id": "98169f87de174ad4ac98c32e59646488",
    >     "volume_type": "cephfsnativetype"
    >   }


# -----------------------------------------------------
# Create a RW access rule for our share.
#[user@kubernator]

    openstack \
        --os-cloud "${cloudname:?}" \
        share access create \
            --format json \
            --access-level 'rw' \
            "${shareid:?}" \
            'cephx' \
            "${sharename:?}-admin" \
    | tee "/tmp/${sharename:?}-access.json"

    accessid=$(
        jq -r '.id' "/tmp/${sharename:?}-access.json"
        )

    openstack \
        --os-cloud "${cloudname:?}" \
            share access show \
                --format json \
                "${accessid:?}"

    >   {
    >     "id": "0f4c7585-8407-45ac-9473-db10685c0e84",
    >     "share_id": "1e6ce46c-46e0-4002-89fe-3a06c436f824",
    >     "access_level": "rw",
    >     "access_to": "rutoria-admin",
    >     "access_type": "cephx",
    >     "state": "active",
    >     "access_key": "AQC6U3VfCkvIHxAAePXJ5SnMDRvUJdsKxYxdpA==",
    >     "created_at": "2020-10-01T03:57:46.000000",
    >     "updated_at": "2020-10-01T03:57:46.000000",
    >     "properties": ""
    >   }


# -----------------------------------------------------
# Try a dry run first ...
#[user@kubernator]

    source "${HOME}/aglais.env"

cat > "/tmp/${sharename:?}-values.yaml" << EOF

aglais:
  dataset: "test-data"

share:
  name:   "${sharename:?}"
  size:   "${sharesize:?}"
  access: "ReadWriteOnce"

openstack:
  shareid:   "${shareid:?}"
  accessid:  "${accessid:?}"
EOF

    helm install \
        --debug \
        --dry-run \
        "${sharename:?}" \
        "/helm/manila-static-share" \
        --values "/tmp/${sharename:?}-values.yaml"

    >   install.go:159: [debug] Original chart version: ""
    >   install.go:176: [debug] CHART PATH: /helm/manila-static-share
    >
    >   NAME: rutoria
    >   LAST DEPLOYED: Thu Oct  1 04:30:33 2020
    >   NAMESPACE: default
    >   STATUS: pending-install
    >   REVISION: 1
    >   TEST SUITE: None
    >   USER-SUPPLIED VALUES:
    >   aglais:
    >     dataset: test-data
    >   openstack:
    >     accessid: 0f4c7585-8407-45ac-9473-db10685c0e84
    >     shareid: 1e6ce46c-46e0-4002-89fe-3a06c436f824
    >   share:
    >     access: ReadWriteOnce
    >     name: rutoria
    >     size: "5"
    >
    >   COMPUTED VALUES:
    >   aglais:
    >     dataset: test-data
    >   openstack:
    >     accessid: 0f4c7585-8407-45ac-9473-db10685c0e84
    >     shareid: 1e6ce46c-46e0-4002-89fe-3a06c436f824
    >   share:
    >     access: ReadWriteOnce
    >     name: rutoria
    >     size: "5"
    >   testpod:
    >     image: fedora:32
    >
    >   HOOKS:
    >   MANIFEST:
    >   ---
    >   # Source: manila-static-share/templates/volume.yaml
    >   apiVersion: v1
    >   kind: PersistentVolume
    >   metadata:
    >     name: "rutoria-volume"
    >     labels:
    >       aglais.name:    "rutoria"
    >       aglais.dataset: "test-data"
    >       helm.sh/chart:  "manila-static-share-0.0.1"
    >       app.kubernetes.io/name:       "manila-static-share"
    >       app.kubernetes.io/instance:   "rutoria"
    >       app.kubernetes.io/version:    "0.0.1"
    >       app.kubernetes.io/component:  "test-data"
    >       app.kubernetes.io/managed-by: "Helm"
    >   spec:
    >     accessModes:
    >     - "ReadWriteOnce"
    >     capacity:
    >       storage: "5G"
    >     csi:
    >       driver: cephfs.manila.csi.openstack.org
    >       nodeStageSecretRef:
    >         name: "os-trustee"
    >         namespace: "kube-system"
    >       nodePublishSecretRef:
    >         name: "os-trustee"
    >         namespace: "kube-system"
    >       volumeHandle: "rutoria-handle"
    >       volumeAttributes:
    >         shareID: "1e6ce46c-46e0-4002-89fe-3a06c436f824"
    >         shareAccessID: "0f4c7585-8407-45ac-9473-db10685c0e84"
    >   ---
    >   # Source: manila-static-share/templates/volume-claim.yaml
    >   apiVersion: v1
    >   kind: PersistentVolumeClaim
    >   metadata:
    >     name: "rutoria-claim"
    >     labels:
    >       aglais.name:    "rutoria"
    >       aglais.dataset: "test-data"
    >       helm.sh/chart:  "manila-static-share-0.0.1"
    >       app.kubernetes.io/name:       "manila-static-share"
    >       app.kubernetes.io/instance:   "rutoria"
    >       app.kubernetes.io/version:    "0.0.1"
    >       app.kubernetes.io/component:  "test-data"
    >       app.kubernetes.io/managed-by: "Helm"
    >   spec:
    >     accessModes:
    >     - "ReadWriteOnce"
    >     resources:
    >       requests:
    >         storage: "5G"
    >     selector:
    >       matchLabels:
    >         aglais.name:    "rutoria"
    >         aglais.dataset: "test-data"
    >         app.kubernetes.io/instance: "rutoria"
    >   ---
    >   # Source: manila-static-share/templates/test-pod.yaml
    >   apiVersion: v1
    >   kind: Pod
    >   metadata:
    >     name: "rutoria-testpod"
    >     labels:
    >       aglais.name:    "rutoria"
    >       aglais.dataset: "test-data"
    >       helm.sh/chart:  "manila-static-share-0.0.1"
    >       app.kubernetes.io/name:       "manila-static-share"
    >       app.kubernetes.io/instance:   "rutoria"
    >       app.kubernetes.io/version:    "0.0.1"
    >       app.kubernetes.io/component:  "test-data"
    >       app.kubernetes.io/managed-by: "Helm"
    >   spec:
    >     volumes:
    >       - name: share-data
    >         persistentVolumeClaim:
    >           claimName: "rutoria-claim"
    >       - name: local-data
    >         emptyDir: {}
    >     containers:
    >       - name: "rutoria-container"
    >         image: "fedora:32"
    >         volumeMounts:
    >           - name: share-data
    >             mountPath: /share-data
    >           - name: local-data
    >             mountPath: /local-data
    >         command: ["/bin/sh"]
    >         args:
    >           - "-c"
    >           - >-
    >             while true; do
    >             date >> /local-data/${HOSTNAME}.log;
    >             sleep 1;
    >             done


# -----------------------------------------------------
# Install our Chart.
#[user@kubernator]

    helm install \
        "${sharename:?}" \
        "/helm/manila-static-share" \
        --values "/tmp/${sharename:?}-values.yaml"

    >   NAME: rutoria
    >   LAST DEPLOYED: Thu Oct  1 04:45:10 2020
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 1
    >   TEST SUITE: None
    >   NOTES:
    >   Use the testpod to check access to the mounted volume.


# -----------------------------------------------------
# Check all the components are there.
#[user@kubernator]

    kubectl describe \
        PersistentVolume \
            "${sharename:?}-volume"

    >   Name:            rutoria-volume
    >   Labels:          aglais.dataset=test-data
    >                    aglais.name=rutoria
    >                    app.kubernetes.io/component=test-data
    >                    app.kubernetes.io/instance=rutoria
    >                    app.kubernetes.io/managed-by=Helm
    >                    app.kubernetes.io/name=manila-static-share
    >                    app.kubernetes.io/version=0.0.1
    >                    helm.sh/chart=manila-static-share-0.0.1
    >   Annotations:     meta.helm.sh/release-name: rutoria
    >                    meta.helm.sh/release-namespace: default
    >                    pv.kubernetes.io/bound-by-controller: yes
    >   Finalizers:      [kubernetes.io/pv-protection]
    >   StorageClass:
    >   Status:          Bound
    >   Claim:           default/rutoria-claim
    >   Reclaim Policy:  Retain
    >   Access Modes:    RWO
    >   VolumeMode:      Filesystem
    >   Capacity:        5G
    >   Node Affinity:   <none>
    >   Message:
    >   Source:
    >       Type:              CSI (a Container Storage Interface (CSI) volume source)
    >       Driver:            cephfs.manila.csi.openstack.org
    >       VolumeHandle:      rutoria-handle
    >       ReadOnly:          false
    >       VolumeAttributes:      shareAccessID=0f4c7585-8407-45ac-9473-db10685c0e84
    >                              shareID=1e6ce46c-46e0-4002-89fe-3a06c436f824
    >   Events:                <none>


    kubectl describe \
        PersistentVolumeClaim \
            "${sharename:?}-claim"

    >   Name:          rutoria-claim
    >   Namespace:     default
    >   StorageClass:
    >   Status:        Bound
    >   Volume:        rutoria-volume
    >   Labels:        aglais.dataset=test-data
    >                  aglais.name=rutoria
    >                  app.kubernetes.io/component=test-data
    >                  app.kubernetes.io/instance=rutoria
    >                  app.kubernetes.io/managed-by=Helm
    >                  app.kubernetes.io/name=manila-static-share
    >                  app.kubernetes.io/version=0.0.1
    >                  helm.sh/chart=manila-static-share-0.0.1
    >   Annotations:   meta.helm.sh/release-name: rutoria
    >                  meta.helm.sh/release-namespace: default
    >                  pv.kubernetes.io/bind-completed: yes
    >                  pv.kubernetes.io/bound-by-controller: yes
    >   Finalizers:    [kubernetes.io/pvc-protection]
    >   Capacity:      5G
    >   Access Modes:  RWO
    >   VolumeMode:    Filesystem
    >   Mounted By:    rutoria-testpod
    >   Events:        <none>


    kubectl describe \
        Pod \
            "${sharename:?}-testpod"

    >   Name:         rutoria-testpod
    >   Namespace:    default
    >   Node:         tiberius-20200923-nqzekodqww64-node-3/10.0.0.41
    >   Start Time:   Thu, 01 Oct 2020 04:45:10 +0000
    >   Labels:       aglais.dataset=test-data
    >                 aglais.name=rutoria
    >                 app.kubernetes.io/component=test-data
    >                 app.kubernetes.io/instance=rutoria
    >                 app.kubernetes.io/managed-by=Helm
    >                 app.kubernetes.io/name=manila-static-share
    >                 app.kubernetes.io/version=0.0.1
    >                 helm.sh/chart=manila-static-share-0.0.1
    >   Annotations:  meta.helm.sh/release-name: rutoria
    >                 meta.helm.sh/release-namespace: default
    >   Status:       Running
    >   IP:           10.100.2.15
    >   Containers:
    >     rutoria-container:
    >       Container ID:  docker://376d5480182a642b584fac95d6efd163276ac0f256f2941cf1f694609d720652
    >       Image:         fedora:32
    >       Image ID:      docker-pullable://docker.io/fedora@sha256:d6a6d60fda1b22b6d5fe3c3b2abe2554b60432b7b215adc11a2b5fae16f50188
    >       Port:          <none>
    >       Host Port:     <none>
    >       Command:
    >         /bin/sh
    >       Args:
    >         -c
    >         while true; do date >> /local-data/${HOSTNAME}.log; sleep 1; done
    >       State:          Running
    >         Started:      Thu, 01 Oct 2020 04:45:17 +0000
    >       Ready:          True
    >       Restart Count:  0
    >       Environment:    <none>
    >       Mounts:
    >         /local-data from local-data (rw)
    >         /share-data from share-data (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from default-token-pxrzv (ro)
    >   Conditions:
    >     Type              Status
    >     Initialized       True
    >     Ready             True
    >     ContainersReady   True
    >     PodScheduled      True
    >   Volumes:
    >     share-data:
    >       Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    >       ClaimName:  rutoria-claim
    >       ReadOnly:   false
    >     local-data:
    >       Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    >       Medium:
    >       SizeLimit:  <unset>
    >     default-token-pxrzv:
    >       Type:        Secret (a volume populated by a Secret)
    >       SecretName:  default-token-pxrzv
    >       Optional:    false
    >   QoS Class:       BestEffort
    >   Node-Selectors:  <none>
    >   Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
    >                    node.kubernetes.io/unreachable:NoExecute for 300s
    >   Events:
    >     Type    Reason     Age        From                                            Message
    >     ----    ------     ----       ----                                            -------
    >     Normal  Scheduled  <unknown>  default-scheduler                               Successfully assigned default/rutoria-testpod to tiberius-20200923-nqzekodqww64-node-3
    >     Normal  Pulling    67s        kubelet, tiberius-20200923-nqzekodqww64-node-3  Pulling image "fedora:32"
    >     Normal  Pulled     63s        kubelet, tiberius-20200923-nqzekodqww64-node-3  Successfully pulled image "fedora:32"
    >     Normal  Created    63s        kubelet, tiberius-20200923-nqzekodqww64-node-3  Created container rutoria-container
    >     Normal  Started    63s        kubelet, tiberius-20200923-nqzekodqww64-node-3  Started container rutoria-container


# -----------------------------------------------------
# Login to our test pod.
#[user@kubernator]

    kubectl exec \
        --tty \
        --stdin \
        "${sharename:?}-testpod" \
            -- \
                bash


# -----------------------------------------------------
# -----------------------------------------------------
# Check we can write to the local data.
#[root@rutoria]

    ls -al /local-data/

    >   total 4
    >   drwxrwxrwx. 2 root root   33 Oct  1 04:54 .
    >   drwxr-xr-x. 1 root root   53 Oct  1 04:54 ..
    >   -rw-r--r--. 1 root root 1276 Oct  1 04:54 rutoria-testpod.log


    tail /local-data/${HOSTNAME:?}.log

    >   ....
    >   ....
    >   Thu Oct  1 04:56:39 UTC 2020
    >   Thu Oct  1 04:56:40 UTC 2020
    >   Thu Oct  1 04:56:41 UTC 2020
    >   Thu Oct  1 04:56:42 UTC 2020


# -----------------------------------------------------
# Check we can write to the shared data.
#[root@rutoria]

    ls -al /share-data/

    >   drwxrwxrwx. 2 root root  0 Oct  1 03:57 .
    >   drwxr-xr-x. 1 root root 53 Oct  1 04:54 ..

    for i in {0..8}
    do
        date >> /share-data/${HOSTNAME:?}.log
    done

    tail    /share-data/${HOSTNAME:?}.log

    >   ....
    >   ....
    >   Thu Oct  1 05:00:54 UTC 2020
    >   Thu Oct  1 05:00:54 UTC 2020
    >   Thu Oct  1 05:00:54 UTC 2020
    >   Thu Oct  1 05:00:54 UTC 2020


# -----------------------------------------------------
# -----------------------------------------------------
# Delete our Pod, Claim and Volume.
#[user@kubernator]

    kubectl delete \
        Pod \
            "${sharename:?}-testpod"

    kubectl delete \
        PersistentVolumeClaim \
            "${sharename:?}-claim"

    kubectl delete \
        PersistentVolume \
            "${sharename:?}-volume"


# -----------------------------------------------------
# Deploy our Chart components again.
#[user@kubernator]

    helm upgrade \
        "${sharename:?}" \
        "/helm/manila-static-share" \
        --values "/tmp/${sharename:?}-values.yaml"

    >   Release "rutoria" has been upgraded. Happy Helming!
    >   NAME: rutoria
    >   LAST DEPLOYED: Thu Oct  1 05:16:50 2020
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 3
    >   TEST SUITE: None
    >   NOTES:
    >   Use the testpod to check access to the mounted volume.


# -----------------------------------------------------
# Login to our test pod.
#[user@kubernator]

    kubectl exec \
        --tty \
        --stdin \
        "${sharename:?}-testpod" \
            -- \
                bash


# -----------------------------------------------------
# -----------------------------------------------------
# Check we can write to the local data.
#[root@rutoria]

    ls -al /local-data/

    >   total 4
    >   drwxrwxrwx. 2 root root   33 Oct  1 05:16 .
    >   drwxr-xr-x. 1 root root   53 Oct  1 05:16 ..
    >   -rw-r--r--. 1 root root 1769 Oct  1 05:17 rutoria-testpod.log


    head /local-data/${HOSTNAME:?}.log

    >   Thu Oct  1 05:16:55 UTC 2020
    >   Thu Oct  1 05:16:56 UTC 2020
    >   Thu Oct  1 05:16:57 UTC 2020
    >   Thu Oct  1 05:16:58 UTC 2020
    >   ....
    >   ....

    tail /local-data/${HOSTNAME:?}.log

    >   ....
    >   ....
    >   Thu Oct  1 05:18:08 UTC 2020
    >   Thu Oct  1 05:18:09 UTC 2020
    >   Thu Oct  1 05:18:10 UTC 2020
    >   Thu Oct  1 05:18:11 UTC 2020


# -----------------------------------------------------
# Check we can write to the shared data.
#[root@rutoria]

    ls -al /share-data/

    >   total 1
    >   drwxrwxrwx. 2 root root 377 Oct  1 04:57 .
    >   drwxr-xr-x. 1 root root  53 Oct  1 05:16 ..
    >   -rw-r--r--. 1 root root 377 Oct  1 05:00 rutoria-testpod.log


    head    /share-data/${HOSTNAME:?}.log

    >   Thu Oct  1 04:57:55 UTC 2020
    >   Thu Oct  1 04:58:19 UTC 2020
    >   Thu Oct  1 04:58:19 UTC 2020
    >   Thu Oct  1 04:58:19 UTC 2020
    >   ....
    >   ....


    for i in {0..8}
    do
        date >> /share-data/${HOSTNAME:?}.log
    done


    tail    /share-data/${HOSTNAME:?}.log

    >   ....
    >   ....
    >   Thu Oct  1 05:19:49 UTC 2020
    >   Thu Oct  1 05:19:49 UTC 2020
    >   Thu Oct  1 05:19:49 UTC 2020
    >   Thu Oct  1 05:19:49 UTC 2020





