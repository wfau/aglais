#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    Spark Read Text File from AWS S3 bucket
    https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/



# -----------------------------------------------------
# Try reading data from S3 data store.
#[user@zeppelin]


    # ---- ----

    %spark

    spark.read.parquet("s3a://gaia-dr2-csv")

    # ---- ----


    >   Class org.apache.hadoop.fs.s3a.S3AFileSystem not found



    List of Jars to include

        https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common/3.3.0
        org.apache.hadoop:hadoop-common:jar:3.3.0

        https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client/3.3.0
        org.apache.hadoop:hadoop-client:jar:3.3.0

        https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws/3.3.0
        org.apache.hadoop:hadoop-aws:jar:3.3.0

    Looks like we already have most of these in the jars directory.

        source "${HOME}/aglais.env"
        pushd "${SPARK_DIST:?}"
            pushd "spark-${SPARK_VERSION:?}"
                pushd "spark-${SPARK_VERSION:?}-bin-${SPARK_HADOOP:?}"

                    find . -name hadoop*

                popd
            popd
        popd


    >   ./jars/hadoop-annotations-3.2.0.jar
    >   ./jars/hadoop-mapreduce-client-jobclient-3.2.0.jar
    >   ./jars/hadoop-yarn-server-web-proxy-3.2.0.jar
    >   ./jars/hadoop-mapreduce-client-common-3.2.0.jar
    >   ./jars/hadoop-yarn-api-3.2.0.jar
    >   ./jars/hadoop-yarn-registry-3.2.0.jar
    >   ./jars/hadoop-mapreduce-client-core-3.2.0.jar
    >   ./jars/hadoop-common-3.2.0.jar
    >   ./jars/hadoop-yarn-server-common-3.2.0.jar
    >   ./jars/hadoop-yarn-common-3.2.0.jar
    >   ./jars/hadoop-auth-3.2.0.jar
    >   ./jars/hadoop-client-3.2.0.jar
    >   ./jars/hadoop-yarn-client-3.2.0.jar
    >   ./jars/hadoop-hdfs-client-3.2.0.jar

        Missing hadoop-aws.
        Get the same version as the rest of the jars.
        https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws/3.2.0

            org.apache.hadoop:hadoop-aws:3.2.0

        Trying to add jars using the 'spark.jars.packages' config property.
        Totally ignored.
        Tried using the interpreter setting page in the GUI.
        https://zeppelin.apache.org/docs/latest/usage/interpreter/dependency_management.html

            No errors, just does nothing.

        Tried explicitly setting it in the notebook
        https://zeppelin.apache.org/docs/latest/interpreter/spark.html#2-loading-spark-properties

            %spark.conf
            spark.jars.packages org.apache.hadoop:hadoop-aws:3.2.0

            No errors, just does nothing.

        https://zeppelin.apache.org/docs/latest/usage/interpreter/dependency_management.html


        This sounds nice, but isn't available
        https://zeppelin.apache.org/docs/latest/interpreter/spark.html#3-dynamic-dependency-loading-via-sparkdep-interpreter

            %spark.dep
            z.load("org.apache.hadoop:hadoop-aws:3.2.0")

            >   Interpreter spark.dep not found


        Our deployment seems to be ignoring settings.
        Which might explain the problems we encountered with pyhton version.




        Solve one problem at a time.
        Skip spark.jars.packages.

        Try adding the jar to our Docker image.
        notes/zrq/20200722-03-spark-k8s.txt



From previous notes ..

# -----------------------------------------------------
# List our EC2 credentials.
#[user@openstacker]

    openstack \
        --os-cloud "${cloudname:?}" \
        ec2 credentials \
            list


        context._jsc.hadoopConfiguration().set(
            "fs.s3a.endpoint", "cumulus.openstack.hpc.cam.ac.uk:6780/swift/v1/"
            )
        context._jsc.hadoopConfiguration().set(
            "fs.s3a.path.style.access", "true"
            )
        context._jsc.hadoopConfiguration().set(
            "fs.s3a.access.key", "93d0....f83c"
            )
        context._jsc.hadoopConfiguration().set(
            "fs.s3a.secret.key", "0e28....25b1"
            )







