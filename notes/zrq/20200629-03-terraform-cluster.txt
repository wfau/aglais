#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    # Create Kubernetes cluster using Terraform.

# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --hostname terraformer \
        --volume "${HOME}/clouds.yaml:/etc/openstack/clouds.yaml:z" \
        --volume "${AGLAIS_CODE}/experiments/zrq/terraform:/terraform:z" \
        atolmis/terraform-client \
        bash


# -----------------------------------------------------
# Set the cloud, credentials and cluster names.
#[user@terraformer]

    cloudname=gaia-prod
    clustername=Tiberius
    keypairname=zrq-gaia-keypair

    # Export them as TF_VAR variables so that Terraform will pick them up.
    # TODO find a better way of doing this ?

    export TF_VAR_zrq_cloud_name=${cloudname:?}-super
    export TF_VAR_zrq_cluster_name=${clustername:?}
    export TF_VAR_zrq_keypair_name=${keypairname}


# -----------------------------------------------------
# Delete the old state.
#[user@terraformer]

    rm -rf /terraform/.terraform
    rm -f  /terraform/tfvars


# -----------------------------------------------------
# Run Terraform to deploy our cluster.
#[user@terraformer]

    pushd "/terraform"

        terraform init

    >   Initializing modules...
    >   
    >   Initializing the backend...
    >   
    >   Initializing provider plugins...
    >   - Checking for available provider plugins...
    >   - Downloading plugin for provider "null" (hashicorp/null) 2.1.2...
    >   - Downloading plugin for provider "openstack" (terraform-providers/openstack) 1.29.0...
    >   
    >   ....
    >   
    >   Terraform has been successfully initialized!


        terraform plan

    >   ....
    >     # module.cluster.null_resource.kubeconfig is tainted, so must be replaced
    >   -/+ resource "null_resource" "kubeconfig" {
    >       ....
    >       }
    >   
    >     # module.cluster.openstack_compute_keypair_v2.zrq_keypair will be created
    >     + resource "openstack_compute_keypair_v2" "zrq_keypair" {
    >       ....
    >       }
    >   
    >     # module.cluster.openstack_containerinfra_cluster_v1.cluster will be created
    >     + resource "openstack_containerinfra_cluster_v1" "cluster" {
    >       ....
    >       }
    >   ....


        terraform apply

    >   ....
    >   ....
    >   module.cluster.openstack_compute_keypair_v2.zrq_keypair: Creating...
    >   module.cluster.null_resource.kubeconfig: Destroying... [id=3967193945064285872]
    >   module.cluster.null_resource.kubeconfig: Destruction complete after 0s
    >   module.cluster.openstack_compute_keypair_v2.zrq_keypair: Creation complete after 1s [id=Tiberius-keypair]
    >   module.cluster.openstack_containerinfra_cluster_v1.zrq_cluster: Creating...
    >   module.cluster.openstack_containerinfra_cluster_v1.zrq_cluster: Still creating... [10s elapsed]
    >   module.cluster.openstack_containerinfra_cluster_v1.zrq_cluster: Still creating... [20s elapsed]
    >   ....
    >   ....
    >   module.cluster.openstack_containerinfra_cluster_v1.zrq_cluster: Still creating... [5m40s elapsed]
    >   module.cluster.openstack_containerinfra_cluster_v1.zrq_cluster: Still creating... [5m50s elapsed]
    >   module.cluster.openstack_containerinfra_cluster_v1.zrq_cluster: Still creating... [6m0s elapsed]
    >   module.cluster.openstack_containerinfra_cluster_v1.zrq_cluster: Creation complete after 6m5s [id=83c8ebdf-4b94-45f1-9dd9-4cecf9278876]
    >   module.cluster.null_resource.kubeconfig: Creating...
    >   module.cluster.null_resource.kubeconfig: Provisioning with 'local-exec'...
    >   module.cluster.null_resource.kubeconfig (local-exec): Executing: ["/bin/sh" "-c" "mkdir -p ~/.kube/Tiberius; openstack --os-cloud gaia-prod-super coe cluster config Tiberius --dir ~/.kube/Tiberius --force;"]
    >   module.cluster.null_resource.kubeconfig: Still creating... [10s elapsed]
    >   module.cluster.null_resource.kubeconfig (local-exec): 'SHELL'
    >   
    >   
    >   Error: Error running command 'mkdir -p ~/.kube/Tiberius; openstack --os-cloud gaia-prod-super coe cluster config Tiberius --dir ~/.kube/Tiberius --force;': exit status 1. Output: 'SHELL'
    >   

    popd

    #
    # Not sure where the error comes from - config gets created OK.
    #


# -----------------------------------------------------
# Get the details for our new cluster.
#[user@terraformer]

    clusterid=$(
        openstack \
        --os-cloud "${cloudname:?}" \
            coe cluster list \
                --format json \
        | jq -r '.[] | select(.name | test("^'${clustername:?}'")) | .uuid'
        )

    openstack \
        --os-cloud "${cloudname:?}" \
        coe cluster show \
            "${clusterid}" \
                --format json \
    | jq '.'

    >   {
    >     "status": "CREATE_COMPLETE",
    >     "health_status": "HEALTHY",
    >     "cluster_template_id": "d54167d9-495f-437e-88fe-d182b2a230ea",
    >     "node_addresses": [
    >       "10.0.0.78"
    >     ],
    >     "uuid": "83c8ebdf-4b94-45f1-9dd9-4cecf9278876",
    >     "stack_id": "7563df81-3e9c-417c-ba5a-532da5fca4fb",
    >     "status_reason": null,
    >     "created_at": "2020-06-30T00:12:48+00:00",
    >     "updated_at": "2020-06-30T00:18:40+00:00",
    >     "coe_version": "v1.15.9",
    >     "labels": {
    >       "auto_healing_controller": "magnum-auto-healer",
    >       "max_node_count": "2",
    >       "cloud_provider_tag": "v1.15.0",
    >       "etcd_tag": "3.3.17",
    >       "monitoring_enabled": "true",
    >       "tiller_enabled": "true",
    >       "autoscaler_tag": "v1.15.2",
    >       "master_lb_floating_ip_enabled": "true",
    >       "min_node_count": "1",
    >       "tiller_tag": "v2.16.1",
    >       "use_podman": "true",
    >       "auto_healing_enabled": "true",
    >       "heat_container_agent_tag": "train-stable-1",
    >       "kube_tag": "v1.15.9",
    >       "auto_scaling_enabled": "true"
    >     },
    >     "labels_overridden": "",
    >     "labels_skipped": "",
    >     "labels_added": "",
    >     "faults": "",
    >     "keypair": "Tiberius-keypair",
    >     "api_address": "https://128.232.227.203:6443",
    >     "master_addresses": [
    >       "10.0.0.50"
    >     ],
    >     "create_timeout": null,
    >     "node_count": 1,
    >     "discovery_url": "https://discovery.etcd.io/f1ad6a34a4a4836d68bb4dbbca1f4afe",
    >     "master_count": 1,
    >     "container_version": "1.12.6",
    >     "name": "Tiberius",
    >     "master_flavor_id": "general.v1.tiny",
    >     "flavor_id": "general.v1.tiny",
    >     "health_status_reason": {
    >       "api": "ok",
    >       "tiberius-alfjmzar3cyv-master-0.Ready": "True",
    >       "tiberius-alfjmzar3cyv-node-0.Ready": "True"
    >     },
    >     "project_id": "21b4ae3a2ea44bc5a9c14005ed2963af"
    >   }

    #
    # OK, looks good.
    #

# -----------------------------------------------------
# Check the kubectl config for our cluster.
#[user@terraformer]

    cat "${HOME}/.kube/${clustername:?}/config"

    >   apiVersion: v1
    >   clusters:
    >   - cluster:
    >       certificate-authority-data: LS0tLS1C........UtLS0tLQ==
    >       server: https://128.232.227.203:6443
    >     name: Tiberius
    >   contexts:
    >   - context:
    >       cluster: Tiberius
    >       user: admin
    >     name: default
    >   current-context: default
    >   kind: Config
    >   preferences: {}
    >   users:
    >   - name: admin
    >     user:
    >       client-certificate-data: LS0tLS1C........RS0tLS0t
    >       client-key-data: LS0tLS1C........tLS0tLQo=


# -----------------------------------------------------
# Use kubectl to get details of our cluster.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        cluster-info

    >   Kubernetes master is running at https://128.232.227.203:6443
    >   Heapster is running at https://128.232.227.203:6443/api/v1/namespaces/kube-system/services/heapster/proxy
    >   CoreDNS is running at https://128.232.227.203:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy



