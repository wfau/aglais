#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#


# -----------------------------------------------------
# -----------------------------------------------------
# Looking at what is running on the worker nodes for the current deployment.
#[fedora@stv-dev-worker-1]

    ps -ef | grep fedora

    >   ....
    >   fedora   16788 30309  0 Jan24 ?        00:00:00
    >       bash
    >           /tmp/hadoop-fedora/nm-local-dir/usercache/fedora/appcache/application_1578764470640_0071/container_1578764470640_0071_01_000002/default_container_executor.sh
    >   
    >   fedora   16790 16788  0 Jan24 ?        00:00:00
    >       /bin/bash -c
    >           /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java
    >               -server
    >               -Xmx13312m
    >               -Djava.io.tmpdir=/tmp/hadoop-fedora/nm-local-dir/usercache/fedora/appcache/application_1578764470640_0071/container_1578764470640_0071_01_000002/tmp
    >               -Dspark.driver.port=39809
    >               -Dspark.yarn.app.container.log.dir=/home/fedora/hadoop/logs/userlogs/application_1578764470640_0071/container_1578764470640_0071_01_000002
    >               -XX:OnOutOfMemoryError='kill %p'
    >               org.apache.spark.executor.CoarseGrainedExecutorBackend
    >               --driver-url spark://CoarseGrainedScheduler@stv-dev-zeppelin:39809
    >               --executor-id 1
    >               --hostname stv-dev-worker-1
    >               --cores 4
    >               --app-id application_1578764470640_0071
    >               --user-class-path file:/tmp/hadoop-fedora/nm-local-dir/usercache/fedora/appcache/application_1578764470640_0071/container_1578764470640_0071_01_000002/__app__.jar
    >               1>/home/fedora/hadoop/logs/userlogs/application_1578764470640_0071/container_1578764470640_0071_01_000002/stdout
    >               2>/home/fedora/hadoop/logs/userlogs/application_1578764470640_0071/container_1578764470640_0071_01_000002/stderr
    >   
    >   fedora   16802 16790  0 Jan24 ?        00:25:59
    >       /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java
    >           -server
    >           -Xmx13312m
    >           -Djava.io.tmpdir=/tmp/hadoop-fedora/nm-local-dir/usercache/fedora/appcache/application_1578764470640_0071/container_1578764470640_0071_01_000002/tmp
    >           -Dspark.driver.port=39809
    >           -Dspark.yarn.app.container.log.dir=/home/fedora/hadoop/logs/userlogs/application_1578764470640_0071/container_1578764470640_0071_01_000002
    >           -XX:OnOutOfMemoryError=kill %p
    >           org.apache.spark.executor.CoarseGrainedExecutorBackend
    >           --driver-url spark://CoarseGrainedScheduler@stv-dev-zeppelin:39809
    >           --executor-id 1
    >           --hostname stv-dev-worker-1
    >           --cores 4
    >           --app-id application_1578764470640_0071
    >           --user-class-path file:/tmp/hadoop-fedora/nm-local-dir/usercache/fedora/appcache/application_1578764470640_0071/container_1578764470640_0071_01_000002/__app__.jar
    >   
    >   fedora   20893 16802  0 10:42 ?        00:00:01
    >       python -m pyspark.daemon
    >   
    >   fedora   30183     1  0 Jan11 ?        00:37:32
    >       /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java
    >           -Dproc_datanode
    >           -Djava.net.preferIPv4Stack=true
    >           -Dhadoop.security.logger=ERROR,RFAS
    >           -Dyarn.log.dir=/home/fedora/hadoop/logs
    >           -Dyarn.log.file=hadoop-fedora-datanode-stv-dev-worker-1.novalocal.log
    >           -Dyarn.home.dir=/home/fedora/hadoop
    >           -Dyarn.root.logger=INFO,console
    >           -Djava.library.path=/home/fedora/hadoop/lib/native
    >           -Dhadoop.log.dir=/home/fedora/hadoop/logs
    >           -Dhadoop.log.file=hadoop-fedora-datanode-stv-dev-worker-1.novalocal.log
    >           -Dhadoop.home.dir=/home/fedora/hadoop
    >           -Dhadoop.id.str=fedora
    >           -Dhadoop.root.logger=INFO,RFA
    >           -Dhadoop.policy.file=hadoop-policy.xml
    >               org.apache.hadoop.hdfs.server.datanode.DataNode
    >   
    >   fedora   30309     1  0 Jan11 ?        01:02:17
    >       /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java
    >           -Dproc_nodemanager
    >           -Djava.net.preferIPv4Stack=true
    >           -Dyarn.log.dir=/home/fedora/hadoop/logs
    >           -Dyarn.log.file=hadoop-fedora-nodemanager-stv-dev-worker-1.novalocal.log
    >           -Dyarn.home.dir=/home/fedora/hadoop
    >           -Dyarn.root.logger=INFO,console
    >           -Djava.library.path=/home/fedora/hadoop/lib/native
    >           -Dhadoop.log.dir=/home/fedora/hadoop/logs
    >           -Dhadoop.log.file=hadoop-fedora-nodemanager-stv-dev-worker-1.novalocal.log
    >           -Dhadoop.home.dir=/home/fedora/hadoop
    >           -Dhadoop.id.str=fedora
    >           -Dhadoop.root.logger=INFO,RFA
    >           -Dhadoop.policy.file=hadoop-policy.xml
    >           -Dhadoop.security.logger=INFO,NullAppender
    >               org.apache.hadoop.yarn.server.nodemanager.NodeManager
    >   
    >   ....

    #
    # Two Java processes started on Jan11.

        org.apache.hadoop.hdfs.server.datanode.DataNode

        org.apache.hadoop.yarn.server.nodemanager.NodeManager

    #
    # One Python process started today.

        python -m pyspark.daemon

    #
    # From stv notes
    # 20191030-openstack-hdfs-yarn-cluster.txt

        ################################################
        ## Format HDFS Master Node on each node
        ################################################

        sudo mkdir /home/hadoop/
        sudo chown -R fedora:root /home/hadoop/
        source /home/fedora/hadoop/bin/hdfs namenode -format

        ################################################
        ## Firewall
        ################################################

	    systemctl start firewalld
	    firewall-cmd --add-port=9000/tcp --permanent
	    firewall-cmd --add-port=9000/udp --permanent
	    firewall-cmd --add-port=2377/tcp --permanent
	    firewall-cmd --add-port=2377/tcp --permanent
	    firewall-cmd --add-port=7946/tcp --permanent
	    firewall-cmd --add-port=4789/tcp --permanent
	    firewall-cmd --add-port=4789/udp --permanent
	    firewall-cmd --add-port=9000/tcp --permanent
	    firewall-cmd --add-port=9000/udp --permanent
	    firewall-cmd --add-port=9866/tcp --permanent
	    firewall-cmd --add-port=9866/udp --permanent
	    firewall-cmd --add-port=8030/tcp --permanent
	    firewall-cmd --add-port=8030/udp --permanent
	    firewall-cmd --add-port=8031/tcp --permanent
	    firewall-cmd --add-port=8031/udp --permanent
	    firewall-cmd --add-port=8032/tcp --permanent
	    firewall-cmd --add-port=8032/udp --permanent
	    firewall-cmd --add-port=8033/tcp --permanent
	    firewall-cmd --add-port=8033/udp --permanent
	    firewall-cmd --add-port=8042/tcp --permanent
	    firewall-cmd --add-port=8042/udp --permanent
	    firewall-cmd --add-port=8050/tcp --permanent
	    firewall-cmd --add-port=8050/udp --permanent
	    firewall-cmd --add-port=8080/tcp --permanent
	    firewall-cmd --add-port=8080/udp --permanent
        firewall-cmd --reload

        ################################################
        ## Start and Stop HDFS
        ################################################

        ## On Master Node:
        start-dfs.sh

        # stop-dfs.sh

        ## Create a directory
        hdfs dfs -mkdir /hadoop/

        ## LS directory
        hdfs dfs -ls /hadoop/

        ################################################
        ## Monitoring HDFS
        ################################################

        ## Get HDFS Report - on each node
        hdfs dfsadmin -report

        ################################################
        ## Start and Stop Yarn
        ################################################

        start-yarn.sh

        yarn node -list

        ################################################
        ## Submit a sample job
        ################################################

        cd /home/hadoop/

        hdfs dfs -mkdir /hadoop/books

        wget -O alice.txt https://www.gutenberg.org/files/11/11-0.txt
        wget -O holmes.txt https://www.gutenberg.org/files/1661/1661-0.txt
        wget -O frankenstein.txt https://www.gutenberg.org/files/84/84-0.txt

        hdfs dfs -put alice.txt holmes.txt frankenstein.txt /hadoop/books

        yarn jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount "/hadoop/books/*" output

    #
    # From stv notes
    # 20191101-01-openstack-spark-yarn-cluster.txt

        ########################################################
        ## Setup Spark Configuration
        ########################################################

        mv $SPARK_HOME/conf/spark-defaults.conf.template $SPARK_HOME/conf/spark-defaults.conf


        ## Update Spark.Master parameters in config

        cat <<EOF >> "$SPARK_HOME/conf/spark-defaults.conf"

        spark.master                     yarn
        spark.driver.memory              512m
        spark.yarn.am.memory		512m
        spark.executor.memory          512m
        spark.eventLog.enabled  true
        spark.eventLog.dir hdfs://master:9000/spark-log

        EOF

        ** Settings from live system are :
        cat $SPARK_HOME/conf/spark-defaults.conf
            ....
            spark.master            yarn
            spark.driver.memory     13g
            spark.yarn.am.memory    13g
            spark.executor.memory   13g
            spark.eventLog.enabled  true
            spark.eventLog.dir hdfs://stv-dev-master:9000/spark-log

        ## Create the log directory in HDFS:

        hdfs dfs -mkdir /spark-log

        ########################################################
        ## Submit an Example Spark application
        ########################################################

        spark-submit
            --deploy-mode client
            --class org.apache.spark.examples.SparkPi
            $SPARK_HOME/examples/jars/spark-examples_2.11-2.2.0.jar 10

            ....
            ....

        ########################################################
        ## Using the Spark Shell
        ########################################################


        scala> var input = spark.read.textFile("/hadoop/books/alice.txt")
        input: org.apache.spark.sql.Dataset[String] = [value: string]

        scala> input.filter(line => line.length()>0).count()
        res0: Long = 2791

    #
    # From stv notes
    # 20191113-openstack-deployment.txt

        #-------------------------------------------------------------------------
        ## Format HDFS Master Node on each node
        #-------------------------------------------------------------------------

        sudo mkdir /home/hadoop/
        sudo chown -R fedora:root /home/hadoop/
        source /home/fedora/hadoop/bin/hdfs namenode -format

        ** /home/hadoop/ is not related to HDFS
        ** /home/hadoop/ appears to contaun plain data files

            ls -al /home/hadoop/ascii/gaia/gdr1/gaia/data
                -rw-r-----. 1 fedora root 44836200 Dec 19 10:00 part-00000-eb15db47-f0d7-474d-a96b-017df830999a.txt
                -rw-r-----. 1 fedora root 44717339 Dec 19 10:00 part-00001-eb15db47-f0d7-474d-a96b-017df830999a.txt
                -rw-r-----. 1 fedora root 44672261 Dec 19 10:00 part-00002-eb15db47-f0d7-474d-a96b-017df830999a.txt

        #-------------------------------------------------------------------------
        ## Start and Stop HDFS services
        #-------------------------------------------------------------------------

        ## On Master Node:
        start-all.sh

        ## Create a directory
        hdfs dfs -mkdir /hadoop/

        ## LS directory
        hdfs dfs -ls /hadoop/

        yarn jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount "/hadoop/books/*" output
        ## Job Completed Successfully
        ## Check Results

        hdfs dfs -ls output
        hdfs dfs -cat output/part-r-00000 | less

        #-------------------------------------------------------------------------
        # Spark
        #-------------------------------------------------------------------------

        ## Setup Spark on Existing yarn & hdfs cluster

        ## Download and Install Spark Binaries
        ## From Node Master

        cd /home/fedora
        wget https://www-eu.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz
        tar -xvf spark-2.4.4-bin-hadoop2.7.tgz
        mv spark-2.4.4-bin-hadoop2.7.tgz spark

        ## Setup Default Spark Config

        mv /home/fedora/spark/conf/spark-defaults.conf.template /home/fedora/spark/conf/spark-defaults.conf

        cat <<EOF >> "/home/fedora/spark/conf/spark-defaults.conf"
        spark.master                     yarn
        spark.driver.memory              80g
        spark.yarn.am.memory            80g
        spark.executor.memory          80g
        spark.eventLog.enabled  true
        spark.eventLog.dir hdfs://${stv-aglais-bastion-ip:?}:9000/spark-log
        EOF

        ## Create the log directory in HDFS:
        hdfs dfs -mkdir /spark-log

        #-------------------------------------------------------------------------
        # Setup Environment variables on each node
        #-------------------------------------------------------------------------

        # For each Worker node & Master node

        cat > "${HOME:?}/.profile" << EOF
        PATH=/home/fedora/spark/bin:/home/fedora/.local/bin:/home/fedora/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/bin:/sbin:/home/fedora/hadoop/bin:/home/fedora/hadoop/sbin
        EOF

        cat <<EOF >> "${HOME:?}/.bashrc"
        export HADOOP_HOME=/home/fedora/hadoop
        export PATH=/home/fedora/.local/bin:/home/fedora/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/bin:/sbin:/home/fedora/hadoop/bin:/home/fedora/hadoop/sbin:/home/fedora/spark/bin
        export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre/
        export HADOOP_CONF_DIR=/home/fedora/hadoop/etc/hadoop
        export SPARK_HOME=/home/fedora/spark
        export LD_LIBRARY_PATH=/home/fedora/hadoop/lib/native
        EOF

    #
    # From stv notes
    # 20191120-openstack-deployment.txt

        #-------------------------------------------------------------------------
        ## Install Python3 and set it as default for each worker node
        #-------------------------------------------------------------------------

        ## for each worker node

        sudo yum install python3
        sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 10

        #-------------------------------------------------------------------------
        ## Start and Stop HDFS services
        #-------------------------------------------------------------------------

        ## On Master Node: (ssh stv-aglais-master)

        start-all.sh

        ## Create a directory
        hdfs dfs -mkdir /hadoop/
        hdfs dfs -mkdir /hadoop/books/

        ## Run test job

        cd /home/hadoop
        wget -O alice.txt https://www.gutenberg.org/files/11/11-0.txt
        wget -O holmes.txt https://www.gutenberg.org/files/1661/1661-0.txt
        wget -O frankenstein.txt https://www.gutenberg.org/files/84/84-0.txt

        hdfs dfs -put alice.txt holmes.txt frankenstein.txt books

        ## LS directory
        hdfs dfs -ls /hadoop/books

        yarn jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount "/hadoop/books/*" output
        ## Job Completed Successfully
        ## Check Results

        hdfs dfs -ls output
        hdfs dfs -cat output/part-r-00000 | less

        #-------------------------------------------------------------------------
        # Spark
        #-------------------------------------------------------------------------

        ## Setup Spark on Existing yarn & hdfs cluster

        ## Download and Install Spark Binaries
        ## On Master Node: (ssh stv-aglais-master)

        cd /home/fedora
        wget https://www-eu.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz
        tar -xvf spark-2.4.4-bin-hadoop2.7.tgz
        mv spark-2.4.4-bin-hadoop2.7.tgz spark


        ## Setup Default Spark Config

        mv /home/fedora/spark/conf/spark-defaults.conf.template /home/fedora/spark/conf/spark-defaults.conf

        cat <<EOF >> "/home/fedora/spark/conf/spark-defaults.conf"
        spark.master                     yarn
        spark.driver.memory              50g
        spark.yarn.am.memory            50g
        spark.executor.memory          50g
        spark.eventLog.enabled  true
        spark.eventLog.dir hdfs://${stv-aglais-master-ip:?}:9000/spark-log
        EOF

        ## Create the log directory in HDFS:
        hdfs dfs -mkdir /spark-log

        ./bin/spark-submit --class org.apache.spark.examples.SparkPi     --master yarn-client     --num-executors 1     --driver-memory 512m     --executor-memory 512m     --executor-cores 1     examples/jars/spark-examples*.jar 10

            Exception in thread "main" java.lang.IllegalArgumentException: Required AM memory (46080+4608 MB) is above the max threshold (50000 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'.


        ## Change Spark Configuration to

        spark.master                     yarn
        spark.driver.memory              40g
        spark.yarn.am.memory            40g
        spark.executor.memory          40g
        spark.eventLog.enabled  true
        spark.eventLog.dir hdfs://${stv-aglais-master-ip:?}:9000/spark-log

        2019-11-20 19:04:39,482 INFO scheduler.DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 0.706995 s
        Pi is roughly 3.1443231443231445

    #
    # From stv notes
    # 20191227-worker-config.txt

        Worker nodes only need Hadoop binaries, not Spark

        [fedora@stv-dev-master ~]$

            ls /home/fedora/spark/

                bin  conf  data  examples  jars  kubernetes  LICENSE  licenses  NOTICE  python  R  README.md  RELEASE  sbin  yarn

        [fedora@stv-dev-worker-1 ~]$

            ls /home/fedora/spark

                ls: cannot access '/home/fedora/spark': No such file or directory



