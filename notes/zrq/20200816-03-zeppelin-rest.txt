#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#


    https://zeppelin.apache.org/docs/0.7.0/rest-api/rest-notebook.html


    http://[zeppelin-server]:[zeppelin-port]/api/notebook
    http://zeppelin.metagrid.xyz/api/notebook


    Simple answer - yes, REST API works.
    Can we transform our notes and tests to use the REST API.


    GET http://zeppelin.metagrid.xyz/api/notebook/2FHNVCRNQ

    >   {
    >     "status": "OK",
    >     "message": "",
    >     "body": {
    >       "paragraphs": [{
    >         "text": "%spark.conf\n\nPYSPARK_PYTHON \"python2\"\nspark.pyspark.python \"python2\"\n\nspark.driver.cores       8\nspark.driver.memory      8g\nspark.executor.cores     8\nspark.executor.memory    8g\nspark.executor.instances 4\n",
    >         "user": "anonymous",
    >         "dateUpdated": "Aug 16, 2020 4:37:03 AM",
    >         "config": {
    >           "colWidth": 12.0,
    >           "fontSize": 9.0,
    >           "enabled": true,
    >           "results": {},
    >           "editorSetting": {
    >             "language": "text",
    >             "editOnDblClick": false,
    >             "completionKey": "TAB",
    >             "completionSupport": true
    >           },
    >           "editorMode": "ace/mode/text"
    >         },
    >         "settings": {
    >           "params": {},
    >           "forms": {}
    >         },
    >         "results": {
    >           "code": "SUCCESS",
    >           "msg": []
    >         },
    >         "apps": [],
    >         "runtimeInfos": {},
    >         "progressUpdateIntervalMs": 500,
    >         "jobName": "paragraph_1597375100057_18651878",
    >         "id": "paragraph_1597375100057_18651878",
    >         "dateCreated": "Aug 14, 2020 3:18:20 AM",
    >         "dateStarted": "Aug 16, 2020 5:24:25 AM",
    >         "dateFinished": "Aug 16, 2020 5:24:25 AM",
    >         "status": "FINISHED"
    >       }, {
    >         "text": "%spark.pyspark\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.endpoint\", \"cumulus.openstack.hpc.cam.ac.uk:6780/\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.path.style.access\", \"true\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.list.version\", \"2\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.bucket.probe\", \"0\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.aws.credentials.provider\",\n    \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\"\n    )",
    >         "user": "anonymous",
    >         "dateUpdated": "Aug 16, 2020 4:37:03 AM",
    >         "config": {
    >           "colWidth": 12.0,
    >           "fontSize": 9.0,
    >           "enabled": true,
    >           "results": {},
    >           "editorSetting": {
    >             "language": "python",
    >             "editOnDblClick": false,
    >             "completionKey": "TAB",
    >             "completionSupport": true
    >           },
    >           "editorMode": "ace/mode/python"
    >         },
    >         "settings": {
    >           "params": {},
    >           "forms": {}
    >         },
    >         "results": {
    >           "code": "SUCCESS",
    >           "msg": []
    >         },
    >         "apps": [],
    >         "runtimeInfos": {},
    >         "progressUpdateIntervalMs": 500,
    >         "jobName": "paragraph_1597375189970_963315047",
    >         "id": "paragraph_1597375189970_963315047",
    >         "dateCreated": "Aug 14, 2020 3:19:49 AM",
    >         "dateStarted": "Aug 16, 2020 5:24:26 AM",
    >         "dateFinished": "Aug 16, 2020 5:24:26 AM",
    >         "status": "FINISHED"
    >       }, {
    >         "text": "%spark.pyspark\n\ndf = sqlContext.read.parquet(\n    \"s3a://albert/\"\n    )\n\nprint \"DF count: \", df.count()\nprint \"DF partitions: \", df.rdd.getNumPartitions()",
    >         "user": "anonymous",
    >         "dateUpdated": "Aug 16, 2020 4:37:27 AM",
    >         "config": {
    >           "colWidth": 12.0,
    >           "fontSize": 9.0,
    >           "enabled": true,
    >           "results": {},
    >           "editorSetting": {
    >             "language": "python",
    >             "editOnDblClick": false,
    >             "completionKey": "TAB",
    >             "completionSupport": true
    >           },
    >           "editorMode": "ace/mode/python"
    >         },
    >         "settings": {
    >           "params": {},
    >           "forms": {}
    >         },
    >         "results": {
    >           "code": "SUCCESS",
    >           "msg": [{
    >             "type": "TEXT",
    >             "data": "DF count:  621626\nDF partitions:  32\n"
    >           }]
    >         },
    >         "apps": [],
    >         "runtimeInfos": {
    >           "jobUrl": {
    >             "propertyName": "jobUrl",
    >             "label": "SPARK JOB",
    >             "tooltip": "View in Spark web UI",
    >             "group": "spark",
    >             "values": [{
    >               "jobUrl": "//4040-spark-tqzfmb.local.zeppelin-project.org:8080/jobs/job?id=8"
    >             }, {
    >               "jobUrl": "//4040-spark-tqzfmb.local.zeppelin-project.org:8080/jobs/job?id=9"
    >             }],
    >             "interpreterSettingId": "spark"
    >           }
    >         },
    >         "progressUpdateIntervalMs": 500,
    >         "jobName": "paragraph_1597418456972_485773191",
    >         "id": "paragraph_1597418456972_485773191",
    >         "dateCreated": "Aug 14, 2020 3:20:56 PM",
    >         "dateStarted": "Aug 16, 2020 5:24:26 AM",
    >         "dateFinished": "Aug 16, 2020 5:24:27 AM",
    >         "status": "FINISHED"
    >       }, {
    >         "text": "%spark.pyspark\n\ndf = sqlContext.read.parquet(\n    \"s3a://gaia-dr2-parquet-0-32/\"\n    )\n\nprint \"DF count: \", df.count()\nprint \"DF partitions: \", df.rdd.getNumPartitions()",
    >         "user": "anonymous",
    >         "dateUpdated": "Aug 16, 2020 4:37:41 AM",
    >         "config": {
    >           "colWidth": 12.0,
    >           "fontSize": 9.0,
    >           "enabled": true,
    >           "results": {},
    >           "editorSetting": {
    >             "language": "python",
    >             "editOnDblClick": false,
    >             "completionKey": "TAB",
    >             "completionSupport": true
    >           },
    >           "editorMode": "ace/mode/python"
    >         },
    >         "settings": {
    >           "params": {},
    >           "forms": {}
    >         },
    >         "results": {
    >           "code": "SUCCESS",
    >           "msg": [{
    >             "type": "TEXT",
    >             "data": "DF count:  52659345\nDF partitions:  186\n"
    >           }]
    >         },
    >         "apps": [],
    >         "runtimeInfos": {
    >           "jobUrl": {
    >             "propertyName": "jobUrl",
    >             "label": "SPARK JOB",
    >             "tooltip": "View in Spark web UI",
    >             "group": "spark",
    >             "values": [{
    >               "jobUrl": "//4040-spark-tqzfmb.local.zeppelin-project.org:8080/jobs/job?id=10"
    >             }, {
    >               "jobUrl": "//4040-spark-tqzfmb.local.zeppelin-project.org:8080/jobs/job?id=11"
    >             }],
    >             "interpreterSettingId": "spark"
    >           }
    >         },
    >         "progressUpdateIntervalMs": 500,
    >         "jobName": "paragraph_1597375159233_2006400480",
    >         "id": "paragraph_1597375159233_2006400480",
    >         "dateCreated": "Aug 14, 2020 3:19:19 AM",
    >         "dateStarted": "Aug 16, 2020 5:24:27 AM",
    >         "dateFinished": "Aug 16, 2020 5:24:28 AM",
    >         "status": "FINISHED"
    >       }, {
    >         "text": "%spark.pyspark\n\ndf = sqlContext.read.parquet(\n    \"s3a://gaia-dr2-parquet-0-16/\"\n    )\n\nprint \"DF count: \", df.count()\nprint \"DF partitions: \", df.rdd.getNumPartitions()",
    >         "user": "anonymous",
    >         "dateUpdated": "Aug 16, 2020 4:37:43 AM",
    >         "config": {
    >           "colWidth": 12.0,
    >           "fontSize": 9.0,
    >           "enabled": true,
    >           "results": {},
    >           "editorSetting": {
    >             "language": "python",
    >             "editOnDblClick": false,
    >             "completionKey": "TAB",
    >             "completionSupport": true
    >           },
    >           "editorMode": "ace/mode/python"
    >         },
    >         "settings": {
    >           "params": {},
    >           "forms": {}
    >         },
    >         "results": {
    >           "code": "SUCCESS",
    >           "msg": [{
    >             "type": "TEXT",
    >             "data": "DF count:  105699833\nDF partitions:  374\n"
    >           }]
    >         },
    >         "apps": [],
    >         "runtimeInfos": {
    >           "jobUrl": {
    >             "propertyName": "jobUrl",
    >             "label": "SPARK JOB",
    >             "tooltip": "View in Spark web UI",
    >             "group": "spark",
    >             "values": [{
    >               "jobUrl": "//4040-spark-tqzfmb.local.zeppelin-project.org:8080/jobs/job?id=12"
    >             }, {
    >               "jobUrl": "//4040-spark-tqzfmb.local.zeppelin-project.org:8080/jobs/job?id=13"
    >             }],
    >             "interpreterSettingId": "spark"
    >           }
    >         },
    >         "progressUpdateIntervalMs": 500,
    >         "jobName": "paragraph_1597375708909_1448755122",
    >         "id": "paragraph_1597375708909_1448755122",
    >         "dateCreated": "Aug 14, 2020 3:28:28 AM",
    >         "dateStarted": "Aug 16, 2020 5:24:28 AM",
    >         "dateFinished": "Aug 16, 2020 5:24:30 AM",
    >         "status": "FINISHED"
    >       }, {
    >         "text": "%spark.pyspark\n\ndf = sqlContext.read.parquet(\n    \"s3a://gaia-dr2-parquet-0-8/\"\n    )\n\nprint \"DF count: \", df.count()\nprint \"DF partitions: \", df.rdd.getNumPartitions()",
    >         "user": "anonymous",
    >         "dateUpdated": "Aug 16, 2020 4:37:45 AM",
    >         "config": {
    >           "colWidth": 12.0,
    >           "fontSize": 9.0,
    >           "enabled": true,
    >           "results": {},
    >           "editorSetting": {
    >             "language": "python",
    >             "editOnDblClick": false,
    >             "completionKey": "TAB",
    >             "completionSupport": true
    >           },
    >           "editorMode": "ace/mode/python"
    >         },
    >         "settings": {
    >           "params": {},
    >           "forms": {}
    >         },
    >         "results": {
    >           "code": "SUCCESS",
    >           "msg": [{
    >             "type": "TEXT",
    >             "data": "DF count:  211537973\nDF partitions:  749\n"
    >           }]
    >         },
    >         "apps": [],
    >         "runtimeInfos": {
    >           "jobUrl": {
    >             "propertyName": "jobUrl",
    >             "label": "SPARK JOB",
    >             "tooltip": "View in Spark web UI",
    >             "group": "spark",
    >             "values": [{
    >               "jobUrl": "//4040-spark-tqzfmb.local.zeppelin-project.org:8080/jobs/job?id=14"
    >             }, {
    >               "jobUrl": "//4040-spark-tqzfmb.local.zeppelin-project.org:8080/jobs/job?id=15"
    >             }],
    >             "interpreterSettingId": "spark"
    >           }
    >         },
    >         "progressUpdateIntervalMs": 500,
    >         "jobName": "paragraph_1597413659420_1753327719",
    >         "id": "paragraph_1597413659420_1753327719",
    >         "dateCreated": "Aug 14, 2020 2:00:59 PM",
    >         "dateStarted": "Aug 16, 2020 5:24:30 AM",
    >         "dateFinished": "Aug 16, 2020 5:24:32 AM",
    >         "status": "FINISHED"
    >       }, {
    >         "text": "%spark.pyspark\n\ndf = sqlContext.read.parquet(\n    \"s3a://gaia-dr2-parquet-0-4/\"\n    )\n\nprint \"DF count: \", df.count()\nprint \"DF partitions: \", df.rdd.getNumPartitions()",
    >         "user": "anonymous",
    >         "dateUpdated": "Aug 16, 2020 4:37:48 AM",
    >         "config": {
    >           "colWidth": 12.0,
    >           "fontSize": 9.0,
    >           "enabled": true,
    >           "results": {},
    >           "editorSetting": {
    >             "language": "python",
    >             "editOnDblClick": false,
    >             "completionKey": "TAB",
    >             "completionSupport": true
    >           },
    >           "editorMode": "ace/mode/python"
    >         },
    >         "settings": {
    >           "params": {},
    >           "forms": {}
    >         },
    >         "results": {
    >           "code": "ERROR",
    >           "msg": [{
    >             "type": "TEXT",
    >             "data": "Py4JJavaError: An error occurred while calling o174.parquet.\n: java.lang.IllegalStateException: Connection pool shut down\n\tat com.amazonaws.thirdparty.apache.http.util.Asserts.check(Asserts.java:34)\n\tat com.amazonaws.thirdparty.apache.http.pool.AbstractConnPool.lease(AbstractConnPool.java:191)\n\tat com.amazonaws.thirdparty.apache.http.impl.conn.PoolingHttpClientConnectionManager.requestConnection(PoolingHttpClientConnectionManager.java:268)\n\tat sun.reflect.GeneratedMethodAccessor120.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat com.amazonaws.http.conn.ClientConnectionManagerFactory$Handler.invoke(ClientConnectionManagerFactory.java:76)\n\tat com.amazonaws.http.conn.$Proxy28.requestConnection(Unknown Source)\n\tat com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:176)\n\tat com.amazonaws.thirdparty.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)\n\tat com.amazonaws.thirdparty.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\n\tat com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\n\tat com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\n\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1330)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5062)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5008)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5002)\n\tat com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:941)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$5(S3AFileSystem.java:1276)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:1269)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2237)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)\n\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1700)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.isDirectory(S3AFileSystem.java:2995)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:737)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n(<class \'py4j.protocol.Py4JJavaError\'>, Py4JJavaError(u\'An error occurred while calling o174.parquet.\\n\', JavaObject id=o177), <traceback object at 0x7fa7bd8b8320>)"
    >           }]
    >         },
    >         "apps": [],
    >         "runtimeInfos": {},
    >         "progressUpdateIntervalMs": 500,
    >         "jobName": "paragraph_1597413672091_148192074",
    >         "id": "paragraph_1597413672091_148192074",
    >         "dateCreated": "Aug 14, 2020 2:01:12 PM",
    >         "dateStarted": "Aug 16, 2020 5:24:33 AM",
    >         "dateFinished": "Aug 16, 2020 5:24:33 AM",
    >         "status": "ERROR"
    >       }, {
    >         "text": "%spark.pyspark\n",
    >         "user": "anonymous",
    >         "dateUpdated": "Aug 16, 2020 4:53:53 AM",
    >         "config": {
    >           "colWidth": 12.0,
    >           "fontSize": 9.0,
    >           "enabled": true,
    >           "results": {},
    >           "editorSetting": {
    >             "language": "python",
    >             "editOnDblClick": false,
    >             "completionKey": "TAB",
    >             "completionSupport": true
    >           },
    >           "editorMode": "ace/mode/python"
    >         },
    >         "settings": {
    >           "params": {},
    >           "forms": {}
    >         },
    >         "apps": [],
    >         "runtimeInfos": {},
    >         "progressUpdateIntervalMs": 500,
    >         "jobName": "paragraph_1597551401080_298607740",
    >         "id": "paragraph_1597551401080_298607740",
    >         "dateCreated": "Aug 16, 2020 4:16:41 AM",
    >         "status": "FINISHED"
    >       }],
    >       "name": "Untitled Note 1",
    >       "id": "2FHNVCRNQ",
    >       "defaultInterpreterGroup": "spark",
    >       "version": "0.9.0-SNAPSHOT",
    >       "noteParams": {},
    >       "noteForms": {},
    >       "angularObjects": {},
    >       "config": {
    >         "isZeppelinNotebookCronEnable": false
    >       },
    >       "info": {
    >         "isRunning": false,
    >         "inIsolatedMode": false
    >       },
    >       "path": "/Untitled Note 1"
    >     }
    >   }--END--
    >   
