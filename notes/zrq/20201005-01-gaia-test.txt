#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    Test - add the PersistentVolumeClaim into 100-interpreter-spec.yaml template

    See :
        experiments/zrq/zeppelin/k8s/interpreter/100-interpreter-spec.yaml
        notes/zrq/20200826-01-user-accounts.txt

    Diverted by errors with the cluster load balancers.
    Decided to delete the old cluster and create a new one.


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --hostname kubernator \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --env "clustername=${CLUSTER_NAME:?}" \
        --volume "${HOME}/clouds.yaml:/etc/openstack/clouds.yaml:z" \
        --volume "${AGLAIS_CODE}/experiments/zrq/helm:/helm:z" \
        --volume "${AGLAIS_CODE}/experiments/zrq/kubernetes:/kubernetes:z" \
        atolmis/openstack-client \
        bash


# -----------------------------------------------------
# Get the connection details for our cluster.
#[user@kubernator]

    mkdir -p "${HOME}/.kube"
    openstack \
        --os-cloud "${cloudname:?}" \
        coe cluster config \
            "${clustername:?}" \
                --force \
                --dir "${HOME}/.kube"


    kubectl \
        cluster-info

    >   Unable to connect to the server: unexpected EOF


# -----------------------------------------------------
# Enable verbose debug on kubectl.
# https://kubernetes.io/docs/reference/kubectl/cheatsheet/#kubectl-output-verbosity-and-debugging
#[user@kubernator]

    kubectl \
        --v 6 \
        cluster-info

    >   I1005 12:11:19.742154     123 loader.go:359] Config loaded from file:  /root/.kube/config
    >   I1005 12:11:19.834836     123 round_trippers.go:471] GET https://128.232.227.224:6443/apis/metrics.k8s.io/v1beta1?timeout=32s 503 Service Unavailable in 90 milliseconds
    >   I1005 12:11:19.912837     123 request.go:1150] body was not decodable (unable to check for Status): couldn't get version/kind; json parse error: json: cannot unmarshal string into Go value of type struct { APIVersion string "json:\"apiVersion,omitempty\""; Kind string "json:\"kind,omitempty\"" }
    >   I1005 12:11:19.912867     123 cached_discovery.go:78] skipped caching discovery info due to the server is currently unable to handle the request
    >   I1005 12:11:19.934482     123 round_trippers.go:471] GET https://128.232.227.224:6443/apis/metrics.k8s.io/v1beta1?timeout=32s 503 Service Unavailable in 20 milliseconds
    >   I1005 12:11:19.965707     123 request.go:1150] body was not decodable (unable to check for Status): couldn't get version/kind; json parse error: json: cannot unmarshal string into Go value of type struct { APIVersion string "json:\"apiVersion,omitempty\""; Kind string "json:\"kind,omitempty\"" }
    >   I1005 12:11:19.965754     123 cached_discovery.go:78] skipped caching discovery info due to the server is currently unable to handle the request
    >   I1005 12:11:19.965835     123 shortcut.go:89] Error loading discovery information: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
    >   I1005 12:11:19.988026     123 round_trippers.go:471] GET https://128.232.227.224:6443/apis/metrics.k8s.io/v1beta1?timeout=32s 503 Service Unavailable in 20 milliseconds
    >   I1005 12:11:20.018796     123 request.go:1150] body was not decodable (unable to check for Status): couldn't get version/kind; json parse error: json: cannot unmarshal string into Go value of type struct { APIVersion string "json:\"apiVersion,omitempty\""; Kind string "json:\"kind,omitempty\"" }
    >   I1005 12:11:20.018833     123 cached_discovery.go:78] skipped caching discovery info due to the server is currently unable to handle the request
    >   I1005 12:12:10.041237     123 round_trippers.go:471] GET https://128.232.227.224:6443/api/v1/namespaces/kube-system/services?labelSelector=kubernetes.io%2Fcluster-service%3Dtrue  in 50020 milliseconds
    >   
    >   To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
    >   I1005 12:12:10.041585     123 helpers.go:217] Connection error: Get https://128.232.227.224:6443/api/v1/namespaces/kube-system/services?labelSelector=kubernetes.io%2Fcluster-service%3Dtrue: unexpected EOF
    >   F1005 12:12:10.041676     123 helpers.go:114] Unable to connect to the server: unexpected EOF

    #
    # Contacted John on Slack to see if anything has changed on the Openstack platform.
    #


# -----------------------------------------------------
# -----------------------------------------------------
# Check our Dashboard Ingress.
#[user@desktop]

    host valeria.metagrid.xyz

    >   valeria.metagrid.xyz is an alias for aglais-001.metagrid.xyz.
    >   aglais-001.metagrid.xyz has address 128.232.227.128


    curl -k --head 'https://valeria.metagrid.xyz/'

    >   HTTP/2 502
    >   ....
    >   ....

# -----------------------------------------------------
# Check our Zeppelin Ingress.
#[user@desktop]

    host zeppelin.metagrid.xyz

    >   zeppelin.metagrid.xyz is an alias for aglais-001.metagrid.xyz.
    >   aglais-001.metagrid.xyz has address 128.232.227.128


    curl -k --head 'https://zeppelin.metagrid.xyz/'

    >   HTTP/2 200
    >   server: nginx/1.19.2
    >   ....
    >   ....


# -----------------------------------------------------
# Try using Zeppelin.
#[user@desktop]

    firefox --new-window "http://zeppelin.metagrid.xyz/" &

    # Zeppelin appeared to be working, but restarting the Spark interpreter returned this error :
    #     Operation: [delete] for kind: [Pod] with name: [null] in namespace: [default] failed.


# -----------------------------------------------------
# -----------------------------------------------------
# Check the Openstack details for our cluster.
#[user@kubernator]

    openstack \
        --os-cloud "${cloudname:?}" \
        coe cluster list

    >   +--------------------------------------+-------------------+------------------+------------+--------------+-----------------+---------------+
    >   | uuid                                 | name              | keypair          | node_count | master_count | status          | health_status |
    >   +--------------------------------------+-------------------+------------------+------------+--------------+-----------------+---------------+
    >   | 08bb2885-77d7-4b69-9141-92b41f3580c9 | Tiberius-20200923 | zrq-gaia-keypair |          4 |            1 | CREATE_COMPLETE | UNHEALTHY     |
    >   +--------------------------------------+-------------------+------------------+------------+--------------+-----------------+---------------+


    clusterid=$(
        openstack \
            --os-cloud "${cloudname:?}" \
            coe cluster list \
                --format json \
        | jq -r '.[0].uuid'
        )

    openstack \
        --os-cloud "${cloudname:?}" \
        coe cluster show \
            "${clusterid:?}"

    >   +----------------------+---------------------------------------------------------------+
    >   | Field                | Value                                                         |
    >   +----------------------+---------------------------------------------------------------+
    >   | status               | CREATE_COMPLETE                                               |
    >   | health_status        | UNHEALTHY                                                     |
    >   | cluster_template_id  | 40963ffb-4439-49f8-8e80-f511fc11c4a9                          |
    >   | node_addresses       | ['10.0.0.148', '10.0.0.197', '10.0.0.52', '10.0.0.41']        |
    >   | uuid                 | 08bb2885-77d7-4b69-9141-92b41f3580c9                          |
    >   | stack_id             | 8bcaadbf-cd21-45b7-9a32-673c85a3f0dc                          |
    >   | status_reason        | None                                                          |
    >   | created_at           | 2020-09-23T13:48:57+00:00                                     |
    >   | updated_at           | 2020-10-03T17:04:00+00:00                                     |
    >   | coe_version          | v1.17.2                                                       |
    >   | labels               | {'auto_healing_controller': 'magnum-auto-healer', '... }      |
    >   | labels_overridden    |                                                               |
    >   | labels_skipped       |                                                               |
    >   | labels_added         |                                                               |
    >   | faults               |                                                               |
    >   | keypair              | zrq-gaia-keypair                                              |
    >   | api_address          | https://128.232.227.224:6443                                  |
    >   | master_addresses     | ['10.0.0.242']                                                |
    >   | create_timeout       | 60                                                            |
    >   | node_count           | 4                                                             |
    >   | discovery_url        | https://discovery.etcd.io/fea39662b41d94cf72509df8b5615c60    |
    >   | master_count         | 1                                                             |
    >   | container_version    | 1.12.6                                                        |
    >   | name                 | Tiberius-20200923                                             |
    >   | master_flavor_id     | 406a17e0-afd0-47d3-a6ad-8b19198bdd97                          |
    >   | flavor_id            | 996c1c8c-c934-411c-9631-b74eb2829631                          |
    >   | health_status_reason | {'api': '[+]ping ok\n[+]log ok\n[-]etcd failed: .... }        |
    >   | project_id           | 21b4ae3a2ea44bc5a9c14005ed2963af                              |
    >   +----------------------+---------------------------------------------------------------+


    openstack \
        --os-cloud "${cloudname:?}" \
        coe cluster show \
            --format json \
            "${clusterid:?}" \
    | jq '.'

    >   {
    >     "status": "CREATE_COMPLETE",
    >     "health_status": "UNHEALTHY",
    >     "cluster_template_id": "40963ffb-4439-49f8-8e80-f511fc11c4a9",
    >     "node_addresses": [
    >       "10.0.0.148",
    >       "10.0.0.197",
    >       "10.0.0.52",
    >       "10.0.0.41"
    >     ],
    >     "uuid": "08bb2885-77d7-4b69-9141-92b41f3580c9",
    >     "stack_id": "8bcaadbf-cd21-45b7-9a32-673c85a3f0dc",
    >     "status_reason": null,
    >     "created_at": "2020-09-23T13:48:57+00:00",
    >     "updated_at": "2020-10-03T17:04:00+00:00",
    >     "coe_version": "v1.17.2",
    >     "labels": {
    >       "auto_healing_controller": "magnum-auto-healer",
    >       "max_node_count": "4",
    >       "cloud_provider_tag": "v1.17.0",
    >       "etcd_tag": "3.3.17",
    >       "monitoring_enabled": "true",
    >       "tiller_enabled": "true",
    >       "autoscaler_tag": "v1.15.2",
    >       "master_lb_floating_ip_enabled": "true",
    >       "auto_scaling_enabled": "true",
    >       "tiller_tag": "v2.16.1",
    >       "use_podman": "true",
    >       "auto_healing_enabled": "true",
    >       "heat_container_agent_tag": "train-stable-1",
    >       "kube_tag": "v1.17.2",
    >       "min_node_count": "1"
    >     },
    >     "labels_overridden": "",
    >     "labels_skipped": "",
    >     "labels_added": "",
    >     "faults": "",
    >     "keypair": "zrq-gaia-keypair",
    >     "api_address": "https://128.232.227.224:6443",
    >     "master_addresses": [
    >       "10.0.0.242"
    >     ],
    >     "create_timeout": 60,
    >     "node_count": 4,
    >     "discovery_url": "https://discovery.etcd.io/fea39662b41d94cf72509df8b5615c60",
    >     "master_count": 1,
    >     "container_version": "1.12.6",
    >     "name": "Tiberius-20200923",
    >     "master_flavor_id": "406a17e0-afd0-47d3-a6ad-8b19198bdd97",
    >     "flavor_id": "996c1c8c-c934-411c-9631-b74eb2829631",
    >     "health_status_reason": {
    >       "api": "[+]ping ok\n[+]log ok\n[-]etcd failed: ..."
    >     },
    >     "project_id": "21b4ae3a2ea44bc5a9c14005ed2963af"
    >   }


    openstack \
        --os-cloud "${cloudname:?}" \
        coe cluster show \
            --format json \
            "${clusterid:?}" \
    | jq -r '.health_status_reason.api' \
    | sed 's/\\n/\n/g'

    >   [+]ping ok
    >   [+]log ok
    >   [-]etcd failed: reason withheld
    >   [+]poststarthook/start-kube-apiserver-admission-initializer ok
    >   [+]poststarthook/generic-apiserver-start-informers ok
    >   [+]poststarthook/start-apiextensions-informers ok
    >   [+]poststarthook/start-apiextensions-controllers ok
    >   [+]poststarthook/crd-informer-synced ok
    >   [+]poststarthook/bootstrap-controller ok
    >   [+]poststarthook/rbac/bootstrap-roles ok
    >   [+]poststarthook/scheduling/bootstrap-system-priority-classes ok
    >   [+]poststarthook/start-cluster-authentication-info-controller ok
    >   [+]poststarthook/start-kube-aggregator-informers ok
    >   [+]poststarthook/apiservice-registration-controller ok
    >   [+]poststarthook/apiservice-status-available-controller ok
    >   [+]poststarthook/kube-apiserver-autoregistration ok
    >   [+]autoregister-completion ok
    >   [+]poststarthook/apiservice-openapi-controller ok
    >   healthz check failed


# -----------------------------------------------------
# Check the load balancer status.
#[user@kubernator]

    openstack \
        --os-cloud "${cloudname:?}" \
        loadbalancer list

    >   +--------------------------------------+-----------------------------------------------------------------------------------------------------+----------------------------------+-------------+---------------------+----------+
    >   | id                                   | name                                                                                                | project_id                       | vip_address | provisioning_status | provider |
    >   +--------------------------------------+-----------------------------------------------------------------------------------------------------+----------------------------------+-------------+---------------------+----------+
    >   | 0cb0af38-91ed-4b7e-9a64-32b3bea778c2 | tiberius-20200923-nqzekodqww64-api_lb-u27rww5bqhag-loadbalancer-eukjy5mt6tdd                        | 21b4ae3a2ea44bc5a9c14005ed2963af | 10.0.0.240  | ACTIVE              | amphora  |
    >   | 631f4328-2e4d-4341-8ff3-43a54f0d3bc0 | tiberius-20200923-nqzekodqww64-etcd_lb-uzaqi3lftgwn-loadbalancer-o7lgwgyuqo5w                       | 21b4ae3a2ea44bc5a9c14005ed2963af | 10.0.0.156  | ACTIVE              | amphora  |
    >   | 1538ca11-9213-43e5-93d1-525616950654 | kube_service_08bb2885-77d7-4b69-9141-92b41f3580c9_default_augusta-20200923-ingress-nginx-controller | 21b4ae3a2ea44bc5a9c14005ed2963af | 10.0.0.186  | ACTIVE              | amphora  |
    >   +--------------------------------------+-----------------------------------------------------------------------------------------------------+----------------------------------+-------------+---------------------+----------+


    openstack \
        --os-cloud "${cloudname:?}" \
        loadbalancer show \
            --format json \
            0cb0af38-91ed-4b7e-9a64-32b3bea778c2

    >   {
    >     "admin_state_up": true,
    >     "availability_zone": "",
    >     "created_at": "2020-09-23T13:49:25",
    >     "description": "",
    >     "flavor_id": null,
    >     "id": "0cb0af38-91ed-4b7e-9a64-32b3bea778c2",
    >     "listeners": "089da294-126c-42c1-a0c1-07c84e0536d9",
    >     "name": "tiberius-20200923-nqzekodqww64-api_lb-u27rww5bqhag-loadbalancer-eukjy5mt6tdd",
    >     "operating_status": "ONLINE",
    >     "pools": "cefcd360-1d85-40c5-8b10-cd0e9a5ce4ba",
    >     "project_id": "21b4ae3a2ea44bc5a9c14005ed2963af",
    >     "provider": "amphora",
    >     "provisioning_status": "ACTIVE",
    >     "updated_at": "2020-09-27T23:41:47",
    >     "vip_address": "10.0.0.240",
    >     "vip_network_id": "97a2bd78-e2ba-47da-915b-4e3848968882",
    >     "vip_port_id": "6a43f3f9-be74-4e3a-aec3-b33bd3e02161",
    >     "vip_qos_policy_id": null,
    >     "vip_subnet_id": "35191a79-f3f4-41c3-8986-84ab1b2d03ab"
    >   }


    openstack \
        --os-cloud "${cloudname:?}" \
        loadbalancer show \
            --format json \
            631f4328-2e4d-4341-8ff3-43a54f0d3bc0

    >   {
    >     "admin_state_up": true,
    >     "availability_zone": "",
    >     "created_at": "2020-09-23T13:49:26",
    >     "description": "",
    >     "flavor_id": null,
    >     "id": "631f4328-2e4d-4341-8ff3-43a54f0d3bc0",
    >     "listeners": "16144a78-af98-48df-b24d-0624a3be5788",
    >     "name": "tiberius-20200923-nqzekodqww64-etcd_lb-uzaqi3lftgwn-loadbalancer-o7lgwgyuqo5w",
    >     "operating_status": "ERROR",
    >     "pools": "9eb04e28-458f-4154-8770-d85e572af9d6",
    >     "project_id": "21b4ae3a2ea44bc5a9c14005ed2963af",
    >     "provider": "amphora",
    >     "provisioning_status": "ACTIVE",
    >     "updated_at": "2020-10-05T12:54:42",
    >     "vip_address": "10.0.0.156",
    >     "vip_network_id": "97a2bd78-e2ba-47da-915b-4e3848968882",
    >     "vip_port_id": "1c4a96ec-2756-4c37-bbee-7441e3cb7a02",
    >     "vip_qos_policy_id": null,
    >     "vip_subnet_id": "35191a79-f3f4-41c3-8986-84ab1b2d03ab"
    >   }


    openstack \
        --os-cloud "${cloudname:?}" \
        loadbalancer show \
            --format json \
            1538ca11-9213-43e5-93d1-525616950654

    >   {
    >     "admin_state_up": true,
    >     "availability_zone": "",
    >     "created_at": "2020-09-23T14:00:45",
    >     "description": "Kubernetes external service default/augusta-20200923-ingress-nginx-controller from cluster 08bb2885-77d7-4b69-9141-92b41f3580c9",
    >     "flavor_id": null,
    >     "id": "1538ca11-9213-43e5-93d1-525616950654",
    >     "listeners": "9bbfcc0e-35af-4822-b5da-dc13af33ef5b\nf96f3930-74bd-4269-9483-dd4559cda6cd",
    >     "name": "kube_service_08bb2885-77d7-4b69-9141-92b41f3580c9_default_augusta-20200923-ingress-nginx-controller",
    >     "operating_status": "ONLINE",
    >     "pools": "2cde3cdf-0c9b-46d6-a66a-a0c879935570\nb273ecb2-e2aa-4bf8-8e73-3dc9ec73825f",
    >     "project_id": "21b4ae3a2ea44bc5a9c14005ed2963af",
    >     "provider": "amphora",
    >     "provisioning_status": "ACTIVE",
    >     "updated_at": "2020-09-27T23:43:56",
    >     "vip_address": "10.0.0.186",
    >     "vip_network_id": "97a2bd78-e2ba-47da-915b-4e3848968882",
    >     "vip_port_id": "038ac2b1-66d4-44f8-b915-3188d543cda1",
    >     "vip_qos_policy_id": null,
    >     "vip_subnet_id": "35191a79-f3f4-41c3-8986-84ab1b2d03ab"
    >   }

# -----------------------------------------------------

    Advice from Paul Browne on Slack:

    Paul Browne:

        Hmm, seems to etcd reachability at least, which may also still be a loadbalancer issue.
        The Octavia LB sitting in front of the (one-node, here) etcd cluster is reporting an operational status of ERROR ;


    openstack \
        --os-cloud "${cloudname:?}" \
        loadbalancer show \
            --format json \
            631f4328-2e4d-4341-8ff3-43a54f0d3bc0

    >   {
    >     ....
    >     "id": "631f4328-2e4d-4341-8ff3-43a54f0d3bc0",
    >     "name": "tiberius-20200923-nqzekodqww64-etcd_lb-uzaqi3lftgwn-loadbalancer-o7lgwgyuqo5w",
    >     "operating_status": "ERROR",
    >     ....
    >   }


    Paul Browne:

        So that implies the etcd in the k8s cluster may not be healthy, or otherwise not responding.
        Without remote access into the k8s cluster with kubectl, you could bounch SSH into the master
        with a throwaway SSH bastion booted to the same private network, and inspect the state of the master's etcd.


    openstack \
        --os-cloud "${cloudname:?}" \
        loadbalancer show \
            --format json \
            631f4328-2e4d-4341-8ff3-43a54f0d3bc0

    >   {
    >     ....
    >     "id": "631f4328-2e4d-4341-8ff3-43a54f0d3bc0",
    >     ....
    >     "operating_status": "ERROR",
    >     "pools": "9eb04e28-458f-4154-8770-d85e572af9d6",
    >     ....
    >   }


    openstack \
        --os-cloud "${cloudname:?}" \
        loadbalancer member list \
            --format json \
            9eb04e28-458f-4154-8770-d85e572af9d6

    >   [
    >     {
    >       "id": "070bc874-1f4a-450d-9726-f297ebd2a619",
    >       "name": "",
    >       "project_id": "21b4ae3a2ea44bc5a9c14005ed2963af",
    >       "provisioning_status": "ACTIVE",
    >       "address": "10.0.0.242",
    >       "protocol_port": 2379,
    >       "operating_status": "ERROR",
    >       "weight": 1
    >     }
    >   ]


    openstack \
        --os-cloud "${cloudname:?}" \
        loadbalancer member show \
            --format json \
            9eb04e28-458f-4154-8770-d85e572af9d6 \
            070bc874-1f4a-450d-9726-f297ebd2a619

    >   {
    >     "address": "10.0.0.242",
    >     "admin_state_up": true,
    >     "created_at": "2020-09-23T13:50:44",
    >     "id": "070bc874-1f4a-450d-9726-f297ebd2a619",
    >     "name": "",
    >     "operating_status": "ERROR",
    >     "project_id": "21b4ae3a2ea44bc5a9c14005ed2963af",
    >     "protocol_port": 2379,
    >     "provisioning_status": "ACTIVE",
    >     "subnet_id": "35191a79-f3f4-41c3-8986-84ab1b2d03ab",
    >     "updated_at": "2020-10-05T12:54:42",
    >     "weight": 1,
    >     "monitor_port": null,
    >     "monitor_address": null,
    >     "backup": false
    >   }

    Dave Morris:

        OK - thanks, that gives me some clues to chase, but I won't take it further this time.
        This is a dev instance and I need to get on with other things. So I'll leave it there for now and create a cluster.
        Now I know what symptoms to look for I'll be able to recognise it if it happens again.
        If it does, then I'll spend some more time looking into it in more detail.










