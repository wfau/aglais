#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    #
    # Step through the example cluster deploy from StackHPC
    # https://github.com/RSE-Cambridge/iris-magnum/blob/master/magnum-tour/README.md
    #

# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    podman run \
        --rm \
        --tty \
        --interactive \
        --hostname terraformer \
        --volume "${HOME}/clouds.yaml:/etc/openstack/clouds.yaml:z" \
        atolmis/terraform-client \
        bash


# -----------------------------------------------------
# Set the project and cluster names.
#[user@terraformer]

    cloudname=gaia-prod
    clustername=Tiberius


# -----------------------------------------------------
# Clone the StackHPC examples.
#[user@terraformer]

    pushd "${HOME}"

        git clone 'https://github.com/RSE-Cambridge/iris-magnum.git'

    popd


# -----------------------------------------------------
# Fetch our public key from OpenStack.
#[user@terraformer]

    keyname=$(
        openstack \
        --os-cloud "${cloudname:?}" \
            keypair list \
                --format json \
        | jq -r '.[0] | .Name'
        )

    keyfile=/root/.ssh/id_rsa.pub
    mkdir $(dirname ${keyfile})

    openstack \
        --os-cloud "${cloudname:?}" \
            keypair show \
            --public-key \
            "${keyname:?}" \
    | tee "${keyfile:?}"

    >   ssh-rsa AAAAB3Nz........zV4ksPOL Cambridge HPC OpenStack

    # The StachHPC module expects this.
    # Difficult to customise the filename.


# -----------------------------------------------------
# Use the unrestricted cloud credentials.
#[user@terraformer]

    # The Terraform module expects OS_CLOUD.
    export OS_CLOUD="${cloudname:?}-super"


# -----------------------------------------------------
# Set the cluster name.
#[user@terraformer]

    # This is hard-coded into the templates
    export clustername=my-test


# -----------------------------------------------------
# Run Terraform to deploy the example cluster.
#[user@terraformer]

    pushd "${HOME}/iris-magnum"
        pushd 'terraform/examples/cluster'

            terraform init

    >   Initializing modules...
    >   ....
    >   Terraform has been successfully initialized!
    >   ....


            terraform plan

    >   Terraform will perform the following actions:
    >   
    >     # module.cluster.null_resource.kubeconfig will be created
    >     + resource "null_resource" "kubeconfig" {
    >       ....
    >       }
    >   
    >     # module.cluster.openstack_compute_keypair_v2.keypair will be created
    >     + resource "openstack_compute_keypair_v2" "keypair" {
    >       ....
    >       }
    >   
    >     # module.cluster.openstack_containerinfra_cluster_v1.cluster will be created
    >     + resource "openstack_containerinfra_cluster_v1" "cluster" {
    >       ....
    >       }
    >   
    >   Plan: 3 to add, 0 to change, 0 to destroy.


            terraform apply

    >   ....
    >   ....
    >   Do you want to perform these actions?
    >     Terraform will perform the actions described above.
    >     Only 'yes' will be accepted to approve.
    >   
    >     Enter a value: yes
    >   ....
    >   ....
    >   Error: Unable to create openstack_compute_keypair_v2 my-test: Expected HTTP response code [200 201] when accessing [POST https://cumulus.openstack.hpc.cam.ac.uk:8774/v2.1/os-keypairs], but got 409 instead
    >   {"conflictingRequest": {"message": "Key pair 'my-test' already exists.", "code": 409}}
    >   
    >     on ../../modules/cluster/main.tf line 9, in resource "openstack_compute_keypair_v2" "keypair":
    >      9: resource "openstack_compute_keypair_v2" "keypair" {

    #
    # Terraform state is empty but Openstack already contains a ssh key called 'my-test'.
    # Terraform is so fragile to local state.
    #


# -----------------------------------------------------
# Delete the duplicate ssh key.
#[user@terraformer]

    openstack \
        --os-cloud "${cloudname:?}" \
            keypair delete \
                "${clustername:?}"


    openstack \
        --os-cloud "${cloudname:?}" \
            keypair list

    >   +------------------+-------------------------------------------------+
    >   | Name             | Fingerprint                                     |
    >   +------------------+-------------------------------------------------+
    >   | zrq-gaia-keypair | a4:8b:f3:0a:31:eb:93:b2:98:62:c5:d2:02:31:0f:b4 |
    >   +------------------+-------------------------------------------------+


# -----------------------------------------------------
# Run Terraform to deploy the example cluster.
#[user@terraformer]

    pushd "${HOME}/iris-magnum"
        pushd 'terraform/examples/cluster'

            terraform init

    >   Initializing modules...
    >   ....
    >   Terraform has been successfully initialized!
    >   ....


            terraform plan

    >   ....
    >   Terraform will perform the following actions:
    >   
    >     # module.cluster.null_resource.kubeconfig will be created
    >     + resource "null_resource" "kubeconfig" {
    >       ....
    >       }
    >   
    >     # module.cluster.openstack_compute_keypair_v2.keypair will be created
    >     + resource "openstack_compute_keypair_v2" "keypair" {
    >       ....
    >       }
    >   
    >     # module.cluster.openstack_containerinfra_cluster_v1.cluster will be created
    >     + resource "openstack_containerinfra_cluster_v1" "cluster" {
    >       ....
    >       }
    >   
    >   Plan: 3 to add, 0 to change, 0 to destroy.


            terraform apply

    >   ....
    >   ....
    >   Do you want to perform these actions?
    >     Terraform will perform the actions described above.
    >     Only 'yes' will be accepted to approve.
    >   
    >     Enter a value: yes
    >   
    >   module.cluster.openstack_compute_keypair_v2.keypair: Creating...
    >   module.cluster.openstack_compute_keypair_v2.keypair: Creation complete after 1s [id=my-test]
    >   module.cluster.openstack_containerinfra_cluster_v1.cluster: Creating...
    >   module.cluster.openstack_containerinfra_cluster_v1.cluster: Still creating... [10s elapsed]
    >   ....
    >   ....
    >   module.cluster.openstack_containerinfra_cluster_v1.cluster: Still creating... [1m0s elapsed]
    >   
    >   Error: Error waiting for openstack_containerinfra_cluster_v1 ab708936-f205-4953-9ce0-50f9097fbecd to become ready: json: cannot unmarshal object into Go struct field Cluster.health_status_reason of type string
    >   
    >     on ../../modules/cluster/main.tf line 14, in resource "openstack_containerinfra_cluster_v1" "cluster":
    >     14: resource "openstack_containerinfra_cluster_v1" "cluster" {

    # This always times out.
    # Ignore and watch the cluster list.

        popd
    popd


# -----------------------------------------------------
# List our clusters.
#[user@terraformer]

    watch \
        openstack \
            --os-cloud "${cloudname:?}" \
            coe cluster list


    >   +--------------------------------------+---------+---------+------------+--------------+--------------------+---------------+
    >   | uuid                                 | name    | keypair | node_count | master_count | status             | health_status |
    >   +--------------------------------------+---------+---------+------------+--------------+--------------------+---------------+
    >   | ab708936-f205-4953-9ce0-50f9097fbecd | my-test | my-test |          1 |            1 | CREATE_IN_PROGRESS | None          |
    >   +--------------------------------------+---------+---------+------------+--------------+--------------------+---------------+


    >   +--------------------------------------+---------+---------+------------+--------------+-----------------+---------------+
    >   | uuid                                 | name    | keypair | node_count | master_count | status          | health_status |
    >   +--------------------------------------+---------+---------+------------+--------------+-----------------+---------------+
    >   | ab708936-f205-4953-9ce0-50f9097fbecd | my-test | my-test |          1 |            1 | CREATE_COMPLETE | HEALTHY       |
    >   +--------------------------------------+---------+---------+------------+--------------+-----------------+---------------+


# -----------------------------------------------------
# Get the details for our new cluster.
#[user@terraformer]


    clusterid=$(
        openstack \
        --os-cloud "${cloudname:?}" \
            coe cluster list \
                --format json \
        | jq -r '.[] | select(.name == "'${clustername:?}'") | .uuid'
        )

    openstack \
        --os-cloud "${cloudname:?}" \
        coe cluster show \
            "${clusterid}" \
                --format json \
    | jq '.'


    >   {
    >     "status": "CREATE_COMPLETE",
    >     "health_status": "HEALTHY",
    >     "cluster_template_id": "d54167d9-495f-437e-88fe-d182b2a230ea",
    >     "node_addresses": [
    >       "10.0.0.83"
    >     ],
    >     "uuid": "ab708936-f205-4953-9ce0-50f9097fbecd",
    >     "stack_id": "33553b5b-7221-4898-8a3f-92c0c61da418",
    >     "status_reason": null,
    >     "created_at": "2020-06-23T11:09:38+00:00",
    >     "updated_at": "2020-06-23T11:15:40+00:00",
    >     "coe_version": "v1.15.9",
    >     "labels": {
    >       "auto_healing_controller": "magnum-auto-healer",
    >       "max_node_count": "2",
    >       "cloud_provider_tag": "v1.15.0",
    >       "etcd_tag": "3.3.17",
    >       "monitoring_enabled": "true",
    >       "tiller_enabled": "true",
    >       "autoscaler_tag": "v1.15.2",
    >       "master_lb_floating_ip_enabled": "true",
    >       "min_node_count": "1",
    >       "tiller_tag": "v2.16.1",
    >       "use_podman": "true",
    >       "auto_healing_enabled": "true",
    >       "heat_container_agent_tag": "train-stable-1",
    >       "kube_tag": "v1.15.9",
    >       "auto_scaling_enabled": "true"
    >     },
    >     "faults": "",
    >     "keypair": "my-test",
    >     "api_address": "https://128.232.227.218:6443",
    >     "master_addresses": [
    >       "10.0.0.95"
    >     ],
    >     "create_timeout": null,
    >     "node_count": 1,
    >     "discovery_url": "https://discovery.etcd.io/b0a06da82b49d3a4cda9233a01cedc6f",
    >     "master_count": 1,
    >     "container_version": "1.12.6",
    >     "name": "my-test",
    >     "master_flavor_id": "general.v1.tiny",
    >     "flavor_id": "general.v1.tiny",
    >     "health_status_reason": {
    >       "my-test-lmck76anbakg-master-0.Ready": "True",
    >       "my-test-lmck76anbakg-node-0.Ready": "True",
    >       "api": "ok"
    >     },
    >     "project_id": "21b4ae3a2ea44bc5a9c14005ed2963af"
    >   }


# -----------------------------------------------------
# Get the kubectl config for our cluster.
#[user@terraformer]

    pushd "${HOME}"

        openstack \
            --os-cloud "${cloudname:?}" \
            coe cluster config \
                "${clusterid}"

    popd

    cat "${HOME}/config"


    >   apiVersion: v1
    >   clusters:
    >   - cluster:
    >       certificate-authority-data: LS0tLS1C........RS0tLS0t
    >       server: https://128.232.227.218:6443
    >     name: my-test
    >   contexts:
    >   - context:
    >       cluster: my-test
    >       user: admin
    >     name: default
    >   current-context: default
    >   kind: Config
    >   preferences: {}
    >   users:
    >   - name: admin
    >     user:
    >       client-certificate-data: LS0tLS1C........FLS0tLS0=
    >       client-key-data: LS0tLS1C........tLS0tLQo=


# -----------------------------------------------------
# Use kubectl to get details of our cluster.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        cluster-info

    >   Kubernetes master is running at https://128.232.227.218:6443
    >   Heapster is running at https://128.232.227.218:6443/api/v1/namespaces/kube-system/services/heapster/proxy
    >   CoreDNS is running at https://128.232.227.218:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
    >   
    >   To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.


# -----------------------------------------------------
# Create a 'hello world' deployment.
#[user@terraformer]

    helloname=hello-node
    helloname=AlberiusCant

    kubectl \
        --kubeconfig "${HOME}/config" \
        create deployment "${helloname:?}" \
            --image=gcr.io/hello-minikube-zero-install/hello-node

    >   The Deployment "AlberiusCant" is invalid: metadata.name:
    >       Invalid value: "AlberiusCant": a DNS-1123 subdomain must consist of lower case alphanumeric characters, '-' or '.',
    >       and must start and end with an alphanumeric character (e.g. 'example.com',
    >       regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')

    #
    # Wow - OK, back to the 1980's :-(
    #

    helloname=alberius-cant

    kubectl \
        --kubeconfig "${HOME}/config" \
        create deployment \
            "${helloname:?}" \
            --image=gcr.io/hello-minikube-zero-install/hello-node

    >   deployment.apps/alberius-cant created


# -----------------------------------------------------
# Expose our a 'hello world' deployment with a load balancer.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        expose deployment \
            "${helloname:?}" \
            --type=LoadBalancer \
            --port=8080


    >   service/alberius-cant exposed


# -----------------------------------------------------
# Check how far it has reached.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        get all

    >   NAME                                 READY   STATUS             RESTARTS   AGE
    >   pod/alberius-cant-6d68d7fbbf-sf2b2   0/1     ImagePullBackOff   0          2m33s
    >   
    >   
    >   NAME                    TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
    >   service/alberius-cant   LoadBalancer   10.254.177.129   <pending>     8080:32497/TCP   53s
    >   service/kubernetes      ClusterIP      10.254.0.1       <none>        443/TCP          35m
    >   
    >   
    >   NAME                            READY   UP-TO-DATE   AVAILABLE   AGE
    >   deployment.apps/alberius-cant   0/1     1            0           2m33s
    >   
    >   NAME                                       DESIRED   CURRENT   READY   AGE
    >   replicaset.apps/alberius-cant-6d68d7fbbf   1         1         0       2m33s


    >   NAME                                 READY   STATUS             RESTARTS   AGE
    >   pod/alberius-cant-6d68d7fbbf-sf2b2   0/1     ImagePullBackOff   0          3m48s
    >   
    >   
    >   NAME                    TYPE           CLUSTER-IP       EXTERNAL-IP       PORT(S)          AGE
    >   service/alberius-cant   LoadBalancer   10.254.177.129   128.232.227.214   8080:32497/TCP   2m8s
    >   service/kubernetes      ClusterIP      10.254.0.1       <none>            443/TCP          36m
    >   ....
    >   ....



    >   NAME                                 READY   STATUS             RESTARTS   AGE
    >   pod/alberius-cant-6d68d7fbbf-sf2b2   0/1     ImagePullBackOff   0          5m12s
    >   ....
    >   ....

    #
    # Stuck at ImagePullBackOff for 5m12s sounds like a problem.
    # https://managedkube.com/kubernetes/k8sbot/troubleshooting/imagepullbackoff/2019/02/23/imagepullbackoff.html

# -----------------------------------------------------
# Get the details about the stalled pod.
#[user@terraformer]

    podname=alberius-cant-6d68d7fbbf-sf2b2

    kubectl \
        --kubeconfig "${HOME}/config" \
        describe pod \
            "${podname:?}"

    >   Name:           alberius-cant-6d68d7fbbf-sf2b2
    >   Namespace:      default
    >   Node:           my-test-lmck76anbakg-node-0/10.0.0.83
    >   Start Time:     Tue, 23 Jun 2020 11:45:55 +0000
    >   Labels:         app=alberius-cant
    >                   pod-template-hash=6d68d7fbbf
    >   Annotations:    <none>
    >   Status:         Pending
    >   IP:             10.100.1.48
    >   Controlled By:  ReplicaSet/alberius-cant-6d68d7fbbf
    >   Containers:
    >     hello-node:
    >       Container ID:
    >       Image:          gcr.io/hello-minikube-zero-install/hello-node
    >       Image ID:
    >       Port:           <none>
    >       Host Port:      <none>
    >       State:          Waiting
    >         Reason:       ImagePullBackOff
    >       Ready:          False
    >       Restart Count:  0
    >       Environment:    <none>
    >       Mounts:
    >         /var/run/secrets/kubernetes.io/serviceaccount from default-token-d759j (ro)
    >   Conditions:
    >     Type              Status
    >     Initialized       True
    >     Ready             False
    >     ContainersReady   False
    >     PodScheduled      True
    >   Volumes:
    >     default-token-d759j:
    >       Type:        Secret (a volume populated by a Secret)
    >       SecretName:  default-token-d759j
    >       Optional:    false
    >   QoS Class:       BestEffort
    >   Node-Selectors:  <none>
    >   Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
    >                    node.kubernetes.io/unreachable:NoExecute for 300s
    >   Events:
    >     Type     Reason          Age                  From                                  Message
    >     ----     ------          ----                 ----                                  -------
    >     Normal   Scheduled       30m                  default-scheduler                     Successfully assigned default/alberius-cant-6d68d7fbbf-sf2b2 to my-test-lmck76anbakg-node-0
    >     Normal   Pulling         30m (x2 over 30m)    kubelet, my-test-lmck76anbakg-node-0  Pulling image "gcr.io/hello-minikube-zero-install/hello-node"
    >     Warning  Failed          30m (x2 over 30m)    kubelet, my-test-lmck76anbakg-node-0  Failed to pull image "gcr.io/hello-minikube-zero-install/hello-node": rpc error: code = Unknown desc = unable to retrieve auth token: 401 unauthorized
    >     Warning  Failed          30m (x2 over 30m)    kubelet, my-test-lmck76anbakg-node-0  Error: ErrImagePull
    >     Normal   SandboxChanged  30m (x6 over 30m)    kubelet, my-test-lmck76anbakg-node-0  Pod sandbox changed, it will be killed and re-created.
    >     Normal   BackOff         10m (x92 over 30m)   kubelet, my-test-lmck76anbakg-node-0  Back-off pulling image "gcr.io/hello-minikube-zero-install/hello-node"
    >     Warning  Failed          14s (x136 over 30m)  kubelet, my-test-lmck76anbakg-node-0  Error: ImagePullBackOff

    #
    # This looks like the cause.
    #


    >   ....
    >     Normal   Pulling         30m (x2 over 30m)    kubelet, my-test-lmck76anbakg-node-0  Pulling image "gcr.io/hello-minikube-zero-install/hello-node"
    >     Warning  Failed          30m (x2 over 30m)    kubelet, my-test-lmck76anbakg-node-0  Failed to pull image "gcr.io/hello-minikube-zero-install/hello-node": rpc error: code = Unknown desc = unable to retrieve auth token: 401 unauthorized
    >   ....

    #
    # Failed to pull image "gcr.io/hello-minikube-zero-install/hello-node":
    # rpc error:
    #   code = Unknown
    #   desc = unable to retrieve auth token: 401 unauthorized
    #


    #
    # We have an out of date image name.
    # Looks like the original 'hello-minikube-zero-install' has been removed.
    # https://github.com/kubernetes/website/pull/20598/files
    #

    -   kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node
    +   kubectl create deployment hello-node --image=k8s.gcr.io/echoserver:1.4


# -----------------------------------------------------
# Try deleting the deployment.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        delete deployment \
            "${helloname:?}"

    >   deployment.extensions "alberius-cant" deleted


# -----------------------------------------------------
# Check what we have left.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        get all

    >   NAME                    TYPE           CLUSTER-IP       EXTERNAL-IP       PORT(S)          AGE
    >   service/alberius-cant   LoadBalancer   10.254.177.129   128.232.227.214   8080:32497/TCP   38m
    >   service/kubernetes      ClusterIP      10.254.0.1       <none>            443/TCP          72m


# -----------------------------------------------------
# Delete our load balancer.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        delete service \
            "${helloname:?}"

    >   service "alberius-cant" deleted


# -----------------------------------------------------
# Check what we have left.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        get all

    >   NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
    >   service/kubernetes   ClusterIP   10.254.0.1   <none>        443/TCP   75m


# -----------------------------------------------------
# Try create the deployment again.
#[user@terraformer]

    helloname=alberius-cant
    imagename=k8s.gcr.io/echoserver

    kubectl \
        --kubeconfig "${HOME}/config" \
        create deployment \
            "${helloname:?}" \
            --image=${imagename:?}

    >   deployment.apps/alberius-cant created


# -----------------------------------------------------
# Check what happened.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        get all


    >   NAME                                 READY   STATUS             RESTARTS   AGE
    >   pod/alberius-cant-655df5874b-n482v   0/1     ImagePullBackOff   0          17s
    >   
    >   
    >   NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
    >   service/kubernetes   ClusterIP   10.254.0.1   <none>        443/TCP   77m
    >   
    >   
    >   NAME                            READY   UP-TO-DATE   AVAILABLE   AGE
    >   deployment.apps/alberius-cant   0/1     1            0           17s
    >   
    >   NAME                                       DESIRED   CURRENT   READY   AGE
    >   replicaset.apps/alberius-cant-655df5874b   1         1         0       18s


    #
    # OK, a different error ..
    #


    >   pod/alberius-cant-655df5874b-n482v   0/1     ErrImagePull   0          91s


# -----------------------------------------------------
# Get the details about the failed pod.
#[user@terraformer]

    podname=alberius-cant-655df5874b-n482v

    kubectl \
        --kubeconfig "${HOME}/config" \
        describe pod \
            "${podname:?}"


    >   Name:           alberius-cant-655df5874b-n482v
    >   Namespace:      default
    >   Node:           my-test-lmck76anbakg-node-0/10.0.0.83
    >   Start Time:     Tue, 23 Jun 2020 12:30:32 +0000
    >   Labels:         app=alberius-cant
    >                   pod-template-hash=655df5874b
    >   Annotations:    <none>
    >   Status:         Pending
    >   IP:             10.100.1.65
    >   Controlled By:  ReplicaSet/alberius-cant-655df5874b
    >   Containers:
    >     echoserver:
    >       Container ID:
    >       Image:          k8s.gcr.io/echoserver
    >       Image ID:
    >       Port:           <none>
    >       Host Port:      <none>
    >       State:          Waiting
    >         Reason:       ImagePullBackOff
    >       Ready:          False
    >       Restart Count:  0
    >       Environment:    <none>
    >       Mounts:
    >         /var/run/secrets/kubernetes.io/serviceaccount from default-token-d759j (ro)
    >   Conditions:
    >     Type              Status
    >     Initialized       True
    >     Ready             False
    >     ContainersReady   False
    >     PodScheduled      True
    >   Volumes:
    >     default-token-d759j:
    >       Type:        Secret (a volume populated by a Secret)
    >       SecretName:  default-token-d759j
    >       Optional:    false
    >   QoS Class:       BestEffort
    >   Node-Selectors:  <none>
    >   Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
    >                    node.kubernetes.io/unreachable:NoExecute for 300s
    >   Events:
    >     Type     Reason          Age                    From                                  Message
    >     ----     ------          ----                   ----                                  -------
    >     Normal   Scheduled       2m43s                  default-scheduler                     Successfully assigned default/alberius-cant-655df5874b-n482v to my-test-lmck76anbakg-node-0
    >     Normal   Pulling         2m29s (x2 over 2m41s)  kubelet, my-test-lmck76anbakg-node-0  Pulling image "k8s.gcr.io/echoserver"
    >     Warning  Failed          2m28s (x2 over 2m40s)  kubelet, my-test-lmck76anbakg-node-0  Failed to pull image "k8s.gcr.io/echoserver": rpc error: code = Unknown desc = manifest for k8s.gcr.io/echoserver:latest not found
    >     Warning  Failed          2m28s (x2 over 2m40s)  kubelet, my-test-lmck76anbakg-node-0  Error: ErrImagePull
    >     Normal   SandboxChanged  2m26s (x7 over 2m40s)  kubelet, my-test-lmck76anbakg-node-0  Pod sandbox changed, it will be killed and re-created.
    >     Normal   BackOff         2m24s (x6 over 2m39s)  kubelet, my-test-lmck76anbakg-node-0  Back-off pulling image "k8s.gcr.io/echoserver"
    >     Warning  Failed          2m24s (x6 over 2m39s)  kubelet, my-test-lmck76anbakg-node-0  Error: ImagePullBackOff

    #
    # This looks like the cause.
    #

    >   ....
    >     Normal   Pulling         2m29s (x2 over 2m41s)  kubelet, my-test-lmck76anbakg-node-0  Pulling image "k8s.gcr.io/echoserver"
    >     Warning  Failed          2m28s (x2 over 2m40s)  kubelet, my-test-lmck76anbakg-node-0  Failed to pull image "k8s.gcr.io/echoserver": rpc error: code = Unknown desc = manifest for k8s.gcr.io/echoserver:latest not found
    >   ....

    #
    # Need to specify the image version.
    #

# -----------------------------------------------------
# Delete the failed deployment.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        delete deployment \
            "${helloname:?}"

    >   deployment.extensions "alberius-cant" deleted


# -----------------------------------------------------
# Try create the deployment again.
#[user@terraformer]

    helloname=alberius-cant
    imagename=k8s.gcr.io/echoserver:1.4

    kubectl \
        --kubeconfig "${HOME}/config" \
        create deployment \
            "${helloname:?}" \
            --image=${imagename:?}

    >   deployment.apps/alberius-cant created


# -----------------------------------------------------
# Check what happened.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        get all

    >   NAME                                 READY   STATUS    RESTARTS   AGE
    >   pod/alberius-cant-6f64989475-nkszd   1/1     Running   0          17s
    >   
    >   
    >   NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
    >   service/kubernetes   ClusterIP   10.254.0.1   <none>        443/TCP   82m
    >   
    >   
    >   NAME                            READY   UP-TO-DATE   AVAILABLE   AGE
    >   deployment.apps/alberius-cant   1/1     1            1           17s
    >   
    >   NAME                                       DESIRED   CURRENT   READY   AGE
    >   replicaset.apps/alberius-cant-6f64989475   1         1         1       17s

    #
    # Better :-)
    #


# -----------------------------------------------------
# Expose our a 'hello world' deployment with a load balancer.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        expose deployment \
            "${helloname:?}" \
            --type=LoadBalancer \
            --port=8080


    >   service/alberius-cant exposed


# -----------------------------------------------------
# Check what happened.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        get all

    >   NAME                                 READY   STATUS    RESTARTS   AGE
    >   pod/alberius-cant-6f64989475-nkszd   1/1     Running   0          94s
    >   
    >   NAME                    TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
    >   service/alberius-cant   LoadBalancer   10.254.237.125   <pending>     8080:32701/TCP   25s
    >   service/kubernetes      ClusterIP      10.254.0.1       <none>        443/TCP          84m
    >   
    >   NAME                            READY   UP-TO-DATE   AVAILABLE   AGE
    >   deployment.apps/alberius-cant   1/1     1            1           94s
    >   
    >   NAME                                       DESIRED   CURRENT   READY   AGE
    >   replicaset.apps/alberius-cant-6f64989475   1         1         1       94s


    >   NAME                                 READY   STATUS    RESTARTS   AGE
    >   pod/alberius-cant-6f64989475-nkszd   1/1     Running   0          2m31s
    >   
    >   NAME                    TYPE           CLUSTER-IP       EXTERNAL-IP       PORT(S)          AGE
    >   service/alberius-cant   LoadBalancer   10.254.237.125   128.232.227.137   8080:32701/TCP   82s
    >   service/kubernetes      ClusterIP      10.254.0.1       <none>            443/TCP          84m
    >   ....

    #
    # Takes ~2min to create the external IP address.
    #


# -----------------------------------------------------
# Machine readable format.
# https://kubernetes.io/docs/reference/kubectl/cheatsheet/#formatting-output
#[user@terraformer]

    kubectl \
        --output json \
        --kubeconfig "${HOME}/config" \
        get all \
    | jq '.'


    >   {
    >     "apiVersion": "v1",
    >     "items": [
    >       {
    >         "apiVersion": "v1",
    >         "kind": "Pod",
    >         "metadata": {
    >           "creationTimestamp": "2020-06-23T12:35:43Z",
    >           "generateName": "alberius-cant-6f64989475-",
    >           "labels": {
    >             "app": "alberius-cant",
    >             "pod-template-hash": "6f64989475"
    >           },
    >           "name": "alberius-cant-6f64989475-nkszd",
    >           "namespace": "default",
    >           "ownerReferences": [
    >             {
    >               "apiVersion": "apps/v1",
    >               "blockOwnerDeletion": true,
    >               "controller": true,
    >               "kind": "ReplicaSet",
    >               "name": "alberius-cant-6f64989475",
    >               "uid": "6841f4be-7bcc-4dc6-ad28-68db3d705c80"
    >             }
    >           ],
    >           "resourceVersion": "14646",
    >           "selfLink": "/api/v1/namespaces/default/pods/alberius-cant-6f64989475-nkszd",
    >           "uid": "4593c425-e0ed-466d-92ae-7184bec43e52"
    >         },
    >         "spec": {
    >           "containers": [
    >             {
    >               "image": "k8s.gcr.io/echoserver:1.4",
    >               "imagePullPolicy": "IfNotPresent",
    >               "name": "echoserver",
    >               "resources": {},
    >               "terminationMessagePath": "/dev/termination-log",
    >               "terminationMessagePolicy": "File",
    >               "volumeMounts": [
    >                 {
    >                   "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
    >                   "name": "default-token-d759j",
    >                   "readOnly": true
    >                 }
    >               ]
    >             }
    >           ],
    >           "dnsPolicy": "ClusterFirst",
    >           "enableServiceLinks": true,
    >           "nodeName": "my-test-lmck76anbakg-node-0",
    >           "restartPolicy": "Always",
    >           "schedulerName": "default-scheduler",
    >           "securityContext": {},
    >           "serviceAccount": "default",
    >           "serviceAccountName": "default",
    >           "terminationGracePeriodSeconds": 30,
    >           "tolerations": [
    >             {
    >               "effect": "NoExecute",
    >               "key": "node.kubernetes.io/not-ready",
    >               "operator": "Exists",
    >               "tolerationSeconds": 300
    >             },
    >             {
    >               "effect": "NoExecute",
    >               "key": "node.kubernetes.io/unreachable",
    >               "operator": "Exists",
    >               "tolerationSeconds": 300
    >             }
    >           ],
    >           "volumes": [
    >             {
    >               "name": "default-token-d759j",
    >               "secret": {
    >                 "defaultMode": 420,
    >                 "secretName": "default-token-d759j"
    >               }
    >             }
    >           ]
    >         },
    >         "status": {
    >           "conditions": [
    >             {
    >               "lastProbeTime": null,
    >               "lastTransitionTime": "2020-06-23T12:35:43Z",
    >               "status": "True",
    >               "type": "Initialized"
    >             },
    >             {
    >               "lastProbeTime": null,
    >               "lastTransitionTime": "2020-06-23T12:35:50Z",
    >               "status": "True",
    >               "type": "Ready"
    >             },
    >             {
    >               "lastProbeTime": null,
    >               "lastTransitionTime": "2020-06-23T12:35:50Z",
    >               "status": "True",
    >               "type": "ContainersReady"
    >             },
    >             {
    >               "lastProbeTime": null,
    >               "lastTransitionTime": "2020-06-23T12:35:43Z",
    >               "status": "True",
    >               "type": "PodScheduled"
    >             }
    >           ],
    >           "containerStatuses": [
    >             {
    >               "containerID": "docker://bc0f88a7880d0f6b3191475c56def26cb29fc9177ed61b13ce1d030d236b2165",
    >               "image": "k8s.gcr.io/echoserver:1.4",
    >               "imageID": "docker-pullable://k8s.gcr.io/echoserver@sha256:5d99aa1120524c801bc8c1a7077e8f5ec122ba16b6dda1a5d3826057f67b9bcb",
    >               "lastState": {},
    >               "name": "echoserver",
    >               "ready": true,
    >               "restartCount": 0,
    >               "state": {
    >                 "running": {
    >                   "startedAt": "2020-06-23T12:35:50Z"
    >                 }
    >               }
    >             }
    >           ],
    >           "hostIP": "10.0.0.83",
    >           "phase": "Running",
    >           "podIP": "10.100.1.66",
    >           "qosClass": "BestEffort",
    >           "startTime": "2020-06-23T12:35:43Z"
    >         }
    >       },
    >       {
    >         "apiVersion": "v1",
    >         "kind": "Service",
    >         "metadata": {
    >           "creationTimestamp": "2020-06-23T12:36:52Z",
    >           "labels": {
    >             "app": "alberius-cant"
    >           },
    >           "name": "alberius-cant",
    >           "namespace": "default",
    >           "resourceVersion": "15007",
    >           "selfLink": "/api/v1/namespaces/default/services/alberius-cant",
    >           "uid": "7524a846-f01a-431b-8473-37b23c2ed514"
    >         },
    >         "spec": {
    >           "clusterIP": "10.254.237.125",
    >           "externalTrafficPolicy": "Cluster",
    >           "ports": [
    >             {
    >               "nodePort": 32701,
    >               "port": 8080,
    >               "protocol": "TCP",
    >               "targetPort": 8080
    >             }
    >           ],
    >           "selector": {
    >             "app": "alberius-cant"
    >           },
    >           "sessionAffinity": "None",
    >           "type": "LoadBalancer"
    >         },
    >         "status": {
    >           "loadBalancer": {
    >             "ingress": [
    >               {
    >                 "ip": "128.232.227.137"
    >               }
    >             ]
    >           }
    >         }
    >       },
    >       {
    >         "apiVersion": "v1",
    >         "kind": "Service",
    >         "metadata": {
    >           "creationTimestamp": "2020-06-23T11:13:15Z",
    >           "labels": {
    >             "component": "apiserver",
    >             "provider": "kubernetes"
    >           },
    >           "name": "kubernetes",
    >           "namespace": "default",
    >           "resourceVersion": "160",
    >           "selfLink": "/api/v1/namespaces/default/services/kubernetes",
    >           "uid": "d7f038ec-6b6f-49e0-a497-4eab4bcc6afd"
    >         },
    >         "spec": {
    >           "clusterIP": "10.254.0.1",
    >           "ports": [
    >             {
    >               "name": "https",
    >               "port": 443,
    >               "protocol": "TCP",
    >               "targetPort": 6443
    >             }
    >           ],
    >           "sessionAffinity": "None",
    >           "type": "ClusterIP"
    >         },
    >         "status": {
    >           "loadBalancer": {}
    >         }
    >       },
    >       {
    >         "apiVersion": "apps/v1",
    >         "kind": "Deployment",
    >         "metadata": {
    >           "annotations": {
    >             "deployment.kubernetes.io/revision": "1"
    >           },
    >           "creationTimestamp": "2020-06-23T12:35:43Z",
    >           "generation": 1,
    >           "labels": {
    >             "app": "alberius-cant"
    >           },
    >           "name": "alberius-cant",
    >           "namespace": "default",
    >           "resourceVersion": "14648",
    >           "selfLink": "/apis/apps/v1/namespaces/default/deployments/alberius-cant",
    >           "uid": "4a101ac4-8a3c-4f4b-bfb1-ed4955ff95ac"
    >         },
    >         "spec": {
    >           "progressDeadlineSeconds": 600,
    >           "replicas": 1,
    >           "revisionHistoryLimit": 10,
    >           "selector": {
    >             "matchLabels": {
    >               "app": "alberius-cant"
    >             }
    >           },
    >           "strategy": {
    >             "rollingUpdate": {
    >               "maxSurge": "25%",
    >               "maxUnavailable": "25%"
    >             },
    >             "type": "RollingUpdate"
    >           },
    >           "template": {
    >             "metadata": {
    >               "creationTimestamp": null,
    >               "labels": {
    >                 "app": "alberius-cant"
    >               }
    >             },
    >             "spec": {
    >               "containers": [
    >                 {
    >                   "image": "k8s.gcr.io/echoserver:1.4",
    >                   "imagePullPolicy": "IfNotPresent",
    >                   "name": "echoserver",
    >                   "resources": {},
    >                   "terminationMessagePath": "/dev/termination-log",
    >                   "terminationMessagePolicy": "File"
    >                 }
    >               ],
    >               "dnsPolicy": "ClusterFirst",
    >               "restartPolicy": "Always",
    >               "schedulerName": "default-scheduler",
    >               "securityContext": {},
    >               "terminationGracePeriodSeconds": 30
    >             }
    >           }
    >         },
    >         "status": {
    >           "availableReplicas": 1,
    >           "conditions": [
    >             {
    >               "lastTransitionTime": "2020-06-23T12:35:50Z",
    >               "lastUpdateTime": "2020-06-23T12:35:50Z",
    >               "message": "Deployment has minimum availability.",
    >               "reason": "MinimumReplicasAvailable",
    >               "status": "True",
    >               "type": "Available"
    >             },
    >             {
    >               "lastTransitionTime": "2020-06-23T12:35:43Z",
    >               "lastUpdateTime": "2020-06-23T12:35:50Z",
    >               "message": "ReplicaSet \"alberius-cant-6f64989475\" has successfully progressed.",
    >               "reason": "NewReplicaSetAvailable",
    >               "status": "True",
    >               "type": "Progressing"
    >             }
    >           ],
    >           "observedGeneration": 1,
    >           "readyReplicas": 1,
    >           "replicas": 1,
    >           "updatedReplicas": 1
    >         }
    >       },
    >       {
    >         "apiVersion": "apps/v1",
    >         "kind": "ReplicaSet",
    >         "metadata": {
    >           "annotations": {
    >             "deployment.kubernetes.io/desired-replicas": "1",
    >             "deployment.kubernetes.io/max-replicas": "2",
    >             "deployment.kubernetes.io/revision": "1"
    >           },
    >           "creationTimestamp": "2020-06-23T12:35:43Z",
    >           "generation": 1,
    >           "labels": {
    >             "app": "alberius-cant",
    >             "pod-template-hash": "6f64989475"
    >           },
    >           "name": "alberius-cant-6f64989475",
    >           "namespace": "default",
    >           "ownerReferences": [
    >             {
    >               "apiVersion": "apps/v1",
    >               "blockOwnerDeletion": true,
    >               "controller": true,
    >               "kind": "Deployment",
    >               "name": "alberius-cant",
    >               "uid": "4a101ac4-8a3c-4f4b-bfb1-ed4955ff95ac"
    >             }
    >           ],
    >           "resourceVersion": "14647",
    >           "selfLink": "/apis/apps/v1/namespaces/default/replicasets/alberius-cant-6f64989475",
    >           "uid": "6841f4be-7bcc-4dc6-ad28-68db3d705c80"
    >         },
    >         "spec": {
    >           "replicas": 1,
    >           "selector": {
    >             "matchLabels": {
    >               "app": "alberius-cant",
    >               "pod-template-hash": "6f64989475"
    >             }
    >           },
    >           "template": {
    >             "metadata": {
    >               "creationTimestamp": null,
    >               "labels": {
    >                 "app": "alberius-cant",
    >                 "pod-template-hash": "6f64989475"
    >               }
    >             },
    >             "spec": {
    >               "containers": [
    >                 {
    >                   "image": "k8s.gcr.io/echoserver:1.4",
    >                   "imagePullPolicy": "IfNotPresent",
    >                   "name": "echoserver",
    >                   "resources": {},
    >                   "terminationMessagePath": "/dev/termination-log",
    >                   "terminationMessagePolicy": "File"
    >                 }
    >               ],
    >               "dnsPolicy": "ClusterFirst",
    >               "restartPolicy": "Always",
    >               "schedulerName": "default-scheduler",
    >               "securityContext": {},
    >               "terminationGracePeriodSeconds": 30
    >             }
    >           }
    >         },
    >         "status": {
    >           "availableReplicas": 1,
    >           "fullyLabeledReplicas": 1,
    >           "observedGeneration": 1,
    >           "readyReplicas": 1,
    >           "replicas": 1
    >         }
    >       }
    >     ],
    >     "kind": "List",
    >     "metadata": {
    >       "resourceVersion": "",
    >       "selfLink": ""
    >     }
    >   }


# -----------------------------------------------------
# Extract the public endpoint.
#[user@terraformer]

    kubectl \
        --output json \
        --kubeconfig "${HOME}/config" \
        get all \
    | jq '.'


# -----------------------------------------------------
# Try the public endpoint.
#[user@terraformer]

    kubectl \
        --output json \
        --kubeconfig "${HOME}/config" \
        get all \
    | jq -r '
        .items[]
        | select(.kind == "Service")
        | select(.metadata.name == "'${helloname:?}'")
        | .status.loadBalancer.ingress[0].ip
        '
    >   128.232.227.137

    publicip=$(
        kubectl \
            --output json \
            --kubeconfig "${HOME}/config" \
            get all \
        | jq -r '
            .items[]
            | select(.kind == "Service")
            | select(.metadata.name == "'${helloname:?}'")
            | .status.loadBalancer.ingress[0].ip
            '
        )


    curl "http://${publicip}:8080/"

    >   CLIENT VALUES:
    >   client_address=10.100.1.1
    >   command=GET
    >   real path=/
    >   query=nil
    >   request_version=1.1
    >   request_uri=http://128.232.227.137:8080/
    >   
    >   SERVER VALUES:
    >   server_version=nginx: 1.10.0 - lua: 10001
    >   
    >   HEADERS RECEIVED:
    >   accept=*/*
    >   host=128.232.227.137:8080
    >   user-agent=curl/7.69.1
    >   BODY:
    >   -no body in request-


# -----------------------------------------------------
# List the Openstack load balancers.
#[user@terraformer]

    openstack \
    --os-cloud "${cloudname:?}" \
        loadbalancer list

    >   +--------------------------------------+-------------------------------------------------------------------------+----------------------------------+-------------+---------------------+----------+
    >   | id                                   | name                                                                    | project_id                       | vip_address | provisioning_status | provider |
    >   +--------------------------------------+-------------------------------------------------------------------------+----------------------------------+-------------+---------------------+----------+
    >   | 24c37a2c-85ae-46ae-9230-c65d8b323e3c | my-test-lmck76anbakg-api_lb-svzaiuvai6hu-loadbalancer-x7kmmbrh3zhd      | 21b4ae3a2ea44bc5a9c14005ed2963af | 10.0.0.37   | ACTIVE              | amphora  |
    >   | b9369bff-c48d-42b9-bf7a-2e913aeaa02b | my-test-lmck76anbakg-etcd_lb-f5rgsjse3vy2-loadbalancer-pmhynebpnfll     | 21b4ae3a2ea44bc5a9c14005ed2963af | 10.0.0.9    | ACTIVE              | amphora  |
    >   | 01275cba-6ab5-4dcf-a210-ed594ee900bc | kube_service_ab708936-f205-4953-9ce0-50f9097fbecd_default_alberius-cant | 21b4ae3a2ea44bc5a9c14005ed2963af | 10.0.0.58   | ACTIVE              | amphora  |
    >   +--------------------------------------+-------------------------------------------------------------------------+----------------------------------+-------------+---------------------+----------+







