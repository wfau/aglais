#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    #
    # Based on install from 20200416-01-spark-deploy.txt
    # Based on download from 20200417-01-gdr2-download.txt
    #


# -----------------------------------------------------
# Login to the first master node to do the data conversion.
# TODO Can we install Spark on a different node to Yarn ?
#[root@ansibler]

    ssh master01
    
    >   ....
    >   ....


# -----------------------------------------------------
# Start a PySpark session.
#[fedora@master01]

    pyspark

    >   Python 3.7.3 (default, Mar 27 2019, 13:36:35) 
    >   [GCC 9.0.1 20190227 (Red Hat 9.0.1-0.8)] on linux
    >   Type "help", "copyright", "credits" or "license" for more information.
    >   2020-04-20 09:37:33,563 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    >   Setting default log level to "WARN".
    >   To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
    >   2020-04-20 09:37:36,008 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
    >   Welcome to
    >         ____              __
    >        / __/__  ___ _____/ /__
    >       _\ \/ _ \/ _ `/ __/  '_/
    >      /__ / .__/\_,_/_/ /_/\_\   version 3.0.0-preview2
    >         /_/
    >   
    >   Using Python version 3.7.3 (default, Mar 27 2019 13:36:35)
    >   SparkSession available as 'spark'.
    >   >>> 


# -----------------------------------------------------
# Create our Python convert program.
#[pyspark]

import time
import pyspark
import pyspark.sql
import pyspark.conf
import pyspark.context

from pyspark.sql.types import *

schema = StructType(
        [
        StructField("solution_id", LongType(), True),        
        StructField("designation", StringType(), True),
        StructField("source_id", LongType(), True),
        StructField("random_index", LongType(), True),
        StructField("ref_epoch", DoubleType(), True),
        StructField("ra", DoubleType(), True),
        StructField("ra_error", DoubleType(), True),
        StructField("dec", DoubleType(), True),
        StructField("dec_error", DoubleType(), True),
        StructField("parallax", DoubleType(), True),
        StructField("parallax_error", DoubleType(), True),
        StructField("parallax_over_error", FloatType(), True),
        StructField("pmra", DoubleType(), True),
        StructField("pmra_error", DoubleType(), True),
        StructField("pmdec", DoubleType(), True),
        StructField("pmdec_error", DoubleType(), True),
        StructField("ra_dec_corr", FloatType(), True),
        StructField("ra_parallax_corr", FloatType(), True),
        StructField("ra_pmra_corr", FloatType(), True),
        StructField("ra_pmdec_corr", FloatType(), True),
        StructField("dec_parallax_corr", FloatType(), True),
        StructField("dec_pmra_corr", FloatType(), True),
        StructField("dec_pmdec_corr", FloatType(), True),
        StructField("parallax_pmra_corr", FloatType(), True),
        StructField("parallax_pmdec_corr", FloatType(), True),
        StructField("pmra_pmdec_corr", FloatType(), True),
        StructField("astrometric_n_obs_al", IntegerType(), True),
        StructField("astrometric_n_obs_ac", IntegerType(), True),
        StructField("astrometric_n_good_obs_al", IntegerType(), True),
        StructField("astrometric_n_bad_obs_al", IntegerType(), True),
        StructField("astrometric_gof_al", FloatType(), True),
        StructField("astrometric_chi2_al", FloatType(), True),
        StructField("astrometric_excess_noise", DoubleType(), True),
        StructField("astrometric_excess_noise_sig", DoubleType(), True),
        StructField("astrometric_params_solved", ShortType(), True),
        StructField("astrometric_primary_flag", BooleanType(), True),
        StructField("astrometric_weight_al", FloatType(), True),
        StructField("astrometric_pseudo_colour", DoubleType(), True),
        StructField("astrometric_pseudo_colour_error", DoubleType(), True),
        StructField("mean_varpi_factor_al", FloatType(), True),
        StructField("astrometric_matched_observations", DoubleType(), True),
        StructField("visibility_periods_used", ShortType(), True),
        StructField("astrometric_sigma5d_max", FloatType(), True),
        StructField("frame_rotator_object_type", IntegerType(), True),
        StructField("matched_observations", ShortType(), True),
        StructField("duplicated_source", BooleanType(), True),
        StructField("phot_g_n_obs", IntegerType(), True),
        StructField("phot_g_mean_flux", DoubleType(), True),
        StructField("phot_g_mean_flux_error", DoubleType(), True),
        StructField("phot_g_mean_flux_over_error", FloatType(), True),
        StructField("phot_g_mean_mag", FloatType(), True),
        StructField("phot_bp_n_obs", IntegerType(), True),
        StructField("phot_bp_mean_flux", DoubleType(), True),
        StructField("phot_bp_mean_flux_error", DoubleType(), True),
        StructField("phot_bp_mean_flux_over_error", FloatType(), True),
        StructField("phot_bp_mean_mag", FloatType(), True),
        StructField("phot_rp_n_obs", IntegerType(), True),
        StructField("phot_rp_mean_flux", DoubleType(), True),
        StructField("phot_rp_mean_flux_error", DoubleType(), True),
        StructField("phot_rp_mean_flux_over_error", FloatType(), True),
        StructField("phot_rp_mean_mag", FloatType(), True),
        StructField("phot_bp_rp_excess_factor", FloatType(), True),
        StructField("phot_proc_mode", ShortType(), True),
        StructField("bp_rp", FloatType(), True),
        StructField("bp_g", FloatType(), True),
        StructField("g_rp", FloatType(), True),
        StructField("radial_velocity", DoubleType(), True),
        StructField("radial_velocity_error", DoubleType(), True),
        StructField("rv_nb_transits", IntegerType(), True),
        StructField("rv_template_teff", FloatType(), True),
        StructField("rv_template_logg", FloatType(), True),
        StructField("rv_template_fe_h", FloatType(), True),
        StructField("phot_variable_flag", StringType(), True),
        StructField("l", DoubleType(), True),
        StructField("b", DoubleType(), True),
        StructField("ecl_lon", DoubleType(), True),
        StructField("ecl_lat", DoubleType(), True),
        StructField("priam_flags", LongType(), True),
        StructField("teff_val", FloatType(), True),
        StructField("teff_percentile_lower", FloatType(), True),
        StructField("teff_percentile_upper", FloatType(), True),
        StructField("a_g_val", FloatType(), True),
        StructField("a_g_percentile_lower", FloatType(), True),
        StructField("a_g_percentile_upper", FloatType(), True),
        StructField("e_bp_min_rp_val", FloatType(), True),
        StructField("e_bp_min_rp_percentile_lower", FloatType(), True),
        StructField("e_bp_min_rp_percentile_upper", FloatType(), True),
        StructField("flame_flags", LongType(), True),
        StructField("radius_val", FloatType(), True),
        StructField("radius_percentile_lower", FloatType(), True),
        StructField("radius_percentile_upper", FloatType(), True),
        StructField("lum_val", FloatType(), True),
        StructField("lum_percentile_lower", FloatType(), True),
        StructField("lum_percentile_upper", FloatType(), True),
        ]
    )

start = time.time()
print("-- [Staring conversion] --")

dset = spark.read.option(
    "header",
    "true"
    ).schema(
        schema
        ).csv(
            "hdfs://master01:9000/Gaia/gdr2/gaia_source/csv/*"
            )

dset.write.parquet(
    "hdfs://master01:9000/Gaia/gdr2/gaia_source/parquet-01/"
    )


end = time.time()
print("-- [Finished conversion] --")
print(str(end - start) + " seconds taken" )


    >   ....
    >   2020-04-20 09:39:14,687 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
    >   [Stage 1:>                                                      (27 + 2) / 6514]
    >   
    >   ....


# -----------------------------------------------------
# -----------------------------------------------------
# Watch the activity on the worker nodes.
#[user@desktop]

    podman exec -it $(
        podman ps --filter 'name=ansibler' --format "{{.ID}}"
        ) \
        ssh worker01 ... 06

            sudo dnf install -y htop atop
            htop

    #
    # Only worker06 shows any activity, all the others are dormant.
    #

[fedora@aglais-20200417-worker01 ~]$ ps -ef | grep java

    >   fedora   11694     1  0 Apr17 ?        00:14:09 /etc/alternatives/jre/bin/java -Dproc_datanode -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=ERROR,RFAS -Dyarn.log.dir=/var/local/hadoop/logs -Dyarn.log.file=hadoop-fedora-datanode-aglais-20200417-worker01.novalocal.log -Dyarn.home.dir=/opt/hadoop -Dyarn.root.logger=INFO,console -Djava.library.path=/opt/hadoop/lib/native -Dhadoop.log.dir=/var/local/hadoop/logs -Dhadoop.log.file=hadoop-fedora-datanode-aglais-20200417-worker01.novalocal.log -Dhadoop.home.dir=/opt/hadoop -Dhadoop.id.str=fedora -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml org.apache.hadoop.hdfs.server.datanode.DataNode
    >   fedora   11804     1  0 Apr17 ?        00:06:37 /etc/alternatives/jre/bin/java -Dproc_nodemanager -Djava.net.preferIPv4Stack=true -Dyarn.log.dir=/var/local/hadoop/logs -Dyarn.log.file=hadoop-fedora-nodemanager-aglais-20200417-worker01.novalocal.log -Dyarn.home.dir=/opt/hadoop -Dyarn.root.logger=INFO,console -Djava.library.path=/opt/hadoop/lib/native -Dhadoop.log.dir=/var/local/hadoop/logs -Dhadoop.log.file=hadoop-fedora-nodemanager-aglais-20200417-worker01.novalocal.log -Dhadoop.home.dir=/opt/hadoop -Dhadoop.id.str=fedora -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.yarn.server.nodemanager.NodeManager
    >   fedora   29025 28571  0 09:57 pts/0    00:00:00 grep --color=auto java


[fedora@aglais-20200417-worker02 ~]$ ps -ef | grep java

    >   fedora   11677     1  0 Apr17 ?        00:14:07 /etc/alternatives/jre/bin/java -Dproc_datanode -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=ERROR,RFAS -Dyarn.log.dir=/var/local/hadoop/logs -Dyarn.log.file=hadoop-fedora-datanode-aglais-20200417-worker02.novalocal.log -Dyarn.home.dir=/opt/hadoop -Dyarn.root.logger=INFO,console -Djava.library.path=/opt/hadoop/lib/native -Dhadoop.log.dir=/var/local/hadoop/logs -Dhadoop.log.file=hadoop-fedora-datanode-aglais-20200417-worker02.novalocal.log -Dhadoop.home.dir=/opt/hadoop -Dhadoop.id.str=fedora -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml org.apache.hadoop.hdfs.server.datanode.DataNode
    >   fedora   11787     1  0 Apr17 ?        00:06:22 /etc/alternatives/jre/bin/java -Dproc_nodemanager -Djava.net.preferIPv4Stack=true -Dyarn.log.dir=/var/local/hadoop/logs -Dyarn.log.file=hadoop-fedora-nodemanager-aglais-20200417-worker02.novalocal.log -Dyarn.home.dir=/opt/hadoop -Dyarn.root.logger=INFO,console -Djava.library.path=/opt/hadoop/lib/native -Dhadoop.log.dir=/var/local/hadoop/logs -Dhadoop.log.file=hadoop-fedora-nodemanager-aglais-20200417-worker02.novalocal.log -Dhadoop.home.dir=/opt/hadoop -Dhadoop.id.str=fedora -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.yarn.server.nodemanager.NodeManager
    >   fedora   28696 28250  0 09:57 pts/0    00:00:00 grep --color=auto java


[fedora@aglais-20200417-worker03 ~]$ ps -ef | grep java

    >   fedora   11682     1  0 Apr17 ?        00:14:27 /etc/alternatives/jre/bin/java -Dproc_datanode -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=ERROR,RFAS -Dyarn.log.dir=/var/local/hadoop/logs -Dyarn.log.file=hadoop-fedora-datanode-aglais-20200417-worker03.novalocal.log -Dyarn.home.dir=/opt/hadoop -Dyarn.root.logger=INFO,console -Djava.library.path=/opt/hadoop/lib/native -Dhadoop.log.dir=/var/local/hadoop/logs -Dhadoop.log.file=hadoop-fedora-datanode-aglais-20200417-worker03.novalocal.log -Dhadoop.home.dir=/opt/hadoop -Dhadoop.id.str=fedora -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml org.apache.hadoop.hdfs.server.datanode.DataNode
    >   fedora   11792     1  0 Apr17 ?        00:06:14 /etc/alternatives/jre/bin/java -Dproc_nodemanager -Djava.net.preferIPv4Stack=true -Dyarn.log.dir=/var/local/hadoop/logs -Dyarn.log.file=hadoop-fedora-nodemanager-aglais-20200417-worker03.novalocal.log -Dyarn.home.dir=/opt/hadoop -Dyarn.root.logger=INFO,console -Djava.library.path=/opt/hadoop/lib/native -Dhadoop.log.dir=/var/local/hadoop/logs -Dhadoop.log.file=hadoop-fedora-nodemanager-aglais-20200417-worker03.novalocal.log -Dhadoop.home.dir=/opt/hadoop -Dhadoop.id.str=fedora -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.yarn.server.nodemanager.NodeManager
    >   fedora   29096 28660  0 09:57 pts/0    00:00:00 grep --color=auto java


[fedora@aglais-20200417-worker04 ~]$ ps -ef | grep java

    >   fedora   11682     1  0 Apr17 ?        00:13:44 /etc/alternatives/jre/bin/java -Dproc_datanode -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=ERROR,RFAS -Dyarn.log.dir=/var/local/hadoop/logs -Dyarn.log.file=hadoop-fedora-datanode-aglais-20200417-worker04.novalocal.log -Dyarn.home.dir=/opt/hadoop -Dyarn.root.logger=INFO,console -Djava.library.path=/opt/hadoop/lib/native -Dhadoop.log.dir=/var/local/hadoop/logs -Dhadoop.log.file=hadoop-fedora-datanode-aglais-20200417-worker04.novalocal.log -Dhadoop.home.dir=/opt/hadoop -Dhadoop.id.str=fedora -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml org.apache.hadoop.hdfs.server.datanode.DataNode
    >   fedora   11792     1  0 Apr17 ?        00:06:05 /etc/alternatives/jre/bin/java -Dproc_nodemanager -Djava.net.preferIPv4Stack=true -Dyarn.log.dir=/var/local/hadoop/logs -Dyarn.log.file=hadoop-fedora-nodemanager-aglais-20200417-worker04.novalocal.log -Dyarn.home.dir=/opt/hadoop -Dyarn.root.logger=INFO,console -Djava.library.path=/opt/hadoop/lib/native -Dhadoop.log.dir=/var/local/hadoop/logs -Dhadoop.log.file=hadoop-fedora-nodemanager-aglais-20200417-worker04.novalocal.log -Dhadoop.home.dir=/opt/hadoop -Dhadoop.id.str=fedora -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.yarn.server.nodemanager.NodeManager
    >   fedora   28798 28371  0 09:57 pts/0    00:00:00 grep --color=auto java


[fedora@aglais-20200417-worker05 ~]$ ps -ef | grep java

    >   fedora   11683     1  0 Apr17 ?        00:13:28 /etc/alternatives/jre/bin/java -Dproc_datanode -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=ERROR,RFAS -Dyarn.log.dir=/var/local/hadoop/logs -Dyarn.log.file=hadoop-fedora-datanode-aglais-20200417-worker05.novalocal.log -Dyarn.home.dir=/opt/hadoop -Dyarn.root.logger=INFO,console -Djava.library.path=/opt/hadoop/lib/native -Dhadoop.log.dir=/var/local/hadoop/logs -Dhadoop.log.file=hadoop-fedora-datanode-aglais-20200417-worker05.novalocal.log -Dhadoop.home.dir=/opt/hadoop -Dhadoop.id.str=fedora -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml org.apache.hadoop.hdfs.server.datanode.DataNode
    >   fedora   11793     1  0 Apr17 ?        00:06:07 /etc/alternatives/jre/bin/java -Dproc_nodemanager -Djava.net.preferIPv4Stack=true -Dyarn.log.dir=/var/local/hadoop/logs -Dyarn.log.file=hadoop-fedora-nodemanager-aglais-20200417-worker05.novalocal.log -Dyarn.home.dir=/opt/hadoop -Dyarn.root.logger=INFO,console -Djava.library.path=/opt/hadoop/lib/native -Dhadoop.log.dir=/var/local/hadoop/logs -Dhadoop.log.file=hadoop-fedora-nodemanager-aglais-20200417-worker05.novalocal.log -Dhadoop.home.dir=/opt/hadoop -Dhadoop.id.str=fedora -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.yarn.server.nodemanager.NodeManager
    >   fedora   28309 27887  0 09:57 pts/0    00:00:00 grep --color=auto java


[fedora@aglais-20200417-worker06 ~]$ ps -ef | grep java

    >   fedora   11688     1  0 Apr17 ?        00:14:10 /etc/alternatives/jre/bin/java -Dproc_datanode -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=ERROR,RFAS -Dyarn.log.dir=/var/local/hadoop/logs -Dyarn.log.file=hadoop-fedora-datanode-aglais-20200417-worker06.novalocal.log -Dyarn.home.dir=/opt/hadoop -Dyarn.root.logger=INFO,console -Djava.library.path=/opt/hadoop/lib/native -Dhadoop.log.dir=/var/local/hadoop/logs -Dhadoop.log.file=hadoop-fedora-datanode-aglais-20200417-worker06.novalocal.log -Dhadoop.home.dir=/opt/hadoop -Dhadoop.id.str=fedora -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml org.apache.hadoop.hdfs.server.datanode.DataNode
    >   fedora   11798     1  0 Apr17 ?        00:06:26 /etc/alternatives/jre/bin/java -Dproc_nodemanager -Djava.net.preferIPv4Stack=true -Dyarn.log.dir=/var/local/hadoop/logs -Dyarn.log.file=hadoop-fedora-nodemanager-aglais-20200417-worker06.novalocal.log -Dyarn.home.dir=/opt/hadoop -Dyarn.root.logger=INFO,console -Djava.library.path=/opt/hadoop/lib/native -Dhadoop.log.dir=/var/local/hadoop/logs -Dhadoop.log.file=hadoop-fedora-nodemanager-aglais-20200417-worker06.novalocal.log -Dhadoop.home.dir=/opt/hadoop -Dhadoop.id.str=fedora -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.yarn.server.nodemanager.NodeManager
    >   fedora   28667 28665  0 09:37 ?        00:00:00 /bin/bash -c /etc/alternatives/jre/bin/java -server -Xmx1024m -Djava.io.tmpdir=/tmp/hadoop-fedora/nm-local-dir/usercache/fedora/appcache/application_1587087175431_0003/container_1587087175431_0003_01_000002/tmp '-Dspark.driver.port=44439' -Dspark.yarn.app.container.log.dir=/var/local/hadoop/logs/userlogs/application_1587087175431_0003/container_1587087175431_0003_01_000002 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@master01:44439 --executor-id 1 --hostname worker06 --cores 1 --app-id application_1587087175431_0003 --user-class-path file:/tmp/hadoop-fedora/nm-local-dir/usercache/fedora/appcache/application_1587087175431_0003/container_1587087175431_0003_01_000002/__app__.jar 1>/var/local/hadoop/logs/userlogs/application_1587087175431_0003/container_1587087175431_0003_01_000002/stdout 2>/var/local/hadoop/logs/userlogs/application_1587087175431_0003/container_1587087175431_0003_01_000002/stderr
    >   fedora   28678 28667 97 09:37 ?        00:18:36 /etc/alternatives/jre/bin/java -server -Xmx1024m -Djava.io.tmpdir=/tmp/hadoop-fedora/nm-local-dir/usercache/fedora/appcache/application_1587087175431_0003/container_1587087175431_0003_01_000002/tmp -Dspark.driver.port=44439 -Dspark.yarn.app.container.log.dir=/var/local/hadoop/logs/userlogs/application_1587087175431_0003/container_1587087175431_0003_01_000002 -XX:OnOutOfMemoryError=kill %p org.apache.spark.executor.YarnCoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@master01:44439 --executor-id 1 --hostname worker06 --cores 1 --app-id application_1587087175431_0003 --user-class-path file:/tmp/hadoop-fedora/nm-local-dir/usercache/fedora/appcache/application_1587087175431_0003/container_1587087175431_0003_01_000002/__app__.jar
    >   fedora   29436 28974  0 09:56 pts/0    00:00:00 grep --color=auto java


# -----------------------------------------------------
# -----------------------------------------------------
# Ctrl^C the pyspark program.
#[pyspark]

    >   ....
    >   [Stage 1:=>                                                    (138 + 2) / 6514]
    >   ^CTraceback (most recent call last):
    >     File "<stdin>", line 2, in <module>
    >     File "/opt/spark/python/pyspark/sql/readwriter.py", line 879, in parquet
    >       self._jwrite.parquet(path)
    >     File "/opt/spark/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py", line 1284, in __call__
    >     File "/opt/spark/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py", line 1014, in send_command
    >     File "/opt/spark/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py", line 1181, in send_command
    >     File "/usr/lib64/python3.7/socket.py", line 589, in readinto
    >       return self._sock.recv_into(b)
    >     File "/opt/spark/python/pyspark/context.py", line 273, in signal_handler
    >       raise KeyboardInterrupt()
    >   KeyboardInterrupt
    >   
    >   ....
    >   >>> 2020-04-20 10:01:13,397 ERROR datasources.FileFormatWriter: Aborting job 683dbecd-7e38-4990-a584-98dd83c880a5.
    >   org.apache.spark.SparkException: Job 1 cancelled as part of cancellation of all jobs
    >   	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1989)
    >   	at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1924)
    >   	at org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:861)
    >   	at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)
    >   	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
    >   	at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:860)
    >   	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2178)
    >   	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2155)
    >   	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2144)
    >   	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    >   	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:758)
    >   	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2116)
    >   	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)
    >   	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)
    >   	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)
    >   	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)
    >   	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:123)
    >   	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:173)
    >   	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:211)
    >   	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    >   	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:208)
    >   	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:169)
    >   	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:110)
    >   	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:109)
    >   	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:828)
    >   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$4(SQLExecution.scala:100)
    >   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
    >   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:87)
    >   	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:828)
    >   	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:309)
    >   	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
    >   	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:236)
    >   	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:717)
    >   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    >   	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    >   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    >   	at java.lang.reflect.Method.invoke(Method.java:498)
    >   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    >   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    >   	at py4j.Gateway.invoke(Gateway.java:282)
    >   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    >   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
    >   	at py4j.GatewayConnection.run(GatewayConnection.java:238)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   2020-04-20 10:01:13,921 WARN scheduler.TaskSetManager: Lost task 206.0 in stage 1.0 (TID 10138, worker06, executor 1): TaskKilled (Stage cancelled)
    >   2020-04-20 10:01:13,929 WARN scheduler.TaskSetManager: Lost task 213.0 in stage 1.0 (TID 10139, worker07, executor 2): TaskKilled (Stage cancelled)
    >   
    >   Traceback (most recent call last):
    >     File "/opt/spark/python/pyspark/context.py", line 273, in signal_handler
    >       raise KeyboardInterrupt()
    >   KeyboardInterrupt
    >   ....

    #
    # Do we need to split the data before we do the conversion ?
    # Can we do a parallel write ?
    #

    #
    # Turns out the cluster has 8 nodes, and we were only watching the first 6, so we didn't monitor two of the nodes.
    #

# -----------------------------------------------------
# -----------------------------------------------------
# Take a look at the data classes involved ...
#


    >   Welcome to
    >         ____              __
    >        / __/__  ___ _____/ /__
    >       _\ \/ _ \/ _ `/ __/  '_/
    >      /__ / .__/\_,_/_/ /_/\_\   version 3.0.0-preview2
    >         /_/
    >   
    >   Using Python version 3.7.3 (default, Mar 27 2019 13:36:35)
    >   SparkSession available as 'spark'.


        SparkSession spark
        https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/sql/SparkSession.html

    dset = spark.read.option(
        "header",
        "true"
        ).schema(
            schema
            ).csv(
                "hdfs://master01:9000/Gaia/gdr2/gaia_source/csv/*"
                )

    
        spark.read()
        https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/sql/SparkSession.html#read--
            "Returns a DataFrameReader that can be used to read non-streaming data in as a DataFrame."

        spark.read().option(..)
        DataFrameReader.option(..)
        https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/sql/DataFrameReader.html#option-java.lang.String-java.lang.String-
            "Adds an input option for the underlying data source."

        spark.read().option(..).schema(..)
        DataFrameReader.schema(..)
        https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/sql/DataFrameReader.html#schema-org.apache.spark.sql.types.StructType-
            "Specifies the input schema. Some data sources (e.g. JSON) can infer the input schema automatically from data."


        spark.read().option(..).schema(..).csv(..)
        DataFrameReader.csv(..)
        https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/sql/DataFrameReader.html#csv-java.lang.String...-
            "Loads CSV files and returns the result as a DataFrame."


    dset.write.parquet(
        "hdfs://master01:9000/Gaia/gdr2/gaia_source/parquet-01/"
        )

        dset.write()
        https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/sql/Dataset.html#write--
            "Interface for saving the content of the non-streaming Dataset out into external storage."

        dset.write.parquet(..)
        https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/sql/DataFrameWriter.html#parquet-java.lang.String-
            "Saves the content of the DataFrame in Parquet format at the specified path."


# -----------------------------------------------------
# Try a simple count.
#[pyspark]


    dset = spark.read.option(
        "header",
        "true"
        ).schema(
            schema
            ).csv(
                "hdfs://master01:9000/Gaia/gdr2/gaia_source/csv/*"
                )

    dset.count()

    >   [Stage 1:===============>                                     (1847 + 2) / 6514]

    #
    # Taking ages to execute.
    # Only two workers seem to be doing anything ..
    #

    #
    # We have 8 workers, with 2 cores, 6G RAM and 12G disc each.
    # ** Low on cores and memory compared to the current production setup.
    #

    #
    # Settings from Stelios's notes:
    #   20200109-csv-2-parquet-gdr2.txt
    # 
    #   spark-submit --master yarn-client --num-executors 6 --executor-cores 4 --executor-memory 12GB convert.py  > conversion-log.txt &
    #


# -----------------------------------------------------
# Try increase the number of executors
#[fedora@master01]
    
    pyspark \
        --num-executors 8


    >   Python 3.7.3 (default, Mar 27 2019, 13:36:35) 
    >   [GCC 9.0.1 20190227 (Red Hat 9.0.1-0.8)] on linux
    >   Type "help", "copyright", "credits" or "license" for more information.
    >   2020-04-20 13:05:41,559 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    >   Setting default log level to "WARN".
    >   To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
    >   2020-04-20 13:05:43,965 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
    >   Welcome to
    >         ____              __
    >        / __/__  ___ _____/ /__
    >       _\ \/ _ \/ _ `/ __/  '_/
    >      /__ / .__/\_,_/_/ /_/\_\   version 3.0.0-preview2
    >         /_/
    >   
    >   Using Python version 3.7.3 (default, Mar 27 2019 13:36:35)
    >   SparkSession available as 'spark'.


# -----------------------------------------------------
# Load our data and run tests ..
#[pyspark]

import time
import pyspark
import pyspark.sql
import pyspark.conf
import pyspark.context

from pyspark.sql.types import *

schema = StructType(
        [
        StructField("solution_id", LongType(), True),        
        StructField("designation", StringType(), True),
        StructField("source_id", LongType(), True),
        StructField("random_index", LongType(), True),
        StructField("ref_epoch", DoubleType(), True),
        StructField("ra", DoubleType(), True),
        StructField("ra_error", DoubleType(), True),
        StructField("dec", DoubleType(), True),
        StructField("dec_error", DoubleType(), True),
        StructField("parallax", DoubleType(), True),
        StructField("parallax_error", DoubleType(), True),
        StructField("parallax_over_error", FloatType(), True),
        StructField("pmra", DoubleType(), True),
        StructField("pmra_error", DoubleType(), True),
        StructField("pmdec", DoubleType(), True),
        StructField("pmdec_error", DoubleType(), True),
        StructField("ra_dec_corr", FloatType(), True),
        StructField("ra_parallax_corr", FloatType(), True),
        StructField("ra_pmra_corr", FloatType(), True),
        StructField("ra_pmdec_corr", FloatType(), True),
        StructField("dec_parallax_corr", FloatType(), True),
        StructField("dec_pmra_corr", FloatType(), True),
        StructField("dec_pmdec_corr", FloatType(), True),
        StructField("parallax_pmra_corr", FloatType(), True),
        StructField("parallax_pmdec_corr", FloatType(), True),
        StructField("pmra_pmdec_corr", FloatType(), True),
        StructField("astrometric_n_obs_al", IntegerType(), True),
        StructField("astrometric_n_obs_ac", IntegerType(), True),
        StructField("astrometric_n_good_obs_al", IntegerType(), True),
        StructField("astrometric_n_bad_obs_al", IntegerType(), True),
        StructField("astrometric_gof_al", FloatType(), True),
        StructField("astrometric_chi2_al", FloatType(), True),
        StructField("astrometric_excess_noise", DoubleType(), True),
        StructField("astrometric_excess_noise_sig", DoubleType(), True),
        StructField("astrometric_params_solved", ShortType(), True),
        StructField("astrometric_primary_flag", BooleanType(), True),
        StructField("astrometric_weight_al", FloatType(), True),
        StructField("astrometric_pseudo_colour", DoubleType(), True),
        StructField("astrometric_pseudo_colour_error", DoubleType(), True),
        StructField("mean_varpi_factor_al", FloatType(), True),
        StructField("astrometric_matched_observations", DoubleType(), True),
        StructField("visibility_periods_used", ShortType(), True),
        StructField("astrometric_sigma5d_max", FloatType(), True),
        StructField("frame_rotator_object_type", IntegerType(), True),
        StructField("matched_observations", ShortType(), True),
        StructField("duplicated_source", BooleanType(), True),
        StructField("phot_g_n_obs", IntegerType(), True),
        StructField("phot_g_mean_flux", DoubleType(), True),
        StructField("phot_g_mean_flux_error", DoubleType(), True),
        StructField("phot_g_mean_flux_over_error", FloatType(), True),
        StructField("phot_g_mean_mag", FloatType(), True),
        StructField("phot_bp_n_obs", IntegerType(), True),
        StructField("phot_bp_mean_flux", DoubleType(), True),
        StructField("phot_bp_mean_flux_error", DoubleType(), True),
        StructField("phot_bp_mean_flux_over_error", FloatType(), True),
        StructField("phot_bp_mean_mag", FloatType(), True),
        StructField("phot_rp_n_obs", IntegerType(), True),
        StructField("phot_rp_mean_flux", DoubleType(), True),
        StructField("phot_rp_mean_flux_error", DoubleType(), True),
        StructField("phot_rp_mean_flux_over_error", FloatType(), True),
        StructField("phot_rp_mean_mag", FloatType(), True),
        StructField("phot_bp_rp_excess_factor", FloatType(), True),
        StructField("phot_proc_mode", ShortType(), True),
        StructField("bp_rp", FloatType(), True),
        StructField("bp_g", FloatType(), True),
        StructField("g_rp", FloatType(), True),
        StructField("radial_velocity", DoubleType(), True),
        StructField("radial_velocity_error", DoubleType(), True),
        StructField("rv_nb_transits", IntegerType(), True),
        StructField("rv_template_teff", FloatType(), True),
        StructField("rv_template_logg", FloatType(), True),
        StructField("rv_template_fe_h", FloatType(), True),
        StructField("phot_variable_flag", StringType(), True),
        StructField("l", DoubleType(), True),
        StructField("b", DoubleType(), True),
        StructField("ecl_lon", DoubleType(), True),
        StructField("ecl_lat", DoubleType(), True),
        StructField("priam_flags", LongType(), True),
        StructField("teff_val", FloatType(), True),
        StructField("teff_percentile_lower", FloatType(), True),
        StructField("teff_percentile_upper", FloatType(), True),
        StructField("a_g_val", FloatType(), True),
        StructField("a_g_percentile_lower", FloatType(), True),
        StructField("a_g_percentile_upper", FloatType(), True),
        StructField("e_bp_min_rp_val", FloatType(), True),
        StructField("e_bp_min_rp_percentile_lower", FloatType(), True),
        StructField("e_bp_min_rp_percentile_upper", FloatType(), True),
        StructField("flame_flags", LongType(), True),
        StructField("radius_val", FloatType(), True),
        StructField("radius_percentile_lower", FloatType(), True),
        StructField("radius_percentile_upper", FloatType(), True),
        StructField("lum_val", FloatType(), True),
        StructField("lum_percentile_lower", FloatType(), True),
        StructField("lum_percentile_upper", FloatType(), True),
        ]
    )

start = time.time()
print("-- [Staring conversion] --")

dset = spark.read.option(
    "header",
    "true"
    ).schema(
        schema
        ).csv(
            "hdfs://master01:9000/Gaia/gdr2/gaia_source/csv/*"
            )

dset.count()

    #
    # Faster to execute.
    # All eight workers are involved .. still slow though.
    # -- because it is doing the test to number conversion ?
    # -- will it cache the numeric version ?
    #

    >   2020-04-20 13:09:49,892 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
    >   1692919135                       

Date/time afterwards Mon 20 Apr 13:44:13 UTC 2020
Suggests ~40min to perform the count.


dset.count()

    #
    # Same speed.
    # So it doesn't cache the data.
    #

    >   [Stage 3:==>                                                   (251 + 8) / 6514]
    >   ^C
    >   Traceback (most recent call last):
    >     File "<stdin>", line 1, in <module>
    >     File "/opt/spark/python/pyspark/sql/dataframe.py", line 552, in count
    >       return int(self._jdf.count())
    >     File "/opt/spark/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py", line 1284, in __call__
    >     File "/opt/spark/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py", line 1014, in send_command
    >     File "/opt/spark/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py", line 1181, in send_command
    >     File "/usr/lib64/python3.7/socket.py", line 589, in readinto
    >       return self._sock.recv_into(b)
    >     File "/opt/spark/python/pyspark/context.py", line 273, in signal_handler
    >       raise KeyboardInterrupt()
    >   KeyboardInterrupt
    >   >>> 2020-04-20 13:46:13,561 WARN scheduler.TaskSetManager: Lost task 259.0 in stage 3.0 (TID 16770, worker02, executor 5): TaskKilled (Stage cancelled)
    >   2020-04-20 13:46:13,562 WARN scheduler.TaskSetManager: Lost task 257.0 in stage 3.0 (TID 16773, worker03, executor 7): TaskKilled (Stage cancelled)
    >   2020-04-20 13:46:13,562 WARN scheduler.TaskSetManager: Lost task 248.0 in stage 3.0 (TID 16768, worker04, executor 1): TaskKilled (Stage cancelled)
    >   2020-04-20 13:46:13,562 WARN scheduler.TaskSetManager: Lost task 255.0 in stage 3.0 (TID 16772, worker05, executor 8): TaskKilled (Stage cancelled)
    >   2020-04-20 13:46:13,562 WARN scheduler.TaskSetManager: Lost task 252.0 in stage 3.0 (TID 16767, worker06, executor 6): TaskKilled (Stage cancelled)
    >   2020-04-20 13:46:13,563 WARN scheduler.TaskSetManager: Lost task 256.0 in stage 3.0 (TID 16771, worker01, executor 4): TaskKilled (Stage cancelled)
    >   2020-04-20 13:46:13,564 WARN scheduler.TaskSetManager: Lost task 253.0 in stage 3.0 (TID 16769, worker07, executor 3): TaskKilled (Stage cancelled)
    >   2020-04-20 13:46:13,568 WARN scheduler.TaskSetManager: Lost task 247.0 in stage 3.0 (TID 16766, worker08, executor 2): TaskKilled (Stage cancelled)

    #
    # Theory is that the parsing of csv to numeric is taling time, not the acutal count() operation.
    # In which case, the Parquet write should take a similar amount of time.
    #

import datetime    
print(
    datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    )
dset.count()
print(
    datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    )

    >   2020-04-20 13:52:07
    >   1692919135                                                                      
    >   2020-04-20 14:23:45

print(
    datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    )
dset.write.parquet(
    "hdfs://master01:9000/Gaia/gdr2/gaia_source/parquet-01/"
    )
print(
    datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    )


    >   2020-04-20 14:26:55
    >   ....
    >   2020-04-20 18:13:11


# -----------------------------------------------------
# -----------------------------------------------------
# Navigate to the Yarn status pages.
#[firefox-gui]


    #
    # Yarn cluster
    http://master01:8088/

    # Yarn RUNNING
    http://master01:8088/cluster/apps/running

    # PySpark app ..
    http://master01:8088/cluster/app/application_1587087175431_0005

    # Worker node app for the PySpark job
    http://worker03:8042/node/application/application_1587087175431_0005    

    # Two containers.
    http://worker03:8042/node/container/container_1587087175431_0005_01_000001
    http://worker03:8042/node/container/container_1587087175431_0005_01_000008
    
    
    # First worker container is the PySpark application itself.
    http://worker03:8042/node/container/container_1587087175431_0005_01_000001

    >   2020-04-20 13:05:54,529 INFO conf.Configuration: resource-types.xml not found
    >   2020-04-20 13:05:54,530 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
    >   2020-04-20 13:05:54,549 INFO yarn.YarnAllocator: Will request 8 executor container(s), each with 1 core(s) and 1408 MB memory (including 384 MB of overhead)
    >   2020-04-20 13:05:54,584 INFO yarn.YarnAllocator: Submitted 8 unlocalized container requests.
    >   2020-04-20 13:05:54,802 INFO yarn.ApplicationMaster: Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals
    >   2020-04-20 13:05:54,832 INFO yarn.YarnAllocator: Launching container container_1587087175431_0005_01_000002 on host worker04 for executor with ID 1
    >   2020-04-20 13:05:54,834 INFO yarn.YarnAllocator: Launching container container_1587087175431_0005_01_000003 on host worker08 for executor with ID 2
    >   2020-04-20 13:05:54,838 INFO yarn.YarnAllocator: Launching container container_1587087175431_0005_01_000004 on host worker07 for executor with ID 3
    >   2020-04-20 13:05:54,848 INFO yarn.YarnAllocator: Launching container container_1587087175431_0005_01_000005 on host worker01 for executor with ID 4
    >   2020-04-20 13:05:54,859 INFO yarn.YarnAllocator: Received 4 containers from YARN, launching executors on 4 of them.
    >   2020-04-20 13:05:55,068 INFO yarn.YarnAllocator: Launching container container_1587087175431_0005_01_000006 on host worker02 for executor with ID 5
    >   2020-04-20 13:05:55,068 INFO yarn.YarnAllocator: Launching container container_1587087175431_0005_01_000007 on host worker06 for executor with ID 6
    >   2020-04-20 13:05:55,070 INFO yarn.YarnAllocator: Received 2 containers from YARN, launching executors on 2 of them.
    >   2020-04-20 13:05:56,284 INFO yarn.YarnAllocator: Launching container container_1587087175431_0005_01_000008 on host worker03 for executor with ID 7
    >   2020-04-20 13:05:56,285 INFO yarn.YarnAllocator: Launching container container_1587087175431_0005_01_000009 on host worker05 for executor with ID 8
    >   2020-04-20 13:05:56,286 INFO yarn.YarnAllocator: Received 2 containers from YARN, launching executors on 2 of them.
    >   ....
    >   ....


    # Second worker container is a convert job ..
    # http://worker03:8042/node/container/container_1587087175431_0005_01_000008
    # http://worker03:8042/node/containerlogs/container_1587087175431_0005_01_000008/fedora/stderr/?start=0

    >   SLF4J: Class path contains multiple SLF4J bindings.
    >   SLF4J: Found binding in [jar:file:/tmp/hadoop-fedora/nm-local-dir/usercache/fedora/filecache/12/__spark_libs__1471520674222387114.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    >   SLF4J: Found binding in [jar:file:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    >   SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
    >   SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
    >   2020-04-20 13:05:57,390 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 29723@aglais-20200417-worker03.novalocal
    >   2020-04-20 13:05:57,399 INFO util.SignalUtils: Registered signal handler for TERM
    >   2020-04-20 13:05:57,399 INFO util.SignalUtils: Registered signal handler for HUP
    >   2020-04-20 13:05:57,399 INFO util.SignalUtils: Registered signal handler for INT
    >   ....
    >   ....
    >   2020-04-20 13:05:58,762 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fedora); groups with view permissions: Set(); users  with modify permissions: Set(fedora); groups with modify permissions: Set()
    >   2020-04-20 13:05:58,825 INFO client.TransportClientFactory: Successfully created connection to master01/10.10.0.5:43185 after 2 ms (0 ms spent in bootstraps)
    >   2020-04-20 13:05:58,880 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-fedora/nm-local-dir/usercache/fedora/appcache/application_1587087175431_0005/blockmgr-178b163d-761f-4084-8eb6-3bded177994b
    >   2020-04-20 13:05:58,918 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MiB
    >   2020-04-20 13:05:59,240 INFO executor.YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@master01:43185
    >   2020-04-20 13:05:59,362 INFO executor.YarnCoarseGrainedExecutorBackend: Successfully registered with driver
    >   2020-04-20 13:05:59,367 INFO executor.Executor: Starting executor ID 7 on host worker03
    >   2020-04-20 13:05:59,454 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39681.
    >   2020-04-20 13:05:59,455 INFO netty.NettyBlockTransferService: Server created on worker03:39681
    >   2020-04-20 13:05:59,456 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
    >   2020-04-20 13:05:59,477 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(7, worker03, 39681, None)
    >   2020-04-20 13:05:59,488 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(7, worker03, 39681, None)
    >   2020-04-20 13:05:59,490 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(7, worker03, 39681, None)
    >   2020-04-20 13:07:51,941 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 5
    >   2020-04-20 13:07:51,949 INFO executor.Executor: Running task 5.0 in stage 0.0 (TID 5)
    >   2020-04-20 13:07:52,045 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
    >   2020-04-20 13:07:52,205 INFO client.TransportClientFactory: Successfully created connection to master01/10.10.0.5:43169 after 18 ms (0 ms spent in bootstraps)
    >   2020-04-20 13:07:52,248 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 366.3 MiB)
    >   2020-04-20 13:07:52,261 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 216 ms
    >   2020-04-20 13:07:52,366 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 85.0 KiB, free 366.2 MiB)
    >   2020-04-20 13:07:53,507 INFO executor.Executor: Finished task 5.0 in stage 0.0 (TID 5). 4071 bytes result sent to driver
    >   2020-04-20 13:07:53,517 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 11
    >   2020-04-20 13:07:53,518 INFO executor.Executor: Running task 11.0 in stage 0.0 (TID 11)
    >   2020-04-20 13:07:53,578 INFO executor.Executor: Finished task 11.0 in stage 0.0 (TID 11). 3966 bytes result sent to driver
    >   2020-04-20 13:07:53,584 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 20
    >   2020-04-20 13:07:53,585 INFO executor.Executor: Running task 20.0 in stage 0.0 (TID 20)
    >   ....
    >   ....
    >   2020-04-20 14:23:42,408 INFO v2.FilePartitionReader: Reading file path: hdfs://master01:9000/Gaia/gdr2/gaia_source/csv/GaiaSource_4050880834235163264_4050881835020448000.csv.gz, range: 0-1607306, partition values: [empty row]
    >   2020-04-20 14:23:42,435 INFO compress.CodecPool: Got brand-new decompressor [.gz]
    >   2020-04-20 14:23:42,446 INFO v2.FilePartitionReader: Reading file path: hdfs://master01:9000/Gaia/gdr2/gaia_source/csv/GaiaSource_4050406704212123776_4050407636315056000.csv.gz, range: 0-1607261, partition values: [empty row]
    >   2020-04-20 14:23:42,488 INFO compress.CodecPool: Got brand-new decompressor [.gz]
    >   2020-04-20 14:23:42,501 INFO v2.FilePartitionReader: Reading file path: hdfs://master01:9000/Gaia/gdr2/gaia_source/csv/GaiaSource_4050415981341613952_4050416947809097088.csv.gz, range: 0-1607148, partition values: [empty row]
    >   2020-04-20 14:23:42,552 INFO executor.Executor: Finished task 6501.0 in stage 5.0 (TID 23278). 2130 bytes result sent to driver
    >   2020-04-20 14:23:42,554 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 23285
    >   2020-04-20 14:23:42,554 INFO executor.Executor: Running task 6512.0 in stage 5.0 (TID 23285)
    >   2020-04-20 14:23:42,558 INFO compress.CodecPool: Got brand-new decompressor [.gz]
    >   2020-04-20 14:23:42,571 INFO v2.FilePartitionReader: Reading file path: hdfs://master01:9000/Gaia/gdr2/gaia_source/csv/GaiaSource_4062836476999815040_4062837305987184640.csv.gz, range: 0-1440135, partition values: [empty row]
    >   2020-04-20 14:23:42,620 INFO compress.CodecPool: Got brand-new decompressor [.gz]
    >   2020-04-20 14:23:42,858 INFO v2.FilePartitionReader: Reading file path: hdfs://master01:9000/Gaia/gdr2/gaia_source/csv/GaiaSource_4063028376140826880_4063029273863172736.csv.gz, range: 0-1436879, partition values: [empty row]
    >   2020-04-20 14:23:42,939 INFO compress.CodecPool: Got brand-new decompressor [.gz]
    >   2020-04-20 14:23:42,961 INFO v2.FilePartitionReader: Reading file path: hdfs://master01:9000/Gaia/gdr2/gaia_source/csv/GaiaSource_4062317851110413952_4062318748831481600.csv.gz, range: 0-1433017, partition values: [empty row]
    >   2020-04-20 14:23:43,004 INFO compress.CodecPool: Got brand-new decompressor [.gz]
    >   ....
    >   ....
    >   2020-04-20 14:23:44,368 INFO v2.FilePartitionReader: Reading file path: hdfs://master01:9000/Gaia/gdr2/gaia_source/csv/GaiaSource_4063041879514916864_4063042845967683072.csv.gz, range: 0-1361748, partition values: [empty row]
    >   2020-04-20 14:23:44,412 INFO compress.CodecPool: Got brand-new decompressor [.gz]
    >   2020-04-20 14:23:44,445 INFO v2.FilePartitionReader: Reading file path: hdfs://master01:9000/Gaia/gdr2/gaia_source/csv/GaiaSource_4062365679868392960_4062366543250195328.csv.gz, range: 0-1359127, partition values: [empty row]
    >   2020-04-20 14:23:44,468 INFO compress.CodecPool: Got brand-new decompressor [.gz]
    >   2020-04-20 14:23:44,526 INFO v2.FilePartitionReader: Reading file path: hdfs://master01:9000/Gaia/gdr2/gaia_source/csv/GaiaSource_4062462849204892288_4062463678212162688.csv.gz, range: 0-1348139, partition values: [empty row]
    >   2020-04-20 14:23:44,564 INFO executor.Executor: Finished task 6512.0 in stage 5.0 (TID 23285). 2130 bytes result sent to driver
    >   2020-04-20 14:26:57,452 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 23290
    >   2020-04-20 14:26:57,453 INFO executor.Executor: Running task 4.0 in stage 7.0 (TID 23290)
    >   2020-04-20 14:26:57,457 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache
    >   2020-04-20 14:26:57,459 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 14 with 1 pieces (estimated total size 4.0 MiB)
    >   2020-04-20 14:26:57,470 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 365.8 MiB)
    >   2020-04-20 14:26:57,472 INFO broadcast.TorrentBroadcast: Reading broadcast variable 14 took 13 ms
    >   2020-04-20 14:26:57,474 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 222.2 KiB, free 365.5 MiB)
    >   2020-04-20 14:26:57,780 INFO codegen.CodeGenerator: Code generated in 221.414383 ms
    >   2020-04-20 14:26:57,798 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
    >   2020-04-20 14:26:57,798 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
    >   2020-04-20 14:26:57,800 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
    >   2020-04-20 14:26:57,800 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
    >   2020-04-20 14:26:57,800 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
    >   2020-04-20 14:26:57,800 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
    >   2020-04-20 14:26:57,801 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 13 with 1 pieces (estimated total size 4.0 MiB)
    >   2020-04-20 14:26:57,806 INFO client.TransportClientFactory: Successfully created connection to worker01/10.10.0.10:43831 after 3 ms (0 ms spent in bootstraps)
    >   2020-04-20 14:26:57,809 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 28.3 KiB, free 365.5 MiB)
    >   2020-04-20 14:26:57,810 INFO broadcast.TorrentBroadcast: Reading broadcast variable 13 took 9 ms
    >   2020-04-20 14:26:57,817 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 427.0 KiB, free 365.1 MiB)
    >   2020-04-20 14:26:57,831 INFO compress.CodecPool: Got brand-new decompressor [.gz]
    >   2020-04-20 14:26:58,100 INFO codegen.CodeGenerator: Code generated in 103.390591 ms
    >   2020-04-20 14:26:58,106 INFO v2.FilePartitionReader: Reading file path: hdfs://master01:9000/Gaia/gdr2/gaia_source/csv/GaiaSource_4662175008261389568_4662252081483882880.csv.gz, range: 0-37384810, partition values: [empty row]
    >   2020-04-20 14:26:58,131 INFO codec.CodecConfig: Compression: SNAPPY
    >   2020-04-20 14:26:58,134 INFO codec.CodecConfig: Compression: SNAPPY
    >   2020-04-20 14:26:58,167 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
    >   2020-04-20 14:26:58,167 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
    >   2020-04-20 14:26:58,167 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
    >   2020-04-20 14:26:58,167 INFO hadoop.ParquetOutputFormat: Dictionary is on
    >   2020-04-20 14:26:58,167 INFO hadoop.ParquetOutputFormat: Validation is off
    >   2020-04-20 14:26:58,167 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
    >   2020-04-20 14:26:58,167 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
    >   2020-04-20 14:26:58,167 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
    >   2020-04-20 14:26:58,167 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
    >   2020-04-20 14:26:58,167 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
    >   2020-04-20 14:26:58,301 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
    >   {
    >     "type" : "struct",
    >     "fields" : [ {
    >   ....
    >   ....
    >     }, {
    >       "name" : "lum_percentile_upper",
    >       "type" : "float",
    >       "nullable" : true,
    >       "metadata" : { }
    >     } ]
    >   }
    >   and corresponding Parquet message type:
    >   message spark_schema {
    >     optional int64 solution_id;
    >     optional binary designation (UTF8);
    >     optional int64 source_id;
    >     optional int64 random_index;  
    >   ....
    >   ....
    >   
    >     optional float radius_percentile_upper;
    >     optional float lum_val;
    >     optional float lum_percentile_lower;
    >     optional float lum_percentile_upper;
    >   }
    >   ....
    >   ....
    >   2020-04-20 14:26:58,368 INFO compress.CodecPool: Got brand-new compressor [.snappy]
    >   2020-04-20 14:27:09,294 INFO compress.CodecPool: Got brand-new decompressor [.gz]
    >   2020-04-20 14:27:09,350 INFO v2.FilePartitionReader: Reading file path: hdfs://master01:9000/Gaia/gdr2/gaia_source/csv/GaiaSource_4051060363868841728_4051082392898954880.csv.gz, range: 0-37383771, partition values: [empty row]
    >   2020-04-20 14:27:20,314 INFO compress.CodecPool: Got brand-new decompressor [.gz]
    >   2020-04-20 14:27:20,413 INFO v2.FilePartitionReader: Reading file path: hdfs://master01:9000/Gaia/gdr2/gaia_source/csv/GaiaSource_4038718345850324736_4038741336938327168.csv.gz, range: 0-37317322, partition values: [empty row]
    >   2020-04-20 14:27:27,141 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 113761286
    >   2020-04-20 14:27:28,138 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200420142657_0007_m_000004_23290' to hdfs://master01:9000/Gaia/gdr2/gaia_source/parquet-01
    >   2020-04-20 14:27:28,138 INFO mapred.SparkHadoopMapRedUtil: attempt_20200420142657_0007_m_000004_23290: Committed
    >   2020-04-20 14:27:28,143 INFO executor.Executor: Finished task 4.0 in stage 7.0 (TID 23290). 2322 bytes result sent to driver
    >   2020-04-20 14:27:28,146 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 23300
    >   2020-04-20 14:27:28,147 INFO executor.Executor: Running task 15.0 in stage 7.0 (TID 23300)
    >   2020-04-20 14:27:28,154 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
    >   2020-04-20 14:27:28,154 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
    >   2020-04-20 14:27:28,155 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
    >   2020-04-20 14:27:28,155 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
    >   2020-04-20 14:27:28,155 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
    >   2020-04-20 14:27:28,155 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
    >   2020-04-20 14:27:28,157 INFO compress.CodecPool: Got brand-new decompressor [.gz]
    >   2020-04-20 14:27:28,504 INFO v2.FilePartitionReader: Reading file path: hdfs://master01:9000/Gaia/gdr2/gaia_source/csv/GaiaSource_4039600703928162688_4039623626293417216.csv.gz, range: 0-34997968, partition values: [empty row]
    >   2020-04-20 14:27:28,504 INFO codec.CodecConfig: Compression: SNAPPY
    >   2020-04-20 14:27:28,505 INFO codec.CodecConfig: Compression: SNAPPY
    >   2020-04-20 14:27:28,505 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
    >   2020-04-20 14:27:28,505 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
    >   2020-04-20 14:27:28,505 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
    >   2020-04-20 14:27:28,505 INFO hadoop.ParquetOutputFormat: Dictionary is on
    >   2020-04-20 14:27:28,505 INFO hadoop.ParquetOutputFormat: Validation is off
    >   2020-04-20 14:27:28,505 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
    >   2020-04-20 14:27:28,505 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
    >   2020-04-20 14:27:28,505 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
    >   2020-04-20 14:27:28,505 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
    >   2020-04-20 14:27:28,505 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
    >   ....
    >   ....
    >   2020-04-20 14:59:47,633 INFO compress.CodecPool: Got brand-new decompressor [.gz]
    >   2020-04-20 14:59:47,793 INFO v2.FilePartitionReader: Reading file path: hdfs://master01:9000/Gaia/gdr2/gaia_source/csv/GaiaSource_6032914473616255744_6032951483387548416.csv.gz, range: 0-22455111, partition values: [empty row]
    >   2020-04-20 14:59:51,465 INFO compress.CodecPool: Got brand-new decompressor [.gz]
    >   2020-04-20 14:59:51,522 INFO v2.FilePartitionReader: Reading file path: hdfs://master01:9000/Gaia/gdr2/gaia_source/csv/GaiaSource_4314839044527901952_4314877050750756224.csv.gz, range: 0-22453538, partition values: [empty row]
    >   2020-04-20 14:59:55,289 INFO compress.CodecPool: Got brand-new decompressor [.gz]
    >   2020-04-20 14:59:55,337 INFO v2.FilePartitionReader: Reading file path: hdfs://master01:9000/Gaia/gdr2/gaia_source/csv/GaiaSource_5933793809607670528_5933809894356006656.csv.gz, range: 0-22451658, partition values: [empty row]
    >   ....
    >   ....


# -----------------------------------------------------
# Start a new PyS[ark session
#[fedora@master01]
    
    pyspark \
        --num-executors 8


    >   Python 3.7.3 (default, Mar 27 2019, 13:36:35) 
    >   [GCC 9.0.1 20190227 (Red Hat 9.0.1-0.8)] on linux
    >   Type "help", "copyright", "credits" or "license" for more information.
    >   2020-04-20 18:49:45,029 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    >   Setting default log level to "WARN".
    >   To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
    >   2020-04-20 18:49:47,506 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
    >   Welcome to
    >         ____              __
    >        / __/__  ___ _____/ /__
    >       _\ \/ _ \/ _ `/ __/  '_/
    >      /__ / .__/\_,_/_/ /_/\_\   version 3.0.0-preview2
    >         /_/
    >   
    >   Using Python version 3.7.3 (default, Mar 27 2019 13:36:35)
    >   SparkSession available as 'spark'.


# -----------------------------------------------------
# Load our data and run tests ..
#[pyspark]

import time
import datetime
import pyspark
import pyspark.sql
import pyspark.conf
import pyspark.context


print(
    datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    )
dset = spark.read.parquet(
    "hdfs://master01:9000/Gaia/gdr2/gaia_source/parquet-01/*"
    )
print(
    datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    )

    >   2020-04-20 18:51:18
    >   ....
    >   ....
    >   2020-04-20 18:51:32



print(
    datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    )
dset.count()
print(
    datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    )

    >   2020-04-20 18:52:26
    >   ....
    >   1692919135
    >   ....
    >   2020-04-20 18:53:17


print(
    datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    )
dset.count()
print(
    datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    )

    >   2020-04-20 18:54:08
    >   ....
    >   1692919135
    >   ....
    >   2020-04-20 18:54:31


2020-04-21 01:26:04
....
1692919135                                                                      
....
2020-04-21 01:26:21


