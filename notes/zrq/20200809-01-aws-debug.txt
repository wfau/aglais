#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

# -----------------------------------------------------
# -----------------------------------------------------

    Debugging line at a time in Eclipse, managed to 'see' the XML we are getting back from Swift,
    and it doesn't match the S3 schema.

    https://docs.openstack.org/swift/latest/s3_compat.html


        Getting (Swift XML)
        https://opendev.org/openstack/swift/src/branch/master/swift/common/middleware/listing_formats.py#L57
        https://opendev.org/openstack/swift/src/branch/master/swift/common/middleware/listing_formats.py#L92

            <container>
                <object>
                    <name>
                    <hash>
                    <bytes>
                    <content_type>
                    <last_modified>

        Expecting (S3 XML)
        https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjectsV2.html

            <ListBucketResult>
               <IsTruncated>boolean</IsTruncated>
               <Contents>
                  <ETag>string</ETag>
                  <Key>string</Key>
                  <LastModified>timestamp</LastModified>
                  <Owner>
                     <DisplayName>string</DisplayName>
                     <ID>string</ID>
                  </Owner>
                  <Size>integer</Size>
                  <StorageClass>string</StorageClass>
               </Contents>


    Turns out Swift has two modes.
    Swift API, that can produce text, JSON and XML.

    Turns out that the Amazon code just skips things it doesn't recognise.
    So even though the XML comming back from Swift means nothing to the S3 parser,
    it just fails silently and returns an empty list.

    Need to find the selector in Swift that chooses which XML format to return.
    Possibly a different endpoint path ?

# -----------------------------------------------------
# Checkout a copy of the Swift source code.
#[user@desktop]

    source "${HOME}/aglais.env"
    pushd "${AGLAIS_HOME}"

        mkdir 'external'
        pushd 'external'

            mkdir 'openstack'
            pushd 'openstack'

                git clone 'https://opendev.org/openstack/swift.git'

            popd
        popd
    popd


# -----------------------------------------------------
# -----------------------------------------------------

    The S3 API is provided by a proxy handler in front of the Swift API?
    https://docs.openstack.org/swift/latest/overview_architecture.html#proxy-server

    Proxy config
    /home/Zarquan/Desktop/projects/WFAU/aglais/external/openstack/swift/doc/saio/swift/proxy-server.conf
        default port is 8080
        nothing listening on port 8080 on cululus

    Proxy on a different port doesn't make sense.
    The 's3cmd' client works fine using port '6780'.

    S3 XML schema
    openstack/swift/doc/s3api/rnc/list_bucket_result.rnc

    Still trying to figure out how the request handling works in Swift source code.
    ....


# -----------------------------------------------------
# -----------------------------------------------------
# Try another 3rd party S3 client.
# https://boto3.amazonaws.com/v1/documentation/api/latest/guide/clients.html
# https://stackoverflow.com/questions/56430101/how-to-connect-to-openstack-swift-using-boto3
# https://stackoverflow.com/a/60566758
#[user@desktop]

    podman run \
        --rm -it \
        fedora \
            bash

        dnf -y install python3-pip

        pip3 install boto3

        python3


            import boto3
            import botocore

            session = boto3.session.Session()

            boto3.set_stream_logger(name='botocore')

            client = session.client(
                service_name='s3',
                region_name='US',
                endpoint_url='https://cumulus.openstack.hpc.cam.ac.uk:6780/swift/v1/AUTH_21b4ae3a2ea44bc5a9c14005ed2963af',
                aws_access_key_id='3367....0df9',
                aws_secret_access_key='4034....aea0',
                )

            client.list_objects(Bucket='albert')

    #
    # Ends up stuck in an infinite loop and runs out out stack space.
    # Repeatedly sending HEAD requests and then deciding to retyr.

    >   ....
    >   ....
    >   2020-08-10 02:19:06,018 botocore.endpoint [DEBUG] Making request for OperationModel(name=HeadBucket) with params: {'url_path': '/albert', 'query_string': {}, 'method': 'HEAD', 'headers': {'User-Agent': 'Boto3/1.14.38 Python/3.8.3 Linux/5.7.9-100.fc31.x86_64 Botocore/1.17.38'}, 'body': b'', 'url': 'https://cumulus.openstack.hpc.cam.ac.uk:6780/swift/v1/AUTH_21b4ae3a2ea44bc5a9c14005ed2963af/albert', 'context': {'client_region': 'US', 'client_config': <botocore.config.Config object at 0x7ffb6264cd60>, 'has_streaming_input': False, 'auth_type': None, 'signing': {'bucket': 'albert'}}}
    >   2020-08-10 02:19:06,019 botocore.hooks [DEBUG] Event request-created.s3.HeadBucket: calling handler <bound method RequestSigner.handler of <botocore.signers.RequestSigner object at 0x7ffb6264c370>>
    >   2020-08-10 02:19:06,019 botocore.hooks [DEBUG] Event choose-signer.s3.HeadBucket: calling handler <function set_operation_specific_signer at 0x7ffb62a70280>
    >   2020-08-10 02:19:06,019 botocore.hooks [DEBUG] Event before-sign.s3.HeadBucket: calling handler <bound method S3EndpointSetter.set_endpoint of <botocore.utils.S3EndpointSetter object at 0x7ffb62412610>>
    >   2020-08-10 02:19:06,019 botocore.auth [DEBUG] Calculating signature using v4 auth.
    >   2020-08-10 02:19:06,019 botocore.auth [DEBUG] CanonicalRequest:
    >   HEAD
    >   /swift/v1/AUTH_21b4ae3a2ea44bc5a9c14005ed2963af/albert
    >
    >   host:cumulus.openstack.hpc.cam.ac.uk:6780
    >   x-amz-content-sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
    >   x-amz-date:20200810T021906Z
    >
    >   host;x-amz-content-sha256;x-amz-date
    >   e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
    >   2020-08-10 02:19:06,019 botocore.auth [DEBUG] StringToSign:
    >   AWS4-HMAC-SHA256
    >   20200810T021906Z
    >   20200810/US/s3/aws4_request
    >   7415d3e87c8b14c3a95b1d3cd880875ea62f7c89f367a622a609fbacca5d6285
    >   2020-08-10 02:19:06,020 botocore.auth [DEBUG] Signature:
    >   40ca07263e0ddd5fc04d9982cc1b3fa7825c8a084fff35dcd4173cb8a24666ee
    >   2020-08-10 02:19:06,020 botocore.endpoint [DEBUG] Sending http request: <AWSPreparedRequest stream_output=False, method=HEAD, url=https://cumulus.openstack.hpc.cam.ac.uk:6780/swift/v1/AUTH_21b4ae3a2ea44bc5a9c14005ed2963af/albert, headers={'User-Agent': b'Boto3/1.14.38 Python/3.8.3 Linux/5.7.9-100.fc31.x86_64 Botocore/1.17.38', 'X-Amz-Date': b'20200810T021906Z', 'X-Amz-Content-SHA256': b'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855', 'Authorization': b'AWS4-HMAC-SHA256 Credential=3367....0df9/20200810/US/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=40ca07263e0ddd5fc04d9982cc1b3fa7825c8a084fff35dcd4173cb8a24666ee'}>
    >   2020-08-10 02:19:06,046 botocore.parsers [DEBUG] Response headers: {'Content-Length': '0', 'Accept-Ranges': 'bytes', 'Location': '/swift/v1/AUTH_21b4ae3a2ea44bc5a9c14005ed2963af/albert/', 'X-Trans-Id': 'tx000000000000000011588-005f30ae9a-9da2c1-default', 'X-Openstack-Request-Id': 'tx000000000000000011588-005f30ae9a-9da2c1-default', 'Content-Type': 'text/plain; charset=utf-8', 'Date': 'Mon, 10 Aug 2020 02:19:06 GMT'}
    >   2020-08-10 02:19:06,046 botocore.parsers [DEBUG] Response body:
    >   b''
    >   2020-08-10 02:19:06,046 botocore.hooks [DEBUG] Event needs-retry.s3.HeadBucket: calling handler <botocore.retryhandler.RetryHandler object at 0x7ffb62412460>
    >   2020-08-10 02:19:06,047 botocore.retryhandler [DEBUG] No retry needed.
    >   2020-08-10 02:19:06,047 botocore.hooks [DEBUG] Event needs-retry.s3.HeadBucket: calling handler <bound method S3RegionRedirector.redirect_from_error of <botocore.utils.S3RegionRedirector object at 0x7ffb624124c0>>
    >   ....
    >   ....


    >   ....
    >   ....
    >   ....
    >   ....
    >     File "/usr/local/lib/python3.8/site-packages/botocore/utils.py", line 1214, in redirect_from_error
    >       new_region = self.get_bucket_region(bucket, response)
    >     File "/usr/local/lib/python3.8/site-packages/botocore/utils.py", line 1271, in get_bucket_region
    >       response = self._client.head_bucket(Bucket=bucket)
    >     File "/usr/local/lib/python3.8/site-packages/botocore/client.py", line 316, in _api_call
    >       return self._make_api_call(operation_name, kwargs)
    >     File "/usr/local/lib/python3.8/site-packages/botocore/client.py", line 621, in _make_api_call
    >       http, parsed_response = self._make_request(
    >     File "/usr/local/lib/python3.8/site-packages/botocore/client.py", line 641, in _make_request
    >       return self._endpoint.make_request(operation_model, request_dict)
    >     File "/usr/local/lib/python3.8/site-packages/botocore/endpoint.py", line 102, in make_request
    >       return self._send_request(request_dict, operation_model)
    >     File "/usr/local/lib/python3.8/site-packages/botocore/endpoint.py", line 132, in _send_request
    >   ....
    >   ....
    >   ....
    >   ....
    >     File "/usr/lib64/python3.8/logging/__init__.py", line 1544, in makeRecord
    >       rv = _logRecordFactory(name, level, fn, lno, msg, args, exc_info, func,
    >     File "/usr/lib64/python3.8/logging/__init__.py", line 289, in __init__
    >       ct = time.time()
    >   RecursionError: maximum recursion depth exceeded while calling a Python object


    #
    # So this client doesn't work either, but for a different reason ?
    #

    # Different way of using boto ..
    # https://www.youtube.com/watch?v=BfIKPlUpWgE

            import boto3
            import botocore

            session = boto3.Session(
                profile_name='swiftstack-v4'
                )
            s3 = session.resource(
                '3s',
                endpoint_url='https://cumulus.openstack.hpc.cam.ac.uk:6780/swift/v1/AUTH_21b4ae3a2ea44bc5a9c14005ed2963af'
                )
            list(
                s3.buckets.all()
                )

    >   ....
    >   ....
    >   botocore.exceptions.ProfileNotFound: The config profile (swiftstack-v4) could not be found

    #
    # .. and another way
    # (from swift/common/middleware/s3api/s3api.py)

            import boto3

            from boto.s3.connection import S3Connection
            connection = S3Connection(
                aws_access_key_id='c2e3....8f61',
                aws_secret_access_key='baab....ed59',
                port=8080,
                host='127.0.0.1',
                is_secure=False,
                calling_format=boto.s3.connection.OrdinaryCallingFormat())


    >   Traceback (most recent call last):
    >     File "<stdin>", line 1, in <module>
    >   ModuleNotFoundError: No module named 'boto'

    # OK, so that is broken too.



# -----------------------------------------------------
# -----------------------------------------------------

    The s3cmd client works no problems.
    The boto python client explodes in different ways.

    Using the AWS s3 Java library, looks like the XML response is using the Swift XML schema not the Amazon S3 schema.
    All the elelemnst are there, just the wrong format.

    The code and schema for generating Amazon S3 responses is there.
    I just can't find where the server decides which format to respond with.
    What request criteria does the server look for ?


# -----------------------------------------------------
# -----------------------------------------------------



    Found some clues.
    Using the top level URL without path works in browser RESTClient.
    https://cumulus.openstack.hpc.cam.ac.uk:6780/albert/


    Found out how to use anonymous access in the Java client.
    https://stackoverflow.com/a/53812476


    THIS WORKS in Java
    (*) for the short directory listing
    (**) fails for the full DR2 parquet data set :-(

        Configuration configuration = new Configuration();

        //configuration.set("fs.s3a.endpoint", "https://cumulus.openstack.hpc.cam.ac.uk:6780/swift/v1/AUTH_21b4ae3a2ea44bc5a9c14005ed2963af/");
        configuration.set("fs.s3a.endpoint", "https://cumulus.openstack.hpc.cam.ac.uk:6780");
        //configuration.set("fs.s3a.access.key", "");
        //configuration.set("fs.s3a.secret.key", "");
        configuration.set("fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider");
        configuration.set("fs.s3a.path.style.access", "true");
        configuration.set("fs.s3a.bucket.probe", "0");

//        URI baseuri = new URI(
//            "s3a://gaia-dr2-parquet"
//            );
        URI baseuri = new URI(
            "s3a://albert"
            );

        S3AFileSystem fs = (S3AFileSystem) FileSystem.get(
            baseuri,
            configuration
            );

        Path basepath = new Path(
            "/"
            );

        FileStatus check = fs.getFileStatus(
            basepath
            );

        if (check.isDirectory())
            {
            RemoteIterator<FileStatus> iter = fs.listStatusIterator(basepath);
            while(iter.hasNext())
                {
                FileStatus status = iter.next();
                log.debug("Node [{}]", status.getPath());
                }
            }


    Terrible performance on directory listing.
    Options are available:
    https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/performance.html#Speeding_up_directory_listing_operations_through_S3Guard
    https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/s3guard.html

    Best practises for code
    https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/performance.html#Best_Practises_for_Code

        Use listFiles() rather than listStatus()

    Changed our code and performance is much better.

        -   RemoteIterator<FileStatus> iter = fs.listStatusIterator(basepath);
        +   RemoteIterator<LocatedFileStatus> iter = fs.listFiles(basepath, false);

    I suspect we don't need either of the tweaks.

    In the end, all we needed to find was this:

        configuration.set("fs.s3a.endpoint", "https://cumulus.openstack.hpc.cam.ac.uk:6780");
        configuration.set("fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider");

    Although this has some issues.

    Swift has a separate namespace for each account.

    So two auth accounts can have objects with the same name.
        https://cumulus.openstack.hpc.cam.ac.uk:6780/swift/v1/AUTH_0001/albert
    and
        https://cumulus.openstack.hpc.cam.ac.uk:6780/swift/v1/AUTH_0002/albert
    are separate objects.

    How does that map to the single namespace for the S3 endpoint ?
    More to learn.


# -----------------------------------------------------
# -----------------------------------------------------

    Tried these settings in a Zeppelin notebook.

        %spark.pyspark
        sc._jsc.hadoopConfiguration().set(
            "fs.s3a.endpoint", "cumulus.openstack.hpc.cam.ac.uk:6780/"
            )
        sc._jsc.hadoopConfiguration().set(
            "fs.s3a.path.style.access", "true"
            )
        sc._jsc.hadoopConfiguration().set(
            "fs.s3a.list.version", "2"
            )
        sc._jsc.hadoopConfiguration().set(
            "fs.s3a.aws.credentials.provider",
            "org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider"
            )

    Still get the XML parser errors.

        Py4JJavaError: An error occurred while calling o524.parquet.
            org.apache.hadoop.fs.s3a.AWSClientIOException:
                getFileStatus on s3a://gaia-dr2-parquet/:
                    com.amazonaws.SdkClientException:
                        Failed to parse XML document with handler class com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListObjectsV2Handler:
                            Failed to parse XML document with handler class com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListObjectsV2Handler

    Is this caused by the XML mangler ?
    TODO Replace the AWS jar with our modified version and try again.
    TODO rebuild the AWS toolkit with a tweaked version number.
    TODO rebuild the Zeppelin and Spark images with our verion of the AWS toolkit.


# -----------------------------------------------------
# Run local test to check.
#[user@desktop]

    source "${HOME}/aglais.env"
    pushd "${AGLAIS_CODE}"

        pushd 'experiments/zrq/java/spark-tools'

            mvn clean test

        popd
    popd

--START--
....
....
2020-08-10 16:07:05,096 INFO  [main] [MetricsSystemImpl] s3a-file-system metrics system started
2020-08-10 16:07:06,636 DEBUG [main] [AWSTestCase] Node [s3a://albert/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet]
2020-08-10 16:07:06,638 DEBUG [main] [AWSTestCase] Node [s3a://albert/part-00003-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet]
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.202 s - in uk.ac.roe.wfau.aglais.aws.AWSTestCase
--END--


# -----------------------------------------------------
# Edit the Amazon code to enable the XML mangling.
#[user@desktop]

    source "${HOME}/aglais.env"
    pushd "${AGLAIS_HOME}"
        pushd 'external'
            pushd 'amazon'
                pushd 'aws-sdk-java'
                    pushd 'aws-java-sdk-s3'

                        gedit 'src/main/java/com/amazonaws/services/s3/model/transform/XmlResponsesSaxParser.java' &

                        -   private boolean sanitizeXmlDocument = false;
                        +   private boolean sanitizeXmlDocument = true;

                        mvn clean install

                        git diff

                    popd
                popd
            popd
        popd
    popd


# -----------------------------------------------------
# Run local test to check.
#[user@desktop]

    source "${HOME}/aglais.env"
    pushd "${AGLAIS_CODE}"

        pushd 'experiments/zrq/java/spark-tools'

            mvn clean test

        popd
    popd


    #
    # JUnit tests didn't pick up the modified jar.
    # TODO change the AWS-SDK version number.
    #















