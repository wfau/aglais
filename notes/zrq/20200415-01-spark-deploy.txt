#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

# -----------------------------------------------------
# Create a container to work with.
# https://podman.readthedocs.io/en/latest/markdown/podman-run.1.html
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --env "clouduser=${AGLAIS_USER:?}" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --env "ANSIBLE_CODE=/mnt/ansible" \
        --volume "${AGLAIS_CODE:?}/experiments/zrq/ansible:/mnt/ansible:ro,z" \
        atolmis/ansible-client:latest \
        bash


# -----------------------------------------------------
# Create our Ansible include vars file.
#[root@ansibler]

    cat > /tmp/ansible-vars.yml << EOF
buildtag:  'aglais-$(date '+%Y%m%d')'
cloudname: '${cloudname}'
clouduser: '${clouduser}'
EOF


# -----------------------------------------------------
# Run the scripts from the ansible directory.
#[root@ansibler]

    # TODO Make this the working directory in the container ?
    # --env ANSIBLE_CODE=/mnt/ansible

    cd "${ANSIBLE_CODE:?}"


# -----------------------------------------------------
# Run the initial part of our deplyment.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-01.yml"

    >   ....
    >   ....


# -----------------------------------------------------
# Run the Hadoop part of our deplyment.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-02.yml"

    >   ....
    >   ....


# -----------------------------------------------------
# Format the HDFS NameNode on master01.
#[root@ansibler]

    ssh master01 \
        '
        hdfs namenode -format
        '

    >   /..
    >   STARTUP_MSG: Starting NameNode
    >   STARTUP_MSG:   host = master01/10.10.0.22
    >   STARTUP_MSG:   args = [-format]
    >   STARTUP_MSG:   version = 3.2.1
    >   STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/sh ....
    >   ....
    >   ....
    >   2020-04-15 01:34:05,740 INFO namenode.FSImage: Allocated new BlockPoolId: BP-584734168-10.10.0.22-1586914445734
    >   2020-04-15 01:34:05,755 INFO common.Storage: Storage directory /var/local/hadoop/namenode/fsimage has been successfully formatted.
    >   2020-04-15 01:34:05,828 INFO namenode.FSImageFormatProtobuf: Saving image file /var/local/hadoop/namenode/fsimage/current/fsimage.ckpt_0000000000000000000 using no compression
    >   2020-04-15 01:34:05,901 INFO namenode.FSImageFormatProtobuf: Image file /var/local/hadoop/namenode/fsimage/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved in 0 seconds .
    >   2020-04-15 01:34:05,910 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
    >   2020-04-15 01:34:05,916 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
    >   2020-04-15 01:34:05,916 INFO namenode.NameNode: SHUTDOWN_MSG: 
    >   /..
    >   SHUTDOWN_MSG: Shutting down NameNode at master01/10.10.0.22
    >   ************************************************************/


# -----------------------------------------------------
# Start the HDFS services.
#[root@ansibler]

    ssh master01 \
        '
        start-dfs.sh
        '

    >   Starting namenodes on [master01]
    >   Starting datanodes
    >   Starting secondary namenodes [aglais-20200415-master01.novalocal]


# -----------------------------------------------------
# Check the HDFS status.
#[root@ansibler]

    ssh master01 \
        '
        hdfs dfsadmin -report
        '

    >   Configured Capacity: 4398046511104 (4 TB)
    >   Present Capacity: 4389384241152 (3.99 TB)
    >   DFS Remaining: 4389384224768 (3.99 TB)
    >   DFS Used: 16384 (16 KB)
    >   DFS Used%: 0.00%
    >   Replicated Blocks:
    >   	Under replicated blocks: 0
    >   	Blocks with corrupt replicas: 0
    >   	Missing blocks: 0
    >   	Missing blocks (with replication factor 1): 0
    >   	Low redundancy blocks with highest priority to recover: 0
    >   	Pending deletion blocks: 0
    >   Erasure Coded Block Groups: 
    >   	Low redundancy block groups: 0
    >   	Block groups with corrupt internal blocks: 0
    >   	Missing block groups: 0
    >   	Low redundancy blocks with highest priority to recover: 0
    >   	Pending deletion blocks: 0
    >   
    >   -------------------------------------------------
    >   Live datanodes (4):
    >   
    >   Name: 10.10.0.12:9866 (worker04)
    >   Hostname: worker04
    >   Decommission Status : Normal
    >   Configured Capacity: 1099511627776 (1 TB)
    >   DFS Used: 4096 (4 KB)
    >   Non DFS Used: 17297408 (16.50 MB)
    >   DFS Remaining: 1097346056192 (1021.98 GB)
    >   DFS Used%: 0.00%
    >   DFS Remaining%: 99.80%
    >   Configured Cache Capacity: 0 (0 B)
    >   Cache Used: 0 (0 B)
    >   Cache Remaining: 0 (0 B)
    >   Cache Used%: 100.00%
    >   Cache Remaining%: 0.00%
    >   Xceivers: 1
    >   Last contact: Wed Apr 15 01:35:27 UTC 2020
    >   Last Block Report: Wed Apr 15 01:35:09 UTC 2020
    >   Num of Blocks: 0
    >   
    >   
    >   Name: 10.10.0.16:9866 (worker01)
    >   Hostname: worker01
    >   Decommission Status : Normal
    >   Configured Capacity: 1099511627776 (1 TB)
    >   DFS Used: 4096 (4 KB)
    >   Non DFS Used: 17297408 (16.50 MB)
    >   DFS Remaining: 1097346056192 (1021.98 GB)
    >   DFS Used%: 0.00%
    >   DFS Remaining%: 99.80%
    >   Configured Cache Capacity: 0 (0 B)
    >   Cache Used: 0 (0 B)
    >   Cache Remaining: 0 (0 B)
    >   Cache Used%: 100.00%
    >   Cache Remaining%: 0.00%
    >   Xceivers: 1
    >   Last contact: Wed Apr 15 01:35:27 UTC 2020
    >   Last Block Report: Wed Apr 15 01:35:09 UTC 2020
    >   Num of Blocks: 0
    >   
    >   
    >   Name: 10.10.0.7:9866 (worker02)
    >   Hostname: worker02
    >   Decommission Status : Normal
    >   Configured Capacity: 1099511627776 (1 TB)
    >   DFS Used: 4096 (4 KB)
    >   Non DFS Used: 17297408 (16.50 MB)
    >   DFS Remaining: 1097346056192 (1021.98 GB)
    >   DFS Used%: 0.00%
    >   DFS Remaining%: 99.80%
    >   Configured Cache Capacity: 0 (0 B)
    >   Cache Used: 0 (0 B)
    >   Cache Remaining: 0 (0 B)
    >   Cache Used%: 100.00%
    >   Cache Remaining%: 0.00%
    >   Xceivers: 1
    >   Last contact: Wed Apr 15 01:35:27 UTC 2020
    >   Last Block Report: Wed Apr 15 01:35:09 UTC 2020
    >   Num of Blocks: 0
    >   
    >   
    >   Name: 10.10.0.9:9866 (worker03)
    >   Hostname: worker03
    >   Decommission Status : Normal
    >   Configured Capacity: 1099511627776 (1 TB)
    >   DFS Used: 4096 (4 KB)
    >   Non DFS Used: 17297408 (16.50 MB)
    >   DFS Remaining: 1097346056192 (1021.98 GB)
    >   DFS Used%: 0.00%
    >   DFS Remaining%: 99.80%
    >   Configured Cache Capacity: 0 (0 B)
    >   Cache Used: 0 (0 B)
    >   Cache Remaining: 0 (0 B)
    >   Cache Used%: 100.00%
    >   Cache Remaining%: 0.00%
    >   Xceivers: 1
    >   Last contact: Wed Apr 15 01:35:27 UTC 2020
    >   Last Block Report: Wed Apr 15 01:35:09 UTC 2020
    >   Num of Blocks: 0


# -----------------------------------------------------
# Start the YARN services.
#[root@ansibler]

    ssh master01 \
        '
        start-yarn.sh
        '

    >   Starting resourcemanager
    >   Starting nodemanagers


# -----------------------------------------------------
# -----------------------------------------------------
# Tail the logs (separate terminals).
#[user@desktop]


    podman exec -it $(
        podman ps --filter 'name=ansibler' --format "{{.ID}}"
        ) \
        ssh master01 \
            '
            tail -f /var/local/hadoop/logs/hadoop-fedora-resourcemanager-$(hostname).log
            '

    >   ....
    >   2020-04-15 01:35:52,278 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: worker02:46685 Node Transitioned from NEW to RUNNING
    >   2020-04-15 01:35:52,322 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node worker03:33627 clusterResource: <memory:15000, vCores:8>
    >   2020-04-15 01:35:52,324 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node worker04:34641 clusterResource: <memory:30000, vCores:16>
    >   2020-04-15 01:35:52,338 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node worker01:37895 clusterResource: <memory:45000, vCores:24>
    >   2020-04-15 01:35:52,345 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node worker02:46685 clusterResource: <memory:60000, vCores:32>
    >   ....


    podman exec -it $(
        podman ps --filter 'name=ansibler' --format "{{.ID}}"
        ) \
        ssh worker01 \
            '
            tail -f /var/local/hadoop/logs/hadoop-fedora-nodemanager-$(hostname).log
            '

    >   ....
    >   2020-04-15 01:35:51,887 INFO org.eclipse.jetty.server.Server: Started @2399ms
    >   2020-04-15 01:35:51,887 INFO org.apache.hadoop.yarn.webapp.WebApps: Web app node started at 8042
    >   2020-04-15 01:35:51,888 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : worker01:37895
    >   2020-04-15 01:35:51,893 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at master01/10.10.0.22:8031
    >   2020-04-15 01:35:51,895 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
    >   2020-04-15 01:35:51,928 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []
    >   2020-04-15 01:35:51,936 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]
    >   2020-04-15 01:35:52,308 INFO org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id -1876265176
    >   2020-04-15 01:35:52,309 INFO org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 245659702
    >   2020-04-15 01:35:52,310 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as worker01:37895 with total resource of <memory:15000, vCores:8>
    >   ....


# -----------------------------------------------------
# Install the Spark binaries.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "20-install-spark.yml"

    >   ....
    >   ....


# -----------------------------------------------------
# Add the security rules for Spark.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "21-config-spark-security.yml"

    >   ....
    >   ....


# -----------------------------------------------------
# Create our Spark configuration.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "22-config-spark-master.yml"

    >   ....
    >   ....


# -----------------------------------------------------
# Create our HDFS log directory.
#[root@ansibler]

    ssh master01 \
        '
        hdfs dfs -mkdir /spark-log
        '

    >   ....
    >   ....


# -----------------------------------------------------
# Run the SparkPi example from the Spark install instructtions.
# https://spark.apache.org/docs/3.0.0-preview2/running-on-yarn.html#launching-spark-on-yarn
#[root@ansibler]

    ssh master01 \
        '
        cd "${SPARK_HOME:?}"

        spark-submit \
            --class org.apache.spark.examples.SparkPi \
            --master yarn \
            --deploy-mode cluster \
            --driver-memory 1g \
            --executor-memory 1g \
            --executor-cores 1 \
            examples/jars/spark-examples*.jar \
                10
        '

    >   2020-04-15 01:54:43,571 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    >   2020-04-15 01:54:43,637 INFO client.RMProxy: Connecting to ResourceManager at master01/10.10.0.22:8032
    >   2020-04-15 01:54:43,891 INFO yarn.Client: Requesting a new application from cluster with 4 NodeManagers
    >   2020-04-15 01:54:44,324 INFO conf.Configuration: resource-types.xml not found
    >   2020-04-15 01:54:44,324 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
    >   2020-04-15 01:54:44,339 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
    >   2020-04-15 01:54:44,340 INFO yarn.Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead
    >   2020-04-15 01:54:44,340 INFO yarn.Client: Setting up container launch context for our AM
    >   2020-04-15 01:54:44,340 INFO yarn.Client: Setting up the launch environment for our AM container
    >   2020-04-15 01:54:44,348 INFO yarn.Client: Preparing resources for our AM container
    >   2020-04-15 01:54:44,380 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
    >   2020-04-15 01:54:46,620 INFO yarn.Client: Uploading resource file:/tmp/spark-e333adc1-e462-4dc7-a147-0de6d29184f6/__spark_libs__7424337407415574401.zip -> hdfs://master01:9000/user/fedora/.sparkStaging/application_1586914549739_0002/__spark_libs__7424337407415574401.zip
    >   2020-04-15 01:54:47,870 INFO yarn.Client: Uploading resource file:/opt/spark-3.0.0-preview2-bin-hadoop3.2/examples/jars/spark-examples_2.12-3.0.0-preview2.jar -> hdfs://master01:9000/user/fedora/.sparkStaging/application_1586914549739_0002/spark-examples_2.12-3.0.0-preview2.jar
    >   2020-04-15 01:54:48,075 INFO yarn.Client: Uploading resource file:/tmp/spark-e333adc1-e462-4dc7-a147-0de6d29184f6/__spark_conf__2101498582196351929.zip -> hdfs://master01:9000/user/fedora/.sparkStaging/application_1586914549739_0002/__spark_conf__.zip
    >   2020-04-15 01:54:48,136 INFO spark.SecurityManager: Changing view acls to: fedora
    >   2020-04-15 01:54:48,137 INFO spark.SecurityManager: Changing modify acls to: fedora
    >   2020-04-15 01:54:48,137 INFO spark.SecurityManager: Changing view acls groups to: 
    >   2020-04-15 01:54:48,139 INFO spark.SecurityManager: Changing modify acls groups to: 
    >   2020-04-15 01:54:48,140 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fedora); groups with view permissions: Set(); users  with modify permissions: Set(fedora); groups with modify permissions: Set()
    >   2020-04-15 01:54:48,192 INFO yarn.Client: Submitting application application_1586914549739_0002 to ResourceManager
    >   2020-04-15 01:54:48,226 INFO impl.YarnClientImpl: Submitted application application_1586914549739_0002
    >   2020-04-15 01:54:49,231 INFO yarn.Client: Application report for application_1586914549739_0002 (state: ACCEPTED)
    >   2020-04-15 01:54:49,244 INFO yarn.Client: 
    >   	 client token: N/A
    >   	 diagnostics: AM container is launched, waiting for AM container to Register with RM
    >   	 ApplicationMaster host: N/A
    >   	 ApplicationMaster RPC port: -1
    >   	 queue: default
    >   	 start time: 1586915688203
    >   	 final status: UNDEFINED
    >   	 tracking URL: http://master01:8088/proxy/application_1586914549739_0002/
    >   	 user: fedora
    >   2020-04-15 01:54:50,247 INFO yarn.Client: Application report for application_1586914549739_0002 (state: ACCEPTED)
    >   2020-04-15 01:54:51,249 INFO yarn.Client: Application report for application_1586914549739_0002 (state: ACCEPTED)
    >   2020-04-15 01:54:52,251 INFO yarn.Client: Application report for application_1586914549739_0002 (state: ACCEPTED)
    >   2020-04-15 01:54:53,254 INFO yarn.Client: Application report for application_1586914549739_0002 (state: ACCEPTED)
    >   2020-04-15 01:54:54,256 INFO yarn.Client: Application report for application_1586914549739_0002 (state: ACCEPTED)
    >   2020-04-15 01:54:55,258 INFO yarn.Client: Application report for application_1586914549739_0002 (state: RUNNING)
    >   2020-04-15 01:54:55,258 INFO yarn.Client: 
    >   	 client token: N/A
    >   	 diagnostics: N/A
    >   	 ApplicationMaster host: worker03
    >   	 ApplicationMaster RPC port: 42093
    >   	 queue: default
    >   	 start time: 1586915688203
    >   	 final status: UNDEFINED
    >   	 tracking URL: http://master01:8088/proxy/application_1586914549739_0002/
    >   	 user: fedora
    >   2020-04-15 01:54:56,260 INFO yarn.Client: Application report for application_1586914549739_0002 (state: RUNNING)
    >   2020-04-15 01:54:57,263 INFO yarn.Client: Application report for application_1586914549739_0002 (state: RUNNING)
    >   2020-04-15 01:54:58,265 INFO yarn.Client: Application report for application_1586914549739_0002 (state: RUNNING)
    >   2020-04-15 01:54:59,268 INFO yarn.Client: Application report for application_1586914549739_0002 (state: RUNNING)
    >   2020-04-15 01:55:00,270 INFO yarn.Client: Application report for application_1586914549739_0002 (state: RUNNING)
    >   2020-04-15 01:55:01,272 INFO yarn.Client: Application report for application_1586914549739_0002 (state: RUNNING)
    >   2020-04-15 01:55:02,273 INFO yarn.Client: Application report for application_1586914549739_0002 (state: FINISHED)
    >   2020-04-15 01:55:02,274 INFO yarn.Client: 
    >   	 client token: N/A
    >   	 diagnostics: N/A
    >   	 ApplicationMaster host: worker03
    >   	 ApplicationMaster RPC port: 42093
    >   	 queue: default
    >   	 start time: 1586915688203
    >   	 final status: SUCCEEDED
    >   	 tracking URL: http://master01:8088/proxy/application_1586914549739_0002/
    >   	 user: fedora
    >   2020-04-15 01:55:02,289 INFO util.ShutdownHookManager: Shutdown hook called
    >   2020-04-15 01:55:02,292 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-e333adc1-e462-4dc7-a147-0de6d29184f6
    >   2020-04-15 01:55:02,295 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-3bdec4f3-dcbf-4c05-ab03-826496b3c25b


# -----------------------------------------------------
# Install Python.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "23-install-python.yml"

    >   ....
    >   ....


# -----------------------------------------------------
# Install PySpark.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "24-install-pyspark.yml"

    >   ....
    >   ....


# -----------------------------------------------------
# Run some PySpark examples ....
# https://www.tutorialspoint.com/pyspark/pyspark_sparkcontext.htm
#[root@ansibler]


    ssh master01

        pyspark
        

    >   Python 3.7.3 (default, Mar 27 2019, 13:36:35) 
    >   [GCC 9.0.1 20190227 (Red Hat 9.0.1-0.8)] on linux
    >   Type "help", "copyright", "credits" or "license" for more information.
    >   2020-04-15 02:12:00,559 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    >   Setting default log level to "WARN".
    >   To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
    >   2020-04-15 02:12:03,004 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
    >   Welcome to
    >         ____              __
    >        / __/__  ___ _____/ /__
    >       _\ \/ _ \/ _ `/ __/  '_/
    >      /__ / .__/\_,_/_/ /_/\_\   version 3.0.0-preview2
    >         /_/
    >   
    >   Using Python version 3.7.3 (default, Mar 27 2019 13:36:35)
    >   SparkSession available as 'spark'.
    >   >>> 


        logFile = "file:///opt/hadoop/README.txt"
        logData = sc.textFile(logFile).cache()
        numAs = logData.filter(lambda s: 'a' in s).count()
        numBs = logData.filter(lambda s: 'b' in s).count()
        print(
            'Lines with a: {}, lines with b: {}'.format(
                numAs,
                numBs
                )
            )

    >   ....
    >   Lines with a: 25, lines with b: 7


# -----------------------------------------------------
# Run some PySpark examples ....
# https://www.tutorialspoint.com/pyspark/pyspark_sparkcontext.htm
#[root@ansibler]


    ssh master01


        cat > firstapp.py << 'EOF'
from pyspark import SparkContext
sc = SparkContext("local", "first app")
logFile = "file:///opt/hadoop/README.txt"
logData = sc.textFile(logFile).cache()
numAs = logData.filter(lambda s: 'a' in s).count()
numBs = logData.filter(lambda s: 'b' in s).count()
print(
    'Lines with a: {}, lines with b: {}'.format(
        numAs,
        numBs
        )
    )
EOF


    spark-submit firstapp.py

    >   2020-04-15 02:36:04,596 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    >   2020-04-15 02:36:05,167 INFO spark.SparkContext: Running Spark version 3.0.0-preview2
    >   2020-04-15 02:36:05,208 INFO resource.ResourceUtils: ==============================================================
    >   2020-04-15 02:36:05,209 INFO resource.ResourceUtils: Resources for spark.driver:
    >   2020-04-15 02:36:05,210 INFO resource.ResourceUtils: ==============================================================
    >   2020-04-15 02:36:05,210 INFO spark.SparkContext: Submitted application: first app
    >   2020-04-15 02:36:05,267 INFO spark.SecurityManager: Changing view acls to: fedora
    >   2020-04-15 02:36:05,267 INFO spark.SecurityManager: Changing modify acls to: fedora
    >   2020-04-15 02:36:05,268 INFO spark.SecurityManager: Changing view acls groups to: 
    >   2020-04-15 02:36:05,268 INFO spark.SecurityManager: Changing modify acls groups to: 
    >   2020-04-15 02:36:05,268 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fedora); groups with view permissions: Set(); users  with modify permissions: Set(fedora); groups with modify permissions: Set()
    >   2020-04-15 02:36:05,469 INFO util.Utils: Successfully started service 'sparkDriver' on port 44159.
    >   2020-04-15 02:36:05,501 INFO spark.SparkEnv: Registering MapOutputTracker
    >   2020-04-15 02:36:05,540 INFO spark.SparkEnv: Registering BlockManagerMaster
    >   2020-04-15 02:36:05,557 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
    >   2020-04-15 02:36:05,557 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
    >   2020-04-15 02:36:05,568 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat
    >   2020-04-15 02:36:05,584 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-5f2b375c-9568-442a-bc75-2bcac4a59f36
    >   2020-04-15 02:36:05,609 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MiB
    >   2020-04-15 02:36:05,627 INFO spark.SparkEnv: Registering OutputCommitCoordinator
    >   2020-04-15 02:36:05,719 INFO util.log: Logging initialized @2303ms to org.sparkproject.jetty.util.log.Slf4jLog
    >   2020-04-15 02:36:05,815 INFO server.Server: jetty-9.4.z-SNAPSHOT; built: 2019-04-29T20:42:08.989Z; git: e1bc35120a6617ee3df052294e433f3a25ce7097; jvm 1.8.0_242-b08
    >   2020-04-15 02:36:05,835 INFO server.Server: Started @2420ms
    >   2020-04-15 02:36:05,866 INFO server.AbstractConnector: Started ServerConnector@671e8ad1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
    >   2020-04-15 02:36:05,866 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
    >   2020-04-15 02:36:05,890 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@f558944{/jobs,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,892 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48415093{/jobs/json,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,893 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@55851997{/jobs/job,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,896 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4704e262{/jobs/job/json,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,897 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3d6b02ac{/stages,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,898 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@677eb5ff{/stages/json,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,899 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54c0a454{/stages/stage,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,903 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3632ffaf{/stages/stage/json,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,904 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@38034c92{/stages/pool,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,906 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@76abcd20{/stages/pool/json,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,907 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5cfa4ed1{/storage,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,908 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@f51fb3b{/storage/json,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,909 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@bf7663{/storage/rdd,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,910 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@78f580aa{/storage/rdd/json,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,911 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64358b57{/environment,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,912 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21d82d65{/environment/json,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,913 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61e09d5a{/executors,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,916 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@14c75b27{/executors/json,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,917 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@66d88dac{/executors/threadDump,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,918 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b312b1d{/executors/threadDump/json,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,926 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@47a2798{/static,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,927 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63aafff7{/,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,932 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@160d0ada{/api,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,932 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f1782a0{/jobs/job/kill,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,933 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7d69e74a{/stages/stage/kill,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:05,938 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://master01:4040
    >   2020-04-15 02:36:06,160 INFO executor.Executor: Starting executor ID driver on host master01
    >   2020-04-15 02:36:06,187 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46773.
    >   2020-04-15 02:36:06,187 INFO netty.NettyBlockTransferService: Server created on master01:46773
    >   2020-04-15 02:36:06,189 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
    >   2020-04-15 02:36:06,195 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, master01, 46773, None)
    >   2020-04-15 02:36:06,198 INFO storage.BlockManagerMasterEndpoint: Registering block manager master01:46773 with 366.3 MiB RAM, BlockManagerId(driver, master01, 46773, None)
    >   2020-04-15 02:36:06,201 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, master01, 46773, None)
    >   2020-04-15 02:36:06,202 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, master01, 46773, None)
    >   2020-04-15 02:36:06,365 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@445484f9{/metrics/json,null,AVAILABLE,@Spark}
    >   2020-04-15 02:36:07,001 INFO history.SingleEventLogFileWriter: Logging events to hdfs://master01:9000/spark-log/local-1586918166026.inprogress
    >   2020-04-15 02:36:07,616 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 294.3 KiB, free 366.0 MiB)
    >   2020-04-15 02:36:07,681 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 366.0 MiB)
    >   2020-04-15 02:36:07,683 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on master01:46773 (size: 27.3 KiB, free: 366.3 MiB)
    >   2020-04-15 02:36:07,686 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0
    >   2020-04-15 02:36:07,887 INFO mapred.FileInputFormat: Total input files to process : 1
    >   2020-04-15 02:36:07,955 INFO spark.SparkContext: Starting job: count at /home/fedora/firstapp.py:5
    >   2020-04-15 02:36:07,968 INFO scheduler.DAGScheduler: Got job 0 (count at /home/fedora/firstapp.py:5) with 1 output partitions
    >   2020-04-15 02:36:07,968 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (count at /home/fedora/firstapp.py:5)
    >   2020-04-15 02:36:07,969 INFO scheduler.DAGScheduler: Parents of final stage: List()
    >   2020-04-15 02:36:07,978 INFO scheduler.DAGScheduler: Missing parents: List()
    >   2020-04-15 02:36:08,075 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /home/fedora/firstapp.py:5), which has no missing parents
    >   2020-04-15 02:36:08,110 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.2 KiB, free 366.0 MiB)
    >   2020-04-15 02:36:08,120 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 366.0 MiB)
    >   2020-04-15 02:36:08,121 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on master01:46773 (size: 4.8 KiB, free: 366.3 MiB)
    >   2020-04-15 02:36:08,122 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1206
    >   2020-04-15 02:36:08,135 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at count at /home/fedora/firstapp.py:5) (first 15 tasks are for partitions Vector(0))
    >   2020-04-15 02:36:08,141 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
    >   2020-04-15 02:36:08,198 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, master01, executor driver, partition 0, PROCESS_LOCAL, 7368 bytes)
    >   2020-04-15 02:36:08,218 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
    >   2020-04-15 02:36:08,443 INFO rdd.HadoopRDD: Input split: file:/opt/hadoop/README.txt:0+1361
    >   2020-04-15 02:36:08,679 INFO memory.MemoryStore: Block rdd_1_0 stored as values in memory (estimated size 1133.0 B, free 366.0 MiB)
    >   2020-04-15 02:36:08,680 INFO storage.BlockManagerInfo: Added rdd_1_0 in memory on master01:46773 (size: 1133.0 B, free: 366.3 MiB)
    >   2020-04-15 02:36:09,054 INFO python.PythonRunner: Times: total = 363, boot = 303, init = 60, finish = 0
    >   2020-04-15 02:36:09,069 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1592 bytes result sent to driver
    >   2020-04-15 02:36:09,078 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 891 ms on master01 (executor driver) (1/1)
    >   2020-04-15 02:36:09,083 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 36197
    >   2020-04-15 02:36:09,095 INFO scheduler.DAGScheduler: ResultStage 0 (count at /home/fedora/firstapp.py:5) finished in 0.988 s
    >   2020-04-15 02:36:09,101 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
    >   2020-04-15 02:36:09,120 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
    >   2020-04-15 02:36:09,120 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
    >   2020-04-15 02:36:09,123 INFO scheduler.DAGScheduler: Job 0 finished: count at /home/fedora/firstapp.py:5, took 1.167578 s
    >   2020-04-15 02:36:09,148 INFO spark.SparkContext: Starting job: count at /home/fedora/firstapp.py:6
    >   2020-04-15 02:36:09,149 INFO scheduler.DAGScheduler: Got job 1 (count at /home/fedora/firstapp.py:6) with 1 output partitions
    >   2020-04-15 02:36:09,149 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (count at /home/fedora/firstapp.py:6)
    >   2020-04-15 02:36:09,149 INFO scheduler.DAGScheduler: Parents of final stage: List()
    >   2020-04-15 02:36:09,151 INFO scheduler.DAGScheduler: Missing parents: List()
    >   2020-04-15 02:36:09,153 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (PythonRDD[3] at count at /home/fedora/firstapp.py:6), which has no missing parents
    >   2020-04-15 02:36:09,156 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.2 KiB, free 366.0 MiB)
    >   2020-04-15 02:36:09,158 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 366.0 MiB)
    >   2020-04-15 02:36:09,158 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on master01:46773 (size: 4.8 KiB, free: 366.3 MiB)
    >   2020-04-15 02:36:09,159 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1206
    >   2020-04-15 02:36:09,160 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[3] at count at /home/fedora/firstapp.py:6) (first 15 tasks are for partitions Vector(0))
    >   2020-04-15 02:36:09,160 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
    >   2020-04-15 02:36:09,163 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, master01, executor driver, partition 0, PROCESS_LOCAL, 7368 bytes)
    >   2020-04-15 02:36:09,163 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
    >   2020-04-15 02:36:09,168 INFO storage.BlockManager: Found block rdd_1_0 locally
    >   2020-04-15 02:36:09,210 INFO python.PythonRunner: Times: total = 41, boot = -108, init = 149, finish = 0
    >   2020-04-15 02:36:09,211 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 1592 bytes result sent to driver
    >   2020-04-15 02:36:09,218 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 56 ms on master01 (executor driver) (1/1)
    >   2020-04-15 02:36:09,220 INFO scheduler.DAGScheduler: ResultStage 1 (count at /home/fedora/firstapp.py:6) finished in 0.066 s
    >   2020-04-15 02:36:09,220 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
    >   2020-04-15 02:36:09,223 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
    >   2020-04-15 02:36:09,223 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
    >   2020-04-15 02:36:09,224 INFO scheduler.DAGScheduler: Job 1 finished: count at /home/fedora/firstapp.py:6, took 0.075400 s
    >   Lines with a: 25, lines with b: 7
    >   2020-04-15 02:36:09,253 INFO spark.SparkContext: Invoking stop() from shutdown hook
    >   2020-04-15 02:36:09,263 INFO server.AbstractConnector: Stopped Spark@671e8ad1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
    >   2020-04-15 02:36:09,265 INFO ui.SparkUI: Stopped Spark web UI at http://master01:4040
    >   2020-04-15 02:36:09,319 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
    >   2020-04-15 02:36:09,337 INFO memory.MemoryStore: MemoryStore cleared
    >   2020-04-15 02:36:09,338 INFO storage.BlockManager: BlockManager stopped
    >   2020-04-15 02:36:09,346 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
    >   2020-04-15 02:36:09,348 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
    >   2020-04-15 02:36:09,358 INFO spark.SparkContext: Successfully stopped SparkContext
    >   2020-04-15 02:36:09,358 INFO util.ShutdownHookManager: Shutdown hook called
    >   2020-04-15 02:36:09,359 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-9cb17457-c6bc-4a90-9cad-d64595bc3245
    >   2020-04-15 02:36:09,361 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-45302ccf-734e-403c-a743-df2e0eedba26
    >   2020-04-15 02:36:09,363 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-9cb17457-c6bc-4a90-9cad-d64595bc3245/pyspark-ea42609e-cd02-4a9a-9b43-16c4c55bee85



