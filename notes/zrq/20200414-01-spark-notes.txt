#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    #
    # Best description I have seen so far.
    # https://www.ibm.com/support/knowledgecenter/SSCTFE_1.1.0/com.ibm.azk.v1r1.azka100/topics/azkic_t_confignetwork.htm

        Several services listen on <random> ports.
        BlockManager on both Driver and Executor
        <random> are configurable using properties
            (*) 


        The REST server interface, which listens on port 6066 by default, is currently not included in the Apache Spark documentation.
        The REST server does not support TLS nor client authentication; however, Spark applications can be submitted through this interface.
        The REST server is used when applications are submitted using cluster deploy mode (--deploy-mode cluster).
        Client deploy mode is the default behavior for Spark, and is the way that notebooks, like Jupyter Notebook, connect to a Spark cluster.
        Depending on your planned deployment and environment, access to the REST server might be restricted by other controls.
        However, if you want to disable it, you can do so by setting spark.master.rest.enabled to false in $SPARK_CONF_DIR/spark-defaults.conf.

   
    #
    # Block Manager — Key-Value Store for Blocks
    # https://mallikarjuna_g.gitbooks.io/spark/spark-blockmanager.html
        
    #
    # Apache Spark blocks explained
    # https://www.waitingforcode.com/apache-spark/apache-spark-blocks-explained/read
    # "In Spark blocks are everywhere. They represent broadcasted objects, they are used as support for intermediate steps in shuffle process, or finally they're used to store temporary files."



    #
    # Also interesting ..
    # https://spark.apache.org/docs/1.4.0/api/java/org/apache/spark/broadcast/TorrentBroadcastFactory.html
    # "A Broadcast implementation that uses a BitTorrent-like protocol to do a distributed transfer of the broadcasted data to the executors. Refer to TorrentBroadcast for more details."

    #
    # Docker images and Apache Spark applications
    # https://www.waitingforcode.com/apache-spark/docker-images-apache-spark-applications/read
    #

    #
    # Spark source code ..
    # https://github.com/apache/spark
    #


    #
    # What are workers executors cores in Spark
    # https://stackoverflow.com/questions/32621990/what-are-workers-executors-cores-in-spark-standalone-cluster
    #
        
    #
    # https://github.com/JerryLead/SparkInternals
    # Notes talking about the design and implementation of Apache Spark
    #

        Overview Overview of Apache Spark
        Job logical plan Logical plan of a job (data dependency graph)
        Job physical plan Physical plan
        Shuffle details Shuffle process
        Architecture Coordination of system modules in job execution
        Cache and Checkpoint Cache and Checkpoint
        Broadcast Broadcast feature
        Job Scheduling TODO
        Fault-tolerance TODO


    #
    # Deep dive into spark streaming
    # https://www.slideshare.net/TaoLi16/deep-dive-into-spark-streaming
    #

    #    
    # Top 5 mistakes ....
    # https://www.slideshare.net/cloudera/top-5-mistakes-to-avoid-when-writing-apache-spark-applications

    #
    # Spark deployment modes
    # https://trongkhoanguyen.com/spark/understand-the-spark-deployment-modes/

    #
    # Spark on Yarn
    # https://www.slideshare.net/datamantra/spark-on-yarn-54201193
       
       
    #
    # Deep dive into spark cluster management (old, 2015)
    # https://dzone.com/articles/deep-dive-into-spark-cluster-management
    
        Yarn and Mesos .. nothing about K8s yet.
        
        
    #
    # PySpark Internals (old, 2016)
    # https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals        
    
    #
    # PySpark
    # https://www.tutorialspoint.com/pyspark/pyspark_sparkcontext.htm
    # https://www.tutorialspoint.com/pyspark/pyspark_environment_setup.htm
    
    
    
