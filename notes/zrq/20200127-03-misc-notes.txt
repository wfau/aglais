#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2019, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    #
    # TODO
    # Firewall on masters and workers.
    # Security groups on masters and workers.
    # Install Python3 on masters and workers.
    # Which processes need to be run on which nodes ..

        #
        # From stv notes
        # 20191120-openstack-deployment.txt

            ## On Master Node: (ssh stv-aglais-master)

            start-all.sh

# -----------------------------------------------------
# -----------------------------------------------------
# Peek at what start-all.sh does.
#[fedora@stv-dev-master]

    which start-all.sh

    >   ~/hadoop/sbin/start-all.sh


    less ~/hadoop/sbin/start-all.sh

    >   ....
    >   # start hdfs daemons if hdfs is present
    >   if [[ -f "${HADOOP_HDFS_HOME}/sbin/start-dfs.sh" ]]; then
    >     "${HADOOP_HDFS_HOME}/sbin/start-dfs.sh" --config "${HADOOP_CONF_DIR}"
    >   fi
    >   
    >   # start yarn daemons if yarn is present
    >   if [[ -f "${HADOOP_YARN_HOME}/sbin/start-yarn.sh" ]]; then
    >     "${HADOOP_YARN_HOME}/sbin/start-yarn.sh" --config "${HADOOP_CONF_DIR}"
    >   fi
    >   ....

    #
    # OK, so that is what is running on the master node.
    # What is running on the worker nodes ?
    # What starts the Hadoop processes on the workers ?
    #

# -----------------------------------------------------
# -----------------------------------------------------
# Peek at shell history on latest worker.
#[fedora@stv-dev-worker-8]

    history

    >   [fedora@stv-dev-worker-8 ~]$ history
    >       1  sudo yum install -y wget
    >       2  sudo yum install -y nano
    >       3  wget http://apache.cs.utah.edu/hadoop/common/current/hadoop-3.1.3.tar.gz
    >       4  tar -xzf hadoop-3.1.3.tar.gz
    >       5  mv hadoop-3.1.3 hadoop
    >       6  sudo yum install -y java-1.8.0-openjdk
    >       7  sudo su
    >       8  cat > "${HOME:?}/.profile" << EOF
    >       9  PATH=/home/fedora/spark/bin:/home/fedora/.local/bin:/home/fedora/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/bin:/sbin:/home/fedora/hadoop/bin:/home/fedora/hadoop/sbin
    >      10  EOF
    >      11  cat <<EOF >> "${HOME:?}/.bashrc"
    >      12  export HADOOP_HOME=/home/fedora/hadoop
    >      13  export PATH=/home/fedora/.local/bin:/home/fedora/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/bin:/sbin:/home/fedora/hadoop/bin:/home/fedora/hadoop/sbin:/home/fedora/spark/bin
    >      14  export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre/
    >      15  export HADOOP_CONF_DIR=/home/fedora/hadoop/etc/hadoop
    >      16  export SPARK_HOME=/home/fedora/spark
    >      17  export LD_LIBRARY_PATH=/home/fedora/hadoop/lib/native
    >      18  EOF
    >      19  cat /etc/hosts
    >      20  cat > "${HOME:?}/.ssh/config" << EOF
    >           ....
    >      87  EOF
    >      88  nano ~/.ssh/stv-master
    >      89  ls
    >      90  pwd
    >      91  source ~/.bashrc
    >      92  sudo mkdir /home/hadoop/
    >      93  sudo mkdir hadoop/data
    >      94  sudo mkdir hadoop/data/nameNode
    >      95  sudo mkdir hadoop/data/dataNode
    >      96  sudo chown -R fedora:root /home/hadoop/
    >      97  source /home/fedora/hadoop/bin/hdfs namenode -format
    >      98  sudo yum install python3
    >      99  sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 10
    >     100  sudo chown -R fedora:root /home/hadoop/
    >     101  sudo chmod -R 755 /home/hadoop/*
    >     102  sudo chmod -R 755 /home/fedora/hadoop/*
    >     103  source /home/fedora/hadoop/bin/hdfs namenode -format
    >     104  cat /home/fedora/hadoop/data/nameNode/
    >     105  cd /home/fedora/hadoop/data/
    >     106  ls
    >     107  ls -al
    >     108  sudo chown -R fedora:root /home/fedora/hadoop/*
    >     109  source /home/fedora/hadoop/bin/hdfs namenode -format
    >     110  ls
    >     111  nano hadoop/etc/hadoop/workers
    >     112  hdfs dfs -ls /hadoop
    >     113  hdfs dfs -ls /hadoop/gaia
    >     114  sudo rm -r /tmp/*
    >     115  source /home/fedora/hadoop/bin/hdfs namenode -format
    >     116  sudo rm -R /tmp/*
    >     117  rm -rf ~/hadoop/data/nameNode/*
    >     118  rm -rf ~/hadoop/data/dataNode/*
    >     119  hadoop namenode -format
    >     120  rm ~/.ssh/stv-master
    >     121  history | grep 8080
    >     122  nano ~/.ssh/authorized_keys
    >     123  cat ~/.ssh/authorized_keys
    >     124  cat > "${HOME:?}/.ssh/config" << EOF
    >           ....
    >     180  EOF
    >     181  cat ~/.ssh/config
    >     182  rm -r /home/fedora/hadoop/logs/*
    >     183  ls
    >     184  nano ~/.ssh/authorized_keys
    >     186  nano ~/.ssh/authorized_keys
    >     187  history

    #
    # Apache instructions
    # https://hadoop.apache.org/docs/r3.1.2/hadoop-project-dist/hadoop-common/ClusterSetup.html#Hadoop_Startup
    # "To start a Hadoop cluster you will need to start both the HDFS and YARN cluster."

    >   The first time you bring up HDFS, it must be formatted. Format a new distributed filesystem as hdfs:
    >   
    >   [hdfs]$ $HADOOP_HOME/bin/hdfs namenode -format <cluster_name>
    >   
    >   Start the HDFS NameNode with the following command on the designated node as hdfs:
    >   
    >   [hdfs]$ $HADOOP_HOME/bin/hdfs --daemon start namenode
    >   
    >   Start a HDFS DataNode with the following command on each designated node as hdfs:
    >   
    >   [hdfs]$ $HADOOP_HOME/bin/hdfs --daemon start datanode
    >   
    >   If etc/hadoop/workers and ssh trusted access is configured (see Single Node Setup), all of the HDFS processes can be started with a utility script. As hdfs:
    >   
    >   [hdfs]$ $HADOOP_HOME/sbin/start-dfs.sh
    >   
    >   Start the YARN with the following command, run on the designated ResourceManager as yarn:
    >   
    >   [yarn]$ $HADOOP_HOME/bin/yarn --daemon start resourcemanager
    >   
    >   Run a script to start a NodeManager on each designated host as yarn:
    >   
    >   [yarn]$ $HADOOP_HOME/bin/yarn --daemon start nodemanager
    >   
    >   Start a standalone WebAppProxy server. Run on the WebAppProxy server as yarn. If multiple servers are used with load balancing it should be run on each of them:
    >   
    >   [yarn]$ $HADOOP_HOME/bin/yarn --daemon start proxyserver
    >   
    >   If etc/hadoop/workers and ssh trusted access is configured (see Single Node Setup), all of the YARN processes can be started with a utility script. As yarn:
    >   
    >   [yarn]$ $HADOOP_HOME/sbin/start-yarn.sh
    >   
    >   Start the MapReduce JobHistory Server with the following command, run on the designated server as mapred:
    >   
    >   [mapred]$ $HADOOP_HOME/bin/mapred --daemon start historyserver

    #
    # ... but I can't find any of them in the user history.
    #

    #
    # Because they weren't started by a shell

    #
    # Running the start scripts on the master node starts daemons on the worker nodes.
    # Hadoop uses a password-less ssh key to login from master to worker nodes.
    # We need to generate this key and install it on the workers.
    # https://www.linode.com/docs/databases/hadoop/how-to-install-and-set-up-hadoop-cluster/#distribute-authentication-key-pairs-for-the-hadoop-user
    #

    #
    # Implicit "make password less ssh work between master and workers" is not in the notes !!
    #



    https://www.linode.com/docs/databases/hadoop/how-to-install-and-set-up-hadoop-cluster/




