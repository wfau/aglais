#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    External (GitHub) OAuth for Dashboard
    https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/auth/oauth-external-auth

    Kubernetes Dashboard
    https://github.com/kubernetes/dashboard

TODO use secrets
secret Pa6achan.id
secret Pa6achan.secret

ssh -n \
    'dmr@trop01.roe.ac.uk' \
    "bin/secret '${1}'"

# -----------------------------------------------------

    # Deleted old cluster.
    # notes/zrq/20200718-03-openstack-delete.txt

    # Created new cluster.
    # notes/zrq/20200718-04-terraform-create.txt

# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --hostname kubernator \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --env "clustername=${MAGNUM_CLUSTER:?}" \
        --volume "${HOME}/clouds.yaml:/etc/openstack/clouds.yaml:z" \
        --volume "${AGLAIS_CODE}/experiments/zrq/kubernetes:/kubernetes:z" \
        --volume "${ZEPPELIN_CODE}:/zeppelin:z" \
        atolmis/openstack-client \
        bash


# -----------------------------------------------------
# Get the connection details for our cluster.
#[user@kubernator]

    mkdir -p "${HOME}/.kube/${clustername:?}"
    openstack \
        --os-cloud "${cloudname:?}-super" \
        coe cluster config \
            "${clustername:?}" \
                --force \
                --dir "${HOME}/.kube"

    >   'SHELL'


# -----------------------------------------------------
# Check kubectl can get the connection details for our cluster.
#[user@kubernator]

    kubectl \
        cluster-info

    >   Kubernetes master is running at ....
    >   Heapster is running at ....
    >   CoreDNS is running at ....


# -----------------------------------------------------
# Install config editing tools.
#[user@kubernator]

    # TODO - add this to the kubernator image
    mkdir -p "${HOME:?}/bin"
    wget  -O "${HOME:?}/bin/yq" https://github.com/mikefarah/yq/releases/download/3.3.2/yq_linux_amd64
    chmod a+x "${HOME:?}/bin/yq"


# -----------------------------------------------------
# Check versions.
#[user@kubernator]

    kubectl version

    >   Client Version: version.Info{Major:"1", Minor:"15+", GitVersion:"v1.15.8-beta.0", GitCommit:"6c143d35bb11d74970e7bc0b6c45b6bfdffc0bd4", GitTreeState:"archive", BuildDate:"2020-01-29T00:00:00Z", GoVersion:"go1.14beta1", Compiler:"gc", Platform:"linux/amd64"}
    >   Server Version: version.Info{Major:"1", Minor:"15", GitVersion:"v1.15.9", GitCommit:"2e808b7cb054ee242b68e62455323aa783991f03", GitTreeState:"clean", BuildDate:"2020-01-18T23:24:23Z", GoVersion:"go1.12.12", Compiler:"gc", Platform:"linux/amd64"}


    kubectl api-resources

    >   NAME                              SHORTNAMES   APIGROUP                       NAMESPACED   KIND
    >   bindings                                                                      true         Binding
    >   componentstatuses                 cs                                          false        ComponentStatus
    >   configmaps                        cm                                          true         ConfigMap
    >   endpoints                         ep                                          true         Endpoints
    >   events                            ev                                          true         Event
    >   limitranges                       limits                                      true         LimitRange
    >   namespaces                        ns                                          false        Namespace
    >   nodes                             no                                          false        Node
    >   persistentvolumeclaims            pvc                                         true         PersistentVolumeClaim
    >   persistentvolumes                 pv                                          false        PersistentVolume
    >   pods                              po                                          true         Pod
    >   podtemplates                                                                  true         PodTemplate
    >   replicationcontrollers            rc                                          true         ReplicationController
    >   resourcequotas                    quota                                       true         ResourceQuota
    >   secrets                                                                       true         Secret
    >   serviceaccounts                   sa                                          true         ServiceAccount
    >   services                          svc                                         true         Service
    >   mutatingwebhookconfigurations                  admissionregistration.k8s.io   false        MutatingWebhookConfiguration
    >   validatingwebhookconfigurations                admissionregistration.k8s.io   false        ValidatingWebhookConfiguration
    >   customresourcedefinitions         crd,crds     apiextensions.k8s.io           false        CustomResourceDefinition
    >   apiservices                                    apiregistration.k8s.io         false        APIService
    >   controllerrevisions                            apps                           true         ControllerRevision
    >   daemonsets                        ds           apps                           true         DaemonSet
    >   deployments                       deploy       apps                           true         Deployment
    >   replicasets                       rs           apps                           true         ReplicaSet
    >   statefulsets                      sts          apps                           true         StatefulSet
    >   auditsinks                                     auditregistration.k8s.io       false        AuditSink
    >   tokenreviews                                   authentication.k8s.io          false        TokenReview
    >   localsubjectaccessreviews                      authorization.k8s.io           true         LocalSubjectAccessReview
    >   selfsubjectaccessreviews                       authorization.k8s.io           false        SelfSubjectAccessReview
    >   selfsubjectrulesreviews                        authorization.k8s.io           false        SelfSubjectRulesReview
    >   subjectaccessreviews                           authorization.k8s.io           false        SubjectAccessReview
    >   horizontalpodautoscalers          hpa          autoscaling                    true         HorizontalPodAutoscaler
    >   cronjobs                          cj           batch                          true         CronJob
    >   jobs                                           batch                          true         Job
    >   certificatesigningrequests        csr          certificates.k8s.io            false        CertificateSigningRequest
    >   leases                                         coordination.k8s.io            true         Lease
    >   events                            ev           events.k8s.io                  true         Event
    >   daemonsets                        ds           extensions                     true         DaemonSet
    >   deployments                       deploy       extensions                     true         Deployment
    >   ingresses                         ing          extensions                     true         Ingress
    >   networkpolicies                   netpol       extensions                     true         NetworkPolicy
    >   podsecuritypolicies               psp          extensions                     false        PodSecurityPolicy
    >   replicasets                       rs           extensions                     true         ReplicaSet
    >   nodes                                          metrics.k8s.io                 false        NodeMetrics
    >   pods                                           metrics.k8s.io                 true         PodMetrics
    >   alertmanagers                                  monitoring.coreos.com          true         Alertmanager
    >   podmonitors                                    monitoring.coreos.com          true         PodMonitor
    >   prometheuses                                   monitoring.coreos.com          true         Prometheus
    >   prometheusrules                                monitoring.coreos.com          true         PrometheusRule
    >   servicemonitors                                monitoring.coreos.com          true         ServiceMonitor
    >   ingresses                         ing          networking.k8s.io              true         Ingress
    >   networkpolicies                   netpol       networking.k8s.io              true         NetworkPolicy
    >   runtimeclasses                                 node.k8s.io                    false        RuntimeClass
    >   poddisruptionbudgets              pdb          policy                         true         PodDisruptionBudget
    >   podsecuritypolicies               psp          policy                         false        PodSecurityPolicy
    >   clusterrolebindings                            rbac.authorization.k8s.io      false        ClusterRoleBinding
    >   clusterroles                                   rbac.authorization.k8s.io      false        ClusterRole
    >   rolebindings                                   rbac.authorization.k8s.io      true         RoleBinding
    >   roles                                          rbac.authorization.k8s.io      true         Role
    >   priorityclasses                   pc           scheduling.k8s.io              false        PriorityClass
    >   podpresets                                     settings.k8s.io                true         PodPreset
    >   csidrivers                                     storage.k8s.io                 false        CSIDriver
    >   csinodes                                       storage.k8s.io                 false        CSINode
    >   storageclasses                    sc           storage.k8s.io                 false        StorageClass
    >   volumeattachments                              storage.k8s.io                 false        VolumeAttachment


kubectl api-versions

    >   admissionregistration.k8s.io/v1beta1
    >   apiextensions.k8s.io/v1beta1
    >   apiregistration.k8s.io/v1
    >   apiregistration.k8s.io/v1beta1
    >   apps/v1
    >   apps/v1beta1
    >   apps/v1beta2
    >   auditregistration.k8s.io/v1alpha1
    >   authentication.k8s.io/v1
    >   authentication.k8s.io/v1beta1
    >   authorization.k8s.io/v1
    >   authorization.k8s.io/v1beta1
    >   autoscaling/v1
    >   autoscaling/v2beta1
    >   autoscaling/v2beta2
    >   batch/v1
    >   batch/v1beta1
    >   batch/v2alpha1
    >   certificates.k8s.io/v1beta1
    >   coordination.k8s.io/v1
    >   coordination.k8s.io/v1beta1
    >   custom.metrics.k8s.io/v1beta1
    >   events.k8s.io/v1beta1
    >   extensions/v1beta1
    >   metrics.k8s.io/v1beta1
    >   monitoring.coreos.com/v1
    >   networking.k8s.io/v1
    >   networking.k8s.io/v1beta1
    >   node.k8s.io/v1alpha1
    >   node.k8s.io/v1beta1
    >   policy/v1beta1
    >   rbac.authorization.k8s.io/v1
    >   rbac.authorization.k8s.io/v1alpha1
    >   rbac.authorization.k8s.io/v1beta1
    >   scheduling.k8s.io/v1
    >   scheduling.k8s.io/v1alpha1
    >   scheduling.k8s.io/v1beta1
    >   settings.k8s.io/v1alpha1
    >   storage.k8s.io/v1
    >   storage.k8s.io/v1alpha1
    >   storage.k8s.io/v1beta1
    >   v1


# -----------------------------------------------------
# Install the Kubernetes NGINX Ingress using Helm.
# https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/index.md#using-helm
# (*) K8s-NGINX rather than NGINX-NGINX
#[user@kubernator]

    helm repo add \
        ingress-nginx \
        https://kubernetes.github.io/ingress-nginx

    >   "ingress-nginx" has been added to your repositories


    helm install \
        augusta \
        ingress-nginx/ingress-nginx

    >   NAME: augusta
    >   LAST DEPLOYED: Thu Jul 30 03:10:38 2020
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 1
    >   TEST SUITE: None
    >   ....
    >   ....


# -----------------------------------------------------
# Check the controller version.
# https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/index.md#detect-installed-version
#[user@kubernator]

    POD_NAMESPACE=default
    POD_NAME=$(kubectl get pods -n $POD_NAMESPACE -l app.kubernetes.io/name=ingress-nginx --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}')

    kubectl exec -it $POD_NAME -n $POD_NAMESPACE -- /nginx-ingress-controller --version

    >   -------------------------------------------------------------------------------
    >   NGINX Ingress controller
    >     Release:       v0.34.1
    >     Build:         v20200715-ingress-nginx-2.11.0-8-gda5fa45e2
    >     Repository:    https://github.com/kubernetes/ingress-nginx
    >     nginx version: nginx/1.19.1
    >
    >   -------------------------------------------------------------------------------


# -----------------------------------------------------
# Install Dashboard using Helm.
#[user@kubernator]

    helm repo add \
        kubernetes-dashboard \
        https://kubernetes.github.io/dashboard/

    >   "kubernetes-dashboard" has been added to your repositories

    helm install \
        valeria \
        kubernetes-dashboard/kubernetes-dashboard \
        --set 'protocolHttp=true,service.externalPort=80'


    >   NAME: valeria
    >   LAST DEPLOYED: Thu Jul 30 03:12:09 2020
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 1
    >   TEST SUITE: None
    >   ....
    >   ....


# -----------------------------------------------------
# Check what was installed.
#[user@kubernator]

    kubectl get Pods

    >   NAME                                                READY   STATUS    RESTARTS   AGE
    >   augusta-ingress-nginx-controller-79c7d6ff75-n6gjd   1/1     Running   0          2m26s
    >   valeria-kubernetes-dashboard-68c8575456-q4h8t       1/1     Running   0          68s


    kubectl get Services

    >   NAME                                         TYPE           CLUSTER-IP       EXTERNAL-IP       PORT(S)                      AGE
    >   augusta-ingress-nginx-controller             LoadBalancer   10.254.57.181    128.232.227.205   80:30741/TCP,443:31460/TCP   2m58s
    >   augusta-ingress-nginx-controller-admission   ClusterIP      10.254.231.219   <none>            443/TCP                      2m58s
    >   kubernetes                                   ClusterIP      10.254.0.1       <none>            443/TCP                      14m
    >   valeria-kubernetes-dashboard                 ClusterIP      10.254.77.87     <none>            443/TCP                      100s


    kubectl get ServiceAccount

    >   NAME                           SECRETS   AGE
    >   augusta-ingress-nginx          1         3m14s
    >   default                        1         14m
    >   valeria-kubernetes-dashboard   1         117s


    kubectl get ClusterRole

    >   NAME                                                                   AGE
    >   admin                                                                  15m
    >   augusta-ingress-nginx                                                  3m35s
    >   ....
    >   ....
    >   valeria-kubernetes-dashboard-metrics                                   2m17s
    >   view                                                                   15m


    kubectl get ClusterRoleBinding

    >   NAME                                                   AGE
    >   admin                                                  15m
    >   augusta-ingress-nginx                                  4m28s
    >   ....
    >   ....
    >   valeria-kubernetes-dashboard-metrics                   3m10s


# -----------------------------------------------------
# Get a copy of the Kubernetes NGINX Ingress project.
#[user@kubernator]

    dnf install -y  git

    cd ${HOME}
    git clone https://github.com/kubernetes/ingress-nginx.git

    >   Cloning into 'ingress-nginx'...
    >   ....
    >   ....
    >   Receiving objects: 100% (95243/95243), 111.83 MiB | 1.65 MiB/s, done.
    >   Resolving deltas: 100% (53409/53409), done.


# -----------------------------------------------------
# Create our SSL keys.
# https://github.com/kubernetes/ingress-nginx/blob/master/docs/examples/PREREQUISITES.md#tls-certificates
#[user@kubernator]

    openssl req \
        -x509 \
        -sha256 \
        -nodes \
        -days 365 \
        -newkey rsa:2048 \
        -keyout tls.key \
        -out tls.crt \
        -subj "/CN=aglais-001.metagrid.xyz/O=Aglais"

    >   Generating a RSA private key
    >   ...........+++++
    >   ..+++++
    >   writing new private key to 'tls.key'
    >   -----


    kubectl create secret \
        tls \
        tls-secret \
            --key tls.key \
            --cert tls.crt

    >   secret/tls-secret created


# -----------------------------------------------------
# Deploy a test HTTP service.
# https://github.com/kubernetes/ingress-nginx/blob/master/docs/examples/PREREQUISITES.md#test-http-service
#[user@kubernator]

    pushd "${HOME}/ingress-nginx"
        pushd 'docs/examples'

            kubectl create \
                --filename http-svc.yaml

        popd
    popd


    >   deployment.apps/http-svc created
    >   service/http-svc created


    kubectl get Pod

    >   NAME                                                READY   STATUS    RESTARTS   AGE
    >   augusta-ingress-nginx-controller-79c7d6ff75-n6gjd   1/1     Running   0          22m
    >   http-svc-7cd467b9f6-s2mkc                           1/1     Running   0          10s
    >   valeria-kubernetes-dashboard-68c8575456-q4h8t       1/1     Running   0          21m


    kubectl get Service

    >   NAME                                         TYPE           CLUSTER-IP       EXTERNAL-IP       PORT(S)                      AGE
    >   augusta-ingress-nginx-controller             LoadBalancer   10.254.57.181    128.232.227.205   80:30741/TCP,443:31460/TCP   23m
    >   augusta-ingress-nginx-controller-admission   ClusterIP      10.254.231.219   <none>            443/TCP                      23m
    >   http-svc                                     ClusterIP      10.254.10.131    <none>            80/TCP                       49s
    >   kubernetes                                   ClusterIP      10.254.0.1       <none>            443/TCP                      34m
    >   valeria-kubernetes-dashboard                 ClusterIP      10.254.77.87     <none>            443/TCP                      22m


    kubectl get \
        Service http-svc \
        --output json

    >   {
    >       "apiVersion": "v1",
    >       "kind": "Service",
    >       "metadata": {
    >           "creationTimestamp": "2020-07-30T03:33:22Z",
    >           "labels": {
    >               "app": "http-svc"
    >           },
    >           "name": "http-svc",
    >           "namespace": "default",
    >           "resourceVersion": "7917",
    >           "selfLink": "/api/v1/namespaces/default/services/http-svc",
    >           "uid": "0d8be29f-21f8-4d31-b3e7-aed8cc0f1103"
    >       },
    >       "spec": {
    >           "clusterIP": "10.254.10.131",
    >           "ports": [
    >               {
    >                   "name": "http",
    >                   "port": 80,
    >                   "protocol": "TCP",
    >                   "targetPort": 8080
    >               }
    >           ],
    >           "selector": {
    >               "app": "http-svc"
    >           },
    >           "sessionAffinity": "None",
    >           "type": "ClusterIP"
    >       },
    >       "status": {
    >           "loadBalancer": {}
    >       }
    >   }


# -----------------------------------------------------
# Test our HTTP service.
#[user@kubernator]

    kubectl patch \
        Service http-svc \
            -p '{"spec":{"type": "LoadBalancer"}}'

    >   service/http-svc patched

    watch \
        kubectl get \
            Service http-svc

    >   NAME       TYPE           CLUSTER-IP      EXTERNAL-IP       PORT(S)        AGE
    >   http-svc   LoadBalancer   10.254.10.131   128.232.227.231   80:32169/TCP   4m3s


    kubectl get \
        Service http-svc \
        --output json

    >   {
    >       "apiVersion": "v1",
    >       "kind": "Service",
    >       "metadata": {
    >           "creationTimestamp": "2020-07-30T03:33:22Z",
    >           "labels": {
    >               "app": "http-svc"
    >           },
    >           "name": "http-svc",
    >           "namespace": "default",
    >           "resourceVersion": "8541",
    >           "selfLink": "/api/v1/namespaces/default/services/http-svc",
    >           "uid": "0d8be29f-21f8-4d31-b3e7-aed8cc0f1103"
    >       },
    >       "spec": {
    >           "clusterIP": "10.254.10.131",
    >           "externalTrafficPolicy": "Cluster",
    >           "ports": [
    >               {
    >                   "name": "http",
    >                   "nodePort": 32169,
    >                   "port": 80,
    >                   "protocol": "TCP",
    >                   "targetPort": 8080
    >               }
    >           ],
    >           "selector": {
    >               "app": "http-svc"
    >           },
    >           "sessionAffinity": "None",
    >           "type": "LoadBalancer"
    >       },
    >       "status": {
    >           "loadBalancer": {
    >               "ingress": [
    >                   {
    >                       "ip": "128.232.227.231"
    >                   }
    >               ]
    >           }
    >       }
    >   }


    kubectl describe \
        Service http-svc

    >   Name:                     http-svc
    >   Namespace:                default
    >   Labels:                   app=http-svc
    >   Annotations:              <none>
    >   Selector:                 app=http-svc
    >   Type:                     LoadBalancer
    >   IP:                       10.254.10.131
    >   LoadBalancer Ingress:     128.232.227.231
    >   Port:                     http  80/TCP
    >   TargetPort:               8080/TCP
    >   NodePort:                 http  32169/TCP
    >   Endpoints:                10.100.2.8:8080
    >   Session Affinity:         None
    >   External Traffic Policy:  Cluster
    >   Events:
    >     Type    Reason                Age    From                Message
    >     ----    ------                ----   ----                -------
    >     Normal  Type                  2m51s  service-controller  ClusterIP -> LoadBalancer
    >     Normal  EnsuringLoadBalancer  2m51s  service-controller  Ensuring load balancer
    >     Normal  EnsuredLoadBalancer   90s    service-controller  Ensured load balancer


    ingressip=$(
        kubectl get \
            Service http-svc \
                --output json \
        | jq -r '.status.loadBalancer.ingress[0].ip'
        )

    echo "Ingress [${ingressip:?}]"

    >   Ingress [128.232.227.231]



    curl "http://${ingressip:?}/"

    >   Hostname: http-svc-7cd467b9f6-s2mkc
    >
    >   Pod Information:
    >   	node name:	tiberius-izdzbnpgytcb-node-3
    >   	pod name:	http-svc-7cd467b9f6-s2mkc
    >   	pod namespace:	default
    >   	pod IP:	10.100.2.8
    >
    >   Server values:
    >   	server_version=nginx: 1.12.2 - lua: 10010
    >
    >   Request Information:
    >   	client_address=10.100.1.0
    >   	method=GET
    >   	real path=/
    >   	query=
    >   	request_version=1.1
    >   	request_scheme=http
    >   	request_uri=http://128.232.227.231:8080/
    >
    >   Request Headers:
    >   	accept=*/*
    >   	host=128.232.227.231
    >   	user-agent=curl/7.69.1
    >
    >   Request Body:
    >   	-no body in request-


# -----------------------------------------------------
# Re-deploy the test HTTP service.
# https://github.com/kubernetes/ingress-nginx/blob/master/docs/examples/PREREQUISITES.md#test-http-service
#[user@kubernator]

    kubectl delete \
        Deployment http-svc

    >   deployment.extensions "http-svc" deleted


    kubectl delete \
        Service http-svc

    >   service "http-svc" deleted


    sleep 5

    pushd "${HOME}/ingress-nginx"
        pushd 'docs/examples'

            kubectl create \
                --filename http-svc.yaml

        popd
    popd


    >   deployment.apps/http-svc created
    >   service/http-svc created


# -----------------------------------------------------
# Test our SSL keys.
# https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/tls-termination#deployment
# https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/tls-termination#validation
#[user@kubernator]

    hostname=aglais-001.metagrid.xyz
    certname=tls-secret

    cat << EOF > /tmp/test-ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: nginx-test
spec:
  tls:
    - hosts:
      - ${hostname}
      secretName: ${certname}
  rules:
    - host: ${hostname}
      http:
        paths:
        - path: /
          backend:
            # This assumes http-svc exists and routes to healthy endpoints
            serviceName: http-svc
            servicePort: 80
EOF


    kubectl create \
        --filename /tmp/test-ingress.yaml

    >   ingress.networking.k8s.io/nginx-test created


    kubectl describe \
        Ingress nginx-test

    >   Name:             nginx-test
    >   Namespace:        default
    >   Address:          128.232.227.205
    >   Default backend:  default-http-backend:80 (<none>)
    >   TLS:
    >     tls-secret terminates aglais-001.metagrid.xyz
    >   Rules:
    >     Host                     Path  Backends
    >     ----                     ----  --------
    >     aglais-001.metagrid.xyz
    >                              /   http-svc:80 (10.100.3.7:8080)
    >   Annotations:
    >   Events:
    >     Type    Reason  Age   From                      Message
    >     ----    ------  ----  ----                      -------
    >     Normal  CREATE  35s   nginx-ingress-controller  Ingress default/nginx-test
    >     Normal  UPDATE  26s   nginx-ingress-controller  Ingress default/nginx-test


    kubectl get \
        Ingress nginx-test \
            --output json

    >   {
    >       "apiVersion": "extensions/v1beta1",
    >       "kind": "Ingress",
    >       "metadata": {
    >           "creationTimestamp": "2020-07-30T03:41:55Z",
    >           "generation": 1,
    >           "name": "nginx-test",
    >           "namespace": "default",
    >           "resourceVersion": "9641",
    >           "selfLink": "/apis/extensions/v1beta1/namespaces/default/ingresses/nginx-test",
    >           "uid": "1cd45782-7e97-4487-b961-f4b22e766c87"
    >       },
    >       "spec": {
    >           "rules": [
    >               {
    >                   "host": "aglais-001.metagrid.xyz",
    >                   "http": {
    >                       "paths": [
    >                           {
    >                               "backend": {
    >                                   "serviceName": "http-svc",
    >                                   "servicePort": 80
    >                               },
    >                               "path": "/"
    >                           }
    >                       ]
    >                   }
    >               }
    >           ],
    >           "tls": [
    >               {
    >                   "hosts": [
    >                       "aglais-001.metagrid.xyz"
    >                   ],
    >                   "secretName": "tls-secret"
    >               }
    >           ]
    >       },
    >       "status": {
    >           "loadBalancer": {
    >               "ingress": [
    >                   {
    >                       "ip": "128.232.227.205"
    >                   }
    >               ]
    >           }
    >       }
    >   }

    ingressip=$(
        kubectl get \
            Ingress nginx-test \
                --output json \
        | jq -r '.status.loadBalancer.ingress[0].ip'
        )

    echo "Ingress [${ingressip:?}]"


    >   Ingress [128.232.227.205]


    curl --head "http://${ingressip:?}/"

    >   HTTP/1.1 404 Not Found
    >   Server: nginx/1.19.1
    >   Date: Thu, 30 Jul 2020 03:48:53 GMT
    >   Content-Type: text/html
    >   Content-Length: 153
    >   Connection: keep-alive


    curl --head "http://aglais-001.metagrid.xyz/"

    >   HTTP/1.1 308 Permanent Redirect
    >   Server: nginx/1.19.1
    >   Date: Thu, 30 Jul 2020 03:49:30 GMT
    >   Content-Type: text/html
    >   Content-Length: 171
    >   Connection: keep-alive
    >   Location: https://aglais-001.metagrid.xyz/


    curl --head "https://aglais-001.metagrid.xyz/"

    >   curl: (60) SSL certificate problem: self signed certificate
    >   More details here: https://curl.haxx.se/docs/sslcerts.html
    >   ....
    >   ....


    curl --insecure --head "https://aglais-001.metagrid.xyz/"

    >   HTTP/2 200
    >   server: nginx/1.19.1
    >   date: Thu, 30 Jul 2020 03:51:32 GMT
    >   content-type: text/plain
    >   vary: Accept-Encoding
    >   strict-transport-security: max-age=15724800; includeSubDomains


# -----------------------------------------------------
# Remove the test components.
#[user@kubernator]

    kubectl delete \
        Ingress nginx-test

    >   ingress.extensions "nginx-test" deleted


    kubectl delete \
        Deployment http-svc

    >   deployment.extensions "http-svc" deleted


# -----------------------------------------------------
# Create our Google OAuth resource.
#[user@google]


    OAuth2 Proxy - Google Auth Provider
    https://oauth2-proxy.github.io/oauth2-proxy/auth-configuration#google-auth-provider

    Setting up OAuth 2.0
    https://support.google.com/cloud/answer/6158849?hl=en

    To use OAuth 2.0 in your application, you need an OAuth 2.0 client ID, which your application uses when requesting an OAuth 2.0 access token.

    To create an OAuth 2.0 client ID in the console:

        Go to the Google Cloud Platform Console.
        https://console.cloud.google.com/home/dashboard?project=aglais-login&pli=1

        From the projects list, select a project or create a new one.
        https://console.cloud.google.com/iam-admin/settings?project=aglais-login&pli=1

        If the APIs & services page isn't already open, open the console left side menu and select APIs & services.
        On the left, click Credentials.
        https://console.cloud.google.com/apis/credentials?project=aglais-login

        Click New Credentials, then select OAuth client ID.
        Select the appropriate application type for your project and enter any additional information required. Application types are described in more detail in the following sections.

            Application type : Web application
            Application name : Kubernetes login
            Origin URIs   : https://aglais-001.metagrid.xyz
            Redirect URIs : https://aglais-001.metagrid.xyz/oauth2/callback

            Client ID     : 10....76.apps.googleusercontent.com
            Client secret : Xv....Nk

# -----------------------------------------------------
# Configure the oauth2_proxy settings.
#[user@kubernator]

    OAUTH2_PROXY_CLIENT_ID=10....76.apps.googleusercontent.com
    OAUTH2_PROXY_CLIENT_SECRET=Xv....Nk
    OAUTH2_PROXY_COOKIE_SECRET=$(
        python -c 'import os,base64; print(base64.b64encode(os.urandom(16)).decode("ascii"))'
        )

    pushd "${HOME}/ingress-nginx"
        pushd 'docs/examples/auth/oauth-external-auth'

            sed '
                s/provider=github/provider=google/
                s/value: <Client ID>/value: '${OAUTH2_PROXY_CLIENT_ID}'/
                s/value: <Client Secret>/value: '${OAUTH2_PROXY_CLIENT_SECRET}'/
                s/value: SECRET/value: '${OAUTH2_PROXY_COOKIE_SECRET}'/
                ' oauth2-proxy.yaml \
            | tee /tmp/oauth2-proxy.yaml

        popd
    popd


    >   apiVersion: apps/v1
    >   kind: Deployment
    >   metadata:
    >     labels:
    >       k8s-app: oauth2-proxy
    >     name: oauth2-proxy
    >     namespace: kube-system
    >   spec:
    >     replicas: 1
    >     selector:
    >       matchLabels:
    >         k8s-app: oauth2-proxy
    >     template:
    >       metadata:
    >         labels:
    >           k8s-app: oauth2-proxy
    >       spec:
    >         containers:
    >         - args:
    >           - --provider=google
    >           - --email-domain=*
    >           - --upstream=file:///dev/null
    >           - --http-address=0.0.0.0:4180
    >           # Register a new application
    >           # https://github.com/settings/applications/new
    >           env:
    >           - name: OAUTH2_PROXY_CLIENT_ID
    >             value: 10....76.apps.googleusercontent.com
    >           - name: OAUTH2_PROXY_CLIENT_SECRET
    >             value: Xv....Nk
    >           # docker run -ti --rm python:3-alpine python -c 'import secrets,base64; print(base64.b64encode(base64.b64encode(secrets.token_bytes(16))));'
    >           - name: OAUTH2_PROXY_COOKIE_SECRET
    >             value: B5....==
    >           image: quay.io/oauth2-proxy/oauth2-proxy:latest
    >           imagePullPolicy: Always
    >           name: oauth2-proxy
    >           ports:
    >           - containerPort: 4180
    >             protocol: TCP
    >
    >   ---
    >
    >   apiVersion: v1
    >   kind: Service
    >   metadata:
    >     labels:
    >       k8s-app: oauth2-proxy
    >     name: oauth2-proxy
    >     namespace: kube-system
    >   spec:
    >     ports:
    >     - name: http
    >       port: 4180
    >       protocol: TCP
    >       targetPort: 4180
    >     selector:
    >       k8s-app: oauth2-proxy


# -----------------------------------------------------
# Configure the OAuth Ingress settings.
#[user@kubernator]

# The example ingress refers to the wrong copy of dashboard.
# There are two copies of dashboard in two namespaces
#   kube-system:kubernetes-dashboard
#   default:valeria-kubernetes-dashboard
# Ours is the second one, in the default namespace.

# The example ingress is in the wrong namespace.
# https://github.com/kubernetes/kubernetes/issues/17088

    hostname=aglais-001.metagrid.xyz
    certname=tls-secret
    dashname=kubernetes-dashboard
    dashname=valeria-kubernetes-dashboard

# TODO set the *first* serviceName to ${dashname}

    pushd "${HOME}/ingress-nginx"
        pushd 'docs/examples/auth/oauth-external-auth'

            sed '
                /namespace:/d
                s/serviceName: kubernetes-dashboard/serviceName: valeria-kubernetes-dashboard/
                s/__INGRESS_HOST__/'${hostname:?}'/
                s/__INGRESS_SECRET__/'${certname:?}'/
                ' dashboard-ingress.yaml \
            | tee /tmp/dashboard-ingress.yaml

        popd
    popd


    >   apiVersion: networking.k8s.io/v1beta1
    >   kind: Ingress
    >   metadata:
    >     annotations:
    >       nginx.ingress.kubernetes.io/auth-url: "https://$host/oauth2/auth"
    >       nginx.ingress.kubernetes.io/auth-signin: "https://$host/oauth2/start?rd=$escaped_request_uri"
    >     name: external-auth-oauth2
    >     namespace: kube-system
    >   spec:
    >     rules:
    >     - host: aglais-001.metagrid.xyz
    >       http:
    >         paths:
    >         - backend:
    >             serviceName: valeria-kubernetes-dashboard
    >             servicePort: 80
    >           path: /
    >
    >   ---
    >
    >   apiVersion: networking.k8s.io/v1beta1
    >   kind: Ingress
    >   metadata:
    >     name: oauth2-proxy
    >     namespace: kube-system
    >   spec:
    >     rules:
    >     - host: aglais-001.metagrid.xyz
    >       http:
    >         paths:
    >         - backend:
    >             serviceName: oauth2-proxy
    >             servicePort: 4180
    >           path: /oauth2
    >     tls:
    >     - hosts:
    >       - aglais-001.metagrid.xyz
    >       secretName: tls-secret


# -----------------------------------------------------
# Deploy the Ingress and OAuth proxy.
#[user@kubernator]

    kubectl create \
        --filename /tmp/oauth2-proxy.yaml

    >   deployment.apps/oauth2-proxy created
    >   service/oauth2-proxy created


    kubectl create \
        --filename /tmp/dashboard-ingress.yaml

    >   ingress.networking.k8s.io/external-auth-oauth2 created
    >   ingress.networking.k8s.io/oauth2-proxy created


# -----------------------------------------------------
# -----------------------------------------------------
# Test the deployment .
#[user@desktop]

    firefox "https://aglais-001.metagrid.xyz/" &

    >   503 Service Temporarily Unavailable
    >   ....

    # Logout from our Google account

        ....

    # Manually go to the auth-url

    https://aglais-001.metagrid.xyz/oauth2/auth
    - fails 'unauthorized request'

    https://aglais-001.metagrid.xyz/oauth2
    - works, [Login with Google]

    # Redirected back to main page
    https://aglais-001.metagrid.xyz/oauth2/auth
    - fails '503 Service Temporarily Unavailable'


kubectl get ingress -A

kubectl --namespace kube-system describe service kubernetes-dashboard
kubectl --namespace default describe service valeria-kubernetes-dashboard

kubectl --namespace kube-system describe ingress external-auth-oauth2
kubectl --namespace kube-system describe ingress oauth2-proxy

    # Either dashboard isn't working - or it is not connected properly.
    # TODO Use kubctl proxy to find out ..


# -----------------------------------------------------
# -----------------------------------------------------
# Attach kubectl proxy to the valeria dashboard instance.
#[user@kubernator]

    kubectl \
        port-forward \
            valeria-kubernetes-dashboard-5d664844fb-lfxqm \
            9090:9090


# -----------------------------------------------------
# -----------------------------------------------------
# Point firefox at the proxy port.
#[user@desktop]

    firefox "https://localhost:9090/#/login" &

    # We get an empty dashboard.
    # No login required for the dashboard.
    # Dashboard doesn't have access to any of the metrics.

        pods is forbidden: User "system:serviceaccount:default:valeria-kubernetes-dashboard"
        cannot list resource "pods" in API group "" in the namespace "default"

        daemonsets.apps is forbidden: User "system:serviceaccount:default:valeria-kubernetes-dashboard"
        cannot list resource "daemonsets" in API group "apps" in the namespace "default"

        namespaces is forbidden: User "system:serviceaccount:default:valeria-kubernetes-dashboard"
        cannot list resource "namespaces" in API group "" at the cluster scope


# -----------------------------------------------------
# -----------------------------------------------------
# Attach kubectl proxy to the kube-system dashboard.
#[user@kubernator]

    kubectl \
        --namespace kube-system \
        port-forward \
            kubernetes-dashboard-6bcf74b4cd-7rmss \
            9090:8443


# -----------------------------------------------------
# -----------------------------------------------------
# Point firefox at the proxy port.
#[user@desktop]

    firefox "https://localhost:9090/" &

        An error occurred during a connection to localhost:9090. PR_CONNECT_RESET_ERROR


    curl 'https://localhost:9090/'

        curl: (7) Failed to connect to localhost port 9090: Connection refused

    # Suspect the dashboard instance is unwell ?




