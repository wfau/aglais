#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    # Zeppelin instructions for creating a Docker image of Spark.
    # http://zeppelin.apache.org/docs/0.9.0-preview1/quickstart/kubernetes.html#spark-interpreter

        Build spark docker image to use Spark Interpreter.
        Download spark binary distribution and run following command.

            <spark-distribution>/bin/docker-image-tool.sh -m -t 2.4.0 build


    # Spark instructions for creating a Docker image of Spark.
    # https://spark.apache.org/docs/latest/running-on-kubernetes.html#docker-images

        Spark (starting with version 2.3) ships with a Dockerfile that can be used for this purpose,
        or customized to match an individual applicationâ€™s needs.
        It can be found in the kubernetes/dockerfiles/ directory.

        Spark also ships with a bin/docker-image-tool.sh script that can be used to build and publish
        the Docker images to use with the Kubernetes backend.




# -----------------------------------------------------
# Create local directory to work with.
#[user@desktop]

cat >> "${HOME}/aglais.env" << 'EOF'

SPARK_HOME="${PROJECTS_ROOT:?}/WFAU/aglais-spark"
SPARK_BINS="${SPARK_HOME:?}/dist"

EOF

    source "${HOME}/aglais.env"

    mkdir -p "${SPARK_BINS:?}"


# -----------------------------------------------------
# Download the Spark binary.
#[user@desktop]

    source "${HOME}/aglais.env"

    sparkversion=3.0.0
    sparkname=spark-${sparkversion:?}
    sparkdist=${sparkname:?}-bin-hadoop3.2
    sparkgzip=${sparkdist:?}.tgz


    pushd "${SPARK_BINS:?}"
        pushd "${sparkname:?}"

            wget "https://downloads.apache.org/spark/${sparkname:?}/${sparkgzip:?}"

            tar -xvzf "${sparkgzip:?}"

        popd
    popd


# -----------------------------------------------------
# Build our Spark images.
#[user@desktop]

    source "${HOME}/aglais.env"
    sparkpath=${SPARK_BINS:?}/${sparkname:?}/${sparkdist:?}

    buildtag=$(date '+%Y.%m.%d')
    buildtime=$(date '+%Y-%m-%dT%H:%M:%S')

    buildah bud \
        --tag aglais/spark:latest \
        --tag aglais/spark:${buildtag:?} \
        --file "${sparkpath:?}/kubernetes/dockerfiles/spark/Dockerfile" \
        "${sparkpath:?}"

    >   STEP 1: FROM openjdk:8-jre-slim
    >   STEP 2: ARG spark_uid=185
    >   STEP 3: RUN set -ex &&     sed -i 's/http:/https:/g' /etc/apt/sources.list &&     apt-get update &&     ln -s /lib /lib64 &&     apt install -y bash tini libc6 libpam-modules krb5-user libnss3 &&     mkdir -p /opt/spark &&     mkdir -p /opt/spark/examples &&     mkdir -p /opt/spark/work-dir &&     touch /opt/spark/RELEASE &&     rm /bin/sh &&     ln -sv /bin/bash /bin/sh &&     echo "auth required pam_wheel.so use_uid" >> /etc/pam.d/su &&     chgrp root /etc/passwd && chmod ug+rw /etc/passwd &&     rm -rf /var/cache/apt/*
    >   + sed -i s/http:/https:/g /etc/apt/sources.list
    >   + apt-get update
    >   Ign:1 https://security.debian.org/debian-security buster/updates InRelease
    >   Get:2 https://deb.debian.org/debian buster InRelease [121 kB]
    >   Err:3 https://security.debian.org/debian-security buster/updates Release
    >     Certificate verification failed: The certificate is NOT trusted. The name in the certificate does not match the expected.  Could not handshake: Error in the certificate verification. [IP: 2a04:4e42:600::204 443]
    >   ....
    >   ....

    # Errors trying to update packages.



# -----------------------------------------------------
# Experiment with the base container image.
#[user@desktop]

    podman run \
        -it \
        --rm \
        openjdk:8-jre-slim \
            bash

        apt-get update

    >   Get:1 http://security.debian.org/debian-security buster/updates InRelease [65.4 kB]
    >   Get:2 http://deb.debian.org/debian buster InRelease [121 kB]
    >   Get:3 http://deb.debian.org/debian buster-updates InRelease [51.9 kB]
    >   Get:4 http://security.debian.org/debian-security buster/updates/main amd64 Packages [213 kB]
    >   Get:5 http://deb.debian.org/debian buster/main amd64 Packages [7905 kB]
    >   Get:6 http://deb.debian.org/debian buster-updates/main amd64 Packages [7868 B]
    >   Fetched 8364 kB in 6s (1341 kB/s)
    >   Reading package lists... Done

        # That works OK.

# -----------------------------------------------------
# Experiment with the base container image.
#[user@desktop]

    podman run \
        -it \
        --rm \
        openjdk:8-jre-slim \
            bash

        sed -i s/http:/https:/g /etc/apt/sources.list
        apt-get update

    >   Ign:1 https://security.debian.org/debian-security buster/updates InRelease
    >   Get:2 https://deb.debian.org/debian buster InRelease [121 kB]
    >   Err:3 https://security.debian.org/debian-security buster/updates Release
    >     Certificate verification failed: The certificate is NOT trusted. The name in the certificate does not match the expected.  Could not handshake: Error in the certificate verification. [IP: 2a04:4e42:800::204 443]
    >   ....
    >   ....

    # Problems with running apt-get update via https


# -----------------------------------------------------
# Edit the Dockerfile to remove the sed HTTPS line
#[user@desktop]

    gedit "${sparkpath:?}/kubernetes/dockerfiles/spark/Dockerfile" &

        RUN set -ex && \
    -       sed -i 's/http:/https:/g' /etc/apt/sources.list && \
            apt-get update && \


# -----------------------------------------------------
# Build our Spark images.
#[user@desktop]

    source "${HOME}/aglais.env"
    sparkpath=${SPARK_BINS:?}/${sparkname:?}/${sparkdist:?}

    buildtag=$(date '+%Y.%m.%d')
    buildtime=$(date '+%Y-%m-%dT%H:%M:%S')

    buildah bud \
        --tag aglais/spark:latest \
        --tag aglais/spark:${buildtag:?} \
        --file "${sparkpath:?}/kubernetes/dockerfiles/spark/Dockerfile" \
        "${sparkpath:?}"

    >   ....
    >   ....
    >   Setting up krb5-config (2.6) ...
    >   debconf: unable to initialize frontend: Dialog
    >   debconf: (TERM is not set, so the dialog frontend is not usable.)
    >   debconf: falling back to frontend: Readline
    >   debconf: unable to initialize frontend: Readline
    >   debconf: (Can't locate Term/ReadLine.pm in @INC (you may need to install the Term::ReadLine module) (@INC contains: /etc/perl /usr/local/lib/x86_64-linux-gnu/perl/5.28.1 /usr/local/share/perl/5.28.1 /usr/lib/x86_64-linux-gnu/perl5/5.28 /
    >   usr/share/perl5 /usr/lib/x86_64-linux-gnu/perl/5.28 /usr/share/perl/5.28 /usr/local/lib/site_perl /usr/lib/x86_64-linux-gnu/perl-base) at /usr/share/perl5/Debconf/FrontEnd/Readline.pm line 7.)
    >   debconf: falling back to frontend: Teletype
    >   Configuring Kerberos Authentication
    >   -----------------------------------
    >   
    >   When users attempt to use Kerberos and specify a principal or user name without specifying what administrative Kerberos realm that principal belongs to, the system appends the default realm.  The default realm may also be used as the
    >   realm of a Kerberos service running on the local machine.  Often, the default realm is the uppercase version of the local DNS domain.
    >   
    >   Default Kerberos version 5 realm:

    # The install is interactive !!!


# -----------------------------------------------------
# Edit the Dockerfile to add the noninteractive flag.
#[user@desktop]

    gedit "${sparkpath:?}/kubernetes/dockerfiles/spark/Dockerfile" &

    +   ENV DEBIAN_FRONTEND=noninteractive
        RUN set -ex && \
            apt-get update && \
            ....


# -----------------------------------------------------
# Build our Spark images.
#[user@desktop]

    source "${HOME}/aglais.env"
    sparkpath=${SPARK_BINS:?}/${sparkname:?}/${sparkdist:?}

    buildtag=$(date '+%Y.%m.%d')
    buildtime=$(date '+%Y-%m-%dT%H:%M:%S')

    buildah bud \
        --tag aglais/spark:latest \
        --tag aglais/spark:${buildtag:?} \
        --file "${sparkpath:?}/kubernetes/dockerfiles/spark/Dockerfile" \
        "${sparkpath:?}"



    >   STEP 1: FROM openjdk:8-jre-slim
    >   STEP 2: ARG spark_uid=185
    >   STEP 3: ENV DEBIAN_FRONTEND=noninteractive
    >   STEP 4: RUN set -ex &&     apt-get update &&     ln -s /lib /lib64 &&     apt install -y bash tini libc6 libpam-modules krb5-user libnss3 &&     mkdir -p /opt/spark &&     mkdir -p /opt/spark/examples &&     mkdir -p /opt/spark/work-dir &&     touch /opt/spark/RELEASE &&     rm /bin/sh &&     ln -sv /bin/bash /bin/sh &&     echo "auth required pam_wheel.so use_uid" >> /etc/pam.d/su &&     chgrp root /etc/passwd && chmod ug+rw /etc/passwd &&     rm -rf /var/cache/apt/*
    >   + apt-get update
    >   Get:1 http://security.debian.org/debian-security buster/updates InRelease [65.4 kB]
    >   Get:2 http://deb.debian.org/debian buster InRelease [121 kB]
    >   Get:3 http://deb.debian.org/debian buster-updates InRelease [51.9 kB]
    >   Get:4 http://security.debian.org/debian-security buster/updates/main amd64 Packages [213 kB]
    >   Get:5 http://deb.debian.org/debian buster/main amd64 Packages [7905 kB]
    >   Get:6 http://deb.debian.org/debian buster-updates/main amd64 Packages [7868 B]
    >   Fetched 8364 kB in 6s (1292 kB/s)
    >   ...
    >   ...
    >   ...
    >   STEP 5: COPY jars /opt/spark/jars
    >   STEP 6: COPY bin /opt/spark/bin
    >   STEP 7: COPY sbin /opt/spark/sbin
    >   STEP 8: COPY kubernetes/dockerfiles/spark/entrypoint.sh /opt/
    >   STEP 9: COPY examples /opt/spark/examples
    >   STEP 10: COPY kubernetes/tests /opt/spark/tests
    >   STEP 11: COPY data /opt/spark/data
    >   STEP 12: ENV SPARK_HOME /opt/spark
    >   STEP 13: WORKDIR /opt/spark/work-dir
    >   STEP 14: RUN chmod g+w /opt/spark/work-dir
    >   STEP 15: ENTRYPOINT [ "/opt/entrypoint.sh" ]
    >   STEP 16: USER ${spark_uid}
    >   STEP 17: COMMIT aglais/spark:latest
    >   Getting image source signatures
    >   Copying blob 13cb14c2acd3 skipped: already exists
    >   Copying blob 94cf29cec5e1 skipped: already exists
    >   Copying blob 91383663cf66 skipped: already exists
    >   Copying blob 40760429743e skipped: already exists
    >   Copying blob 6a4b5397838a done
    >   Copying config b1d0b2c1de done
    >   Writing manifest to image destination
    >   Storing signatures
    >   --> b1d0b2c1deb
    >   b1d0b2c1deb211a65be918656182372a7e84e9774cc7a76c994a72825283e370

    # Yay - works :-D

# -----------------------------------------------------
# Login to the Docker registry.
#[user@desktop]

    podman login \
        --username $(secret docker.io.user) \
        --password $(secret docker.io.pass) \
        registry-1.docker.io


# -----------------------------------------------------
# Push our images to Docker hub.
#[user@desktop]

    podman push "aglais/spark:${buildtag:?}"

    >   ....
    >   ....


    podman push "aglais/spark:latest"

    >   Getting image source signatures
    >   Copying blob 6a4b5397838a skipped: already exists
    >   Copying blob 91383663cf66 skipped: already exists
    >   Copying blob 13cb14c2acd3 skipped: already exists
    >   Copying blob 40760429743e skipped: already exists
    >   Copying blob 94cf29cec5e1 [--------------------------------------] 0.0b / 0.0b
    >   Copying config b1d0b2c1de [--------------------------------------] 0.0b / 8.0KiB
    >   Writing manifest to image destination
    >   Storing signatures



# -----------------------------------------------------
# Build our PySpark images.
#[user@desktop]

    source "${HOME}/aglais.env"
    sparkpath=${SPARK_BINS:?}/${sparkname:?}/${sparkdist:?}

    #buildtag=$(date '+%Y.%m.%d')
    #buildtime=$(date '+%Y-%m-%dT%H:%M:%S')

    buildah bud \
        --build-arg "base_img=aglais/spark:${buildtag:?}" \
        --tag aglais/pyspark:latest \
        --tag aglais/pyspark:${buildtag:?} \
        --file "${sparkpath:?}/kubernetes/dockerfiles/spark/bindings/python/Dockerfile" \
        "${sparkpath:?}"


    >   STEP 1: FROM aglais/spark:2020.07.22
    >   STEP 2: WORKDIR /
    >   STEP 3: USER 0
    >   STEP 4: RUN mkdir ${SPARK_HOME}/python
    >   STEP 5: RUN apt-get update &&     apt install -y python python-pip &&     apt install -y python3 python3-pip &&     rm -r /usr/lib/python*/ensurepip &&     pip install --upgrade pip setuptools &&     rm -r /root/.cache && rm -rf /var/cache/apt/*
    >   ....
    >   ....
    >   Writing manifest to image destination
    >   Storing signatures
    >   --> 80c6e7f67e8
    >   80c6e7f67e84e2cf4b577719857cb07c30d6e28314ac817810ba0978123c71c2


# -----------------------------------------------------
# Push our images to Docker hub.
#[user@desktop]

    podman push "aglais/pyspark:${buildtag:?}"

    >   Getting image source signatures
    >   Copying blob 4115f9151517 [=========================>------------] 312.0MiB / 457.2MiB
    >   Copying blob 40760429743e skipped: already exists
    >   Copying blob 13cb14c2acd3 skipped: already exists
    >   Copying blob 94cf29cec5e1 skipped: already exists
    >   Copying blob 6a4b5397838a skipped: already exists
    >   Copying blob 91383663cf66 skipped: already exists
    >   ....
    >   ....


    podman push "aglais/pyspark:latest"


    >   Getting image source signatures
    >   Copying blob 91383663cf66 skipped: already exists
    >   Copying blob 13cb14c2acd3 skipped: already exists
    >   Copying blob 6a4b5397838a skipped: already exists
    >   Copying blob 40760429743e skipped: already exists
    >   Copying blob 94cf29cec5e1 skipped: already exists
    >   Copying blob 4115f9151517 [--------------------------------------] 0.0b / 0.0b
    >   Copying config 80c6e7f67e [--------------------------------------] 0.0b / 9.7KiB
    >   Writing manifest to image destination
    >   Storing signatures



    # TODO Create modified images.

        Create a 'mod' image that just inherits the main image.

        aglais/zeppelinbase
        aglais/zeppelin
        aglais/zeppelinmod

        aglais/spark
        aglais/sparkmod

        aglais/pyspark
        aglais/pysparkmod

    # TODO Create a Podman based build container.

        Java, Maven
        Podman, buildah

    # TODO Create a Fedora based Zeppelin container.

    # TODO Create a Fedora based Spark container.


# -----------------------------------------------------
# Update our Zeppelin deployment template.
#[user@kubernator]

    mapname=zeppelin-server-conf-map

    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        patch \
            configmap "${mapname:?}" \
            -p '{
                "data":{
                    "ZEPPELIN_K8S_CONTAINER_IMAGE":"aglais/zeppelin:latest",
                    "ZEPPELIN_K8S_SPARK_CONTAINER_IMAGE":"aglais/pyspark:latest"
                    }
                }'



    #
    # Got it to launch a Spark interpreter once.
    # It reported an out of memory error and failed.
    # Since then, failing to download the image from the docker registry.
    #

    #
    # TODO Upgrade our cluster from tiny to large instances.
    # TODO Figure out SSL conenction to Zeppelin
    # TODO Figure out SSL conenction to Dashboard
    #







