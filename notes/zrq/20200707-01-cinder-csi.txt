#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    #
    # Try installing the Cinder CSI driver.
    # https://kubernetes.io/blog/2020/02/07/deploying-external-openstack-cloud-provider-with-kubeadm/#deploy-cinder-csi


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --hostname kubernator \
        --volume "${HOME}/clouds.yaml:/etc/openstack/clouds.yaml:z" \
        --volume "${AGLAIS_CODE}/experiments/zrq/kubernetes:/kubernetes:z" \
        atolmis/openstack-client \
        bash


# -----------------------------------------------------
# Set the cloud, credentials and cluster names.
#[user@kubernator]

    cloudname=gaia-prod
    clustername=drupal-one


# -----------------------------------------------------
# Get the connection details for our cluster.
#[user@kubernator]

    mkdir -p "${HOME}/.kube/${clustername:?}"
    openstack \
        --os-cloud "${cloudname:?}-super" \
        coe cluster config \
            "${clustername:?}" \
                --force \
                --dir "${HOME}/.kube/${clustername:?}"


# -----------------------------------------------------
# Check kubectl can get the connection details for our cluster.
#[user@kubernator]

    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        cluster-info

    >   Kubernetes master is running at https://128.232.227.226:6443
    >   Heapster is running at https://128.232.227.226:6443/api/v1/namespaces/kube-system/services/heapster/proxy
    >   CoreDNS is running at https://128.232.227.226:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy


# -----------------------------------------------------
# Create a secret with CA certs for OpenStack's API endpoints.
#[user@kubernator]

    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        create secret \
            --namespace kube-system \
                generic openstack-ca-cert

    >   secret/openstack-ca-cert created


    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        --namespace kube-system \
        describe secret \
            openstack-ca-cert

    >   Name:         openstack-ca-cert
    >   Namespace:    kube-system
    >   Labels:       <none>
    >   Annotations:  <none>
    >   
    >   Type:  Opaque
    >   
    >   Data
    >   ====


    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        --namespace kube-system \
        get secret \
            openstack-ca-cert \
                --output json

    >   {
    >       "apiVersion": "v1",
    >       "kind": "Secret",
    >       "metadata": {
    >           "creationTimestamp": "2020-07-07T00:37:25Z",
    >           "name": "openstack-ca-cert",
    >           "namespace": "kube-system",
    >           "resourceVersion": "161496",
    >           "selfLink": "/api/v1/namespaces/kube-system/secrets/openstack-ca-cert",
    >           "uid": "dd743775-ecd9-4f29-b8ea-9618c784f2e5"
    >       },
    >       "type": "Opaque"
    >   }


# -----------------------------------------------------
# Create the RBAC resources.
#[user@kubernator]

    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        apply --filename https://raw.githubusercontent.com/kubernetes/cloud-provider-openstack/release-1.15/manifests/cinder-csi-plugin/cinder-csi-controllerplugin-rbac.yaml

    >   serviceaccount/csi-cinder-controller-sa created
    >   clusterrole.rbac.authorization.k8s.io/csi-attacher-role created
    >   clusterrolebinding.rbac.authorization.k8s.io/csi-attacher-binding created
    >   clusterrole.rbac.authorization.k8s.io/csi-provisioner-role created
    >   clusterrolebinding.rbac.authorization.k8s.io/csi-provisioner-binding created
    >   clusterrole.rbac.authorization.k8s.io/csi-snapshotter-role created
    >   clusterrolebinding.rbac.authorization.k8s.io/csi-snapshotter-binding created


    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        apply --filename https://github.com/kubernetes/cloud-provider-openstack/raw/release-1.15/manifests/cinder-csi-plugin/cinder-csi-nodeplugin-rbac.yaml

    >   serviceaccount/csi-cinder-node-sa created
    >   clusterrole.rbac.authorization.k8s.io/csi-nodeplugin-role created
    >   clusterrolebinding.rbac.authorization.k8s.io/csi-nodeplugin-binding created

# -----------------------------------------------------
# Deploy the controller plugin.
#[user@kubernator]

    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        apply --filename /kubernetes/cinder-csi/cinder-csi-controllerplugin.yaml

    >   service/csi-cinder-controller-service created
    >   statefulset.apps/csi-cinder-controllerplugin created


# -----------------------------------------------------
# Deploy the node plugin.
#[user@kubernator]

    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        apply --filename /kubernetes/cinder-csi/cinder-csi-nodeplugin.yaml

    >   daemonset.apps/csi-cinder-nodeplugin created


# -----------------------------------------------------
# Create a StorageClass.
#[user@kubernator]

    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        apply --filename /kubernetes/cinder-csi/cinder-csi-storageclass.yaml

    >   storageclass.storage.k8s.io/csi-sc-cinderplugin created


# -----------------------------------------------------
# Create a PersistentVolumeClaim.
#[user@kubernator]

    cat > /tmp/test-pvc.yaml << EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-volume
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-sc-cinderplugin
EOF

    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        apply --filename /tmp/test-pvc.yaml


    >   persistentvolumeclaim/test-volume created


# -----------------------------------------------------
# List the PersistentVolumeClaims.
#[user@kubernator]

    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        get pvc

    >   NAME          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS          AGE
    >   test-volume   Pending                                      csi-sc-cinderplugin   65s


# -----------------------------------------------------
# List the Cinder volumes.
#[user@kubernator]

    openstack \
        --os-cloud "${cloudname:?}" \
        volume list

    >   -


# -----------------------------------------------------
# Create a Pod that uses the claim.
#[user@kubernator]

    cat > /tmp/test-pod.yaml << EOF
apiVersion: v1
kind: Pod
metadata:
  name: web
spec:
  containers:
    - name: web
      image: nginx
      ports:
        - name: web
          containerPort: 80
          hostPort: 8081
          protocol: TCP
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: test-volume
EOF

    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        apply --filename /tmp/test-pod.yaml


    >   pod/web created


# -----------------------------------------------------
# List the Pods.
#[user@kubernator]

    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        get pod

    >   NAME   READY   STATUS    RESTARTS   AGE
    >   web    0/1     Pending   0          62s


# -----------------------------------------------------
# List the PersistentVolumeClaims.
#[user@kubernator]

    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        get pvc

    >   NAME          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS          AGE
    >   test-volume   Pending                                      csi-sc-cinderplugin   65s


# -----------------------------------------------------
# List the Cinder volumes.
#[user@kubernator]

    openstack \
        --os-cloud "${cloudname:?}" \
        volume list

    >   -


# -----------------------------------------------------
# Error messages in the dashboard.


    kubelet drupal-one-x5mbegvpiycq-node-3
        MountVolume.SetUp failed for volume "pods-cloud-data" : hostPath type check failed: /var/lib/cloud/data is not a directory

    kubelet drupal-one-x5mbegvpiycq-node-0
        Unable to mount volumes for pod "csi-cinder-nodeplugin-54zjz_kube-system(16f32a87-c66d-4ab5-8b9c-e9d4a595d84f)":
            timeout expired waiting for volumes to attach or mount for pod "kube-system"/"csi-cinder-nodeplugin-54zjz".
                list of unmounted volumes=[pods-cloud-data secret-cinderplugin].
                list of unattached volumes=[socket-dir registration-dir kubelet-dir pods-mount-dir pods-cloud-data pods-probe-dir secret-cinderplugin ca-cert csi-cinder-node-sa-token-z7494]

    kubelet drupal-one-x5mbegvpiycq-node-0
        MountVolume.SetUp failed for volume "secret-cinderplugin" : secret "cloud-config" not found

    kubelet drupal-one-x5mbegvpiycq-node-0
        MountVolume.SetUp failed for volume "pods-cloud-data" : hostPath type check failed: /var/lib/cloud/data is not a directory


    #
    # The secret from ca.cert was a guess.
    # The plugin config assumed things are installed on the worker nodes.
    # First try .. lots still to figure out.
    #






