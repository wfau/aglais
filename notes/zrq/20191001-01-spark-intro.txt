#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2019, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

# -----------------------------------------------------
# Create a couple of virtual machines.
#[user@gaia01]

    createvm

--START--
INFO : Node name [Cinna]
INFO : Base name [fedora-28-8G-docker-base-20181016.qcow]
INFO : Base path [/var/lib/libvirt/images/base/fedora-28-8G-docker-base-20181016.qcow]
INFO : Disc name [Cinna.qcow]
INFO : Disc size [8GiB]

INFO : MAC  [06:00:AC:10:09:01]
INFO : IPv4 [172.16.9.1]
INFO : IPv6 []
--END--

--START--
INFO : Node name [Edieldan]
INFO : Base name [fedora-28-8G-docker-base-20181016.qcow]
INFO : Base path [/var/lib/libvirt/images/base/fedora-28-8G-docker-base-20181016.qcow]
INFO : Disc name [Edieldan.qcow]
INFO : Disc size [8GiB]

INFO : MAC  [06:00:AC:10:09:02]
INFO : IPv4 [172.16.9.2]
INFO : IPv6 []
--END--

# -----------------------------------------------------
# -----------------------------------------------------
# Initialise our Docker swarm on the first VM.
#[Stevedore@Cinna]

    address=172.16.9.1

    docker swarm init --advertise-addr ${address:?}

--START--
Swarm initialized: current node (r0tdsu34p10m8czn48koegmsc) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-0513pjxlkb7tsajshgl8g3d09v0yvxv3xef9sfqu7sq9n9qmc7-bx2iovcm7gjx29upiyzvmk11y 172.16.9.1:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
--END--


# -----------------------------------------------------
# -----------------------------------------------------
# Try to join the Docker swarm on the second VM.
#[Stevedore@Edieldan]

    docker swarm join --token SWMTKN-1-0513pjxlkb7tsajshgl8g3d09v0yvxv3xef9sfqu7sq9n9qmc7-bx2iovcm7gjx29upiyzvmk11y 172.16.9.1:2377

--START--
Error response from daemon: rpc error: code = Unavailable desc = all
    SubConns are in TransientFailure, latest connection error:
        connection error: desc = "transport: Error while dialing dial tcp 172.16.9.1:2377: connect: no route to host"
--END--


# -----------------------------------------------------
# -----------------------------------------------------
# Open network ports on the first VM.
#[Stevedore@Cinna]

    sudo firewall-cmd --add-port=2377/tcp --permanent
    sudo firewall-cmd --add-port=2377/tcp --permanent
    sudo firewall-cmd --add-port=7946/tcp --permanent
    sudo firewall-cmd --add-port=4789/tcp --permanent
    sudo firewall-cmd --add-port=4789/udp --permanent
    sudo iptables -A INPUT -p 50 -j ACCEPT
    sudo firewall-cmd --reload

--START--
success
Warning: ALREADY_ENABLED: 2377:tcp
success
success
success
success
success
--END--


# -----------------------------------------------------
# -----------------------------------------------------
# Open network ports on the second VM.
#[Stevedore@Edieldan]

    sudo firewall-cmd --add-port=2377/tcp --permanent
    sudo firewall-cmd --add-port=2377/tcp --permanent
    sudo firewall-cmd --add-port=7946/tcp --permanent
    sudo firewall-cmd --add-port=4789/tcp --permanent
    sudo firewall-cmd --add-port=4789/udp --permanent
    sudo iptables -A INPUT -p 50 -j ACCEPT
    sudo firewall-cmd --reload

--START--
success
Warning: ALREADY_ENABLED: 2377:tcp
success
success
success
success
success
--END--


# -----------------------------------------------------
# Join the Docker swarm on the second VM.
#[Stevedore@Edieldan]

    docker swarm join --token SWMTKN-1-3rmd0y25xwml0931jrkglks8jd0uszjw272u3b8swfczvyvnch-93barj8s33zsnkd5v6hjitt2c 172.16.9.1:2377

--START--
This node joined a swarm as a worker.
--END--


# -----------------------------------------------------
# -----------------------------------------------------
# Fetch Spark Project source code on the first VM.
#[Stevedore@Cinna]

    pushd "${HOME}"

        git clone https://github.com/stvoutsin/lasair-experimental

    popd


# -----------------------------------------------------
# Edit the project settings.
#[Stevedore@Cinna]

    pushd ${HOME}/lasair-experimental/spark/src

        vi settings.py

            DB_USER = '########'
            DB_PASS = '########'
            DB_HOST = 'lsstuk2.roe.ac.uk'
            DB_NAME = '########'

    popd

# -----------------------------------------------------
# Create the Spark cluster using Docker compose on the first VM.
#[Stevedore@Cinna]

    pushd ${HOME}/lasair-experimental/spark/docker/compose/stack

        docker stack deploy -c docker-compose.yml spark

    popd


--START--
Creating network spark_spark-net
Creating service spark_spark-master
Creating service spark_spark-worker
--END--

# -----------------------------------------------------
# Check what containers are running on the first VM.
#[Stevedore@Cinna]

    docker ps -a

--START--
CONTAINER ID        IMAGE                                  COMMAND                  CREATED             STATUS              PORTS                          NAMES
1db29c585497        bde2020/spark-master:2.4.0-hadoop2.7   "/bin/bash /master.sh"   25 seconds ago      Up 23 seconds       6066/tcp, 7077/tcp, 8080/tcp   spark_spark-master.1.shimlk0jyom665413dsrtgwhw
--END--


--START--
CONTAINER ID        IMAGE                                  COMMAND                  CREATED             STATUS              PORTS                          NAMES
0db0b70c5f10        bde2020/spark-worker:2.4.0-hadoop2.7   "/bin/bash /worker.sh"   27 seconds ago      Up 23 seconds       8081/tcp                       spark_spark-worker.1.ngqmyp87e5b9djipis9ybsjuk
1db29c585497        bde2020/spark-master:2.4.0-hadoop2.7   "/bin/bash /master.sh"   53 seconds ago      Up 52 seconds       6066/tcp, 7077/tcp, 8080/tcp   spark_spark-master.1.shimlk0jyom665413dsrtgwhw
--END--


# -----------------------------------------------------
# -----------------------------------------------------
# Check what containers are running on the second VM.
#[Stevedore@Edieldan]

    docker ps -a

--START--
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
--END--


# -----------------------------------------------------
# -----------------------------------------------------
# Check the spark-master logs on the first VM.
#[Stevedore@Cinna]

    docker logs 1db29c585497

--START--
2019-10-01 15:00:32 INFO  Master:2566 - Started daemon with process name: 14@1db29c585497
2019-10-01 15:00:32 INFO  SignalUtils:54 - Registered signal handler for TERM
2019-10-01 15:00:32 INFO  SignalUtils:54 - Registered signal handler for HUP
2019-10-01 15:00:32 INFO  SignalUtils:54 - Registered signal handler for INT
2019-10-01 15:00:32 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-01 15:00:32 INFO  SecurityManager:54 - Changing view acls to: root
2019-10-01 15:00:32 INFO  SecurityManager:54 - Changing modify acls to: root
2019-10-01 15:00:32 INFO  SecurityManager:54 - Changing view acls groups to:
2019-10-01 15:00:32 INFO  SecurityManager:54 - Changing modify acls groups to:
2019-10-01 15:00:32 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2019-10-01 15:00:33 INFO  Utils:54 - Successfully started service 'sparkMaster' on port 7077.
2019-10-01 15:00:33 INFO  Master:54 - Starting Spark master at spark://1db29c585497:7077
2019-10-01 15:00:33 INFO  Master:54 - Running Spark version 2.4.0
2019-10-01 15:00:33 INFO  log:192 - Logging initialized @2049ms
2019-10-01 15:00:33 INFO  Server:351 - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-10-01 15:00:33 INFO  Server:419 - Started @2215ms
2019-10-01 15:00:33 INFO  AbstractConnector:278 - Started ServerConnector@7470babe{HTTP/1.1,[http/1.1]}{0.0.0.0:8080}
2019-10-01 15:00:33 INFO  Utils:54 - Successfully started service 'MasterUI' on port 8080.
2019-10-01 15:00:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@114db816{/app,null,AVAILABLE,@Spark}
2019-10-01 15:00:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@36da8cb7{/app/json,null,AVAILABLE,@Spark}
2019-10-01 15:00:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5a1ef58c{/,null,AVAILABLE,@Spark}
2019-10-01 15:00:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@19f7bc7f{/json,null,AVAILABLE,@Spark}
2019-10-01 15:00:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7b2a34c2{/static,null,AVAILABLE,@Spark}
2019-10-01 15:00:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3543fada{/app/kill,null,AVAILABLE,@Spark}
2019-10-01 15:00:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@41d26b12{/driver/kill,null,AVAILABLE,@Spark}
2019-10-01 15:00:33 INFO  MasterWebUI:54 - Bound MasterWebUI to 0.0.0.0, and started at http://1db29c585497:8080
2019-10-01 15:00:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@35674f9d{/metrics/master/json,null,AVAILABLE,@Spark}
2019-10-01 15:00:33 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5a46cf49{/metrics/applications/json,null,AVAILABLE,@Spark}
2019-10-01 15:00:33 INFO  Master:54 - I have been elected leader! New state: ALIVE
2019-10-01 15:01:02 INFO  Master:54 - Registering worker 10.0.0.12:46739 with 2 cores, 1024.0 MB RAM
--END--


# -----------------------------------------------------
# Scale the workers up.
#[Stevedore@Cinna]

    docker service scale spark_spark-worker=4

--START--
spark_spark-worker scaled to 4
overall progress: 4 out of 4 tasks
1/4: running   [==================================================>]
2/4: invalid mount config for type "bind": bind mount source path does not exis…
3/4: running   [==================================================>]
4/4: running   [==================================================>]
verify: Service converged
--END--


# -----------------------------------------------------
# Check what is running on the first VM.
#[Stevedore@Cinna]

    docker ps -a

--START--
CONTAINER ID        IMAGE                                  COMMAND                  CREATED             STATUS              PORTS                          NAMES
4f1037fc0b53        bde2020/spark-worker:2.4.0-hadoop2.7   "/bin/bash /worker.sh"   31 seconds ago      Up 25 seconds       8081/tcp                       spark_spark-worker.3.ixx9pwtzwj39y74sv4mh6g34o
c9cf3c78de77        bde2020/spark-worker:2.4.0-hadoop2.7   "/bin/bash /worker.sh"   31 seconds ago      Up 25 seconds       8081/tcp                       spark_spark-worker.4.vtj349c918okl3yk6ccrjtzg1
9df4e584cf71        bde2020/spark-worker:2.4.0-hadoop2.7   "/bin/bash /worker.sh"   42 seconds ago      Up 40 seconds       8081/tcp                       spark_spark-worker.2.0xfln4il0wacq349tlu9a21in
0db0b70c5f10        bde2020/spark-worker:2.4.0-hadoop2.7   "/bin/bash /worker.sh"   3 minutes ago       Up 3 minutes        8081/tcp                       spark_spark-worker.1.ngqmyp87e5b9djipis9ybsjuk
1db29c585497        bde2020/spark-master:2.4.0-hadoop2.7   "/bin/bash /master.sh"   4 minutes ago       Up 3 minutes        6066/tcp, 7077/tcp, 8080/tcp   spark_spark-master.1.shimlk0jyom665413dsrtgwhw
--END--


# -----------------------------------------------------
# -----------------------------------------------------
# Check what is running on the second VM.
#[Stevedore@Edieldan]

    docker ps -a

--START--
CONTAINER ID        IMAGE                                  COMMAND                  CREATED             STATUS              PORTS                          NAMES
--END--


# -----------------------------------------------------
# -----------------------------------------------------
# Check the Swarm status on the on the first VM.
#[Stevedore@Cinna]

    docker node ls

--START--
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
3b9twozgb0j4eyjw4mdkasfdo *   Cinna               Ready               Active              Leader              18.06.1-ce
x57ocbwao9wjdh64rjg88ya2d     Edieldan            Ready               Active                                  18.06.1-ce
--END--


    docker node inspect self --pretty

--START--
ID:			3b9twozgb0j4eyjw4mdkasfdo
Hostname:              	Cinna
Joined at:             	2019-10-01 14:57:01.255905667 +0000 utc
Status:
 State:			Ready
 Availability:         	Active
 Address:		172.16.9.1
Manager Status:
 Address:		172.16.9.1:2377
 Raft Status:		Reachable
 Leader:		Yes
Platform:
 Operating System:	linux
 Architecture:		x86_64
Resources:
 CPUs:			4
 Memory:		3.851GiB
Plugins:
 Log:		awslogs, fluentd, gcplogs, gelf, journald, json-file, logentries, splunk, syslog
 Network:		bridge, host, macvlan, null, overlay
 Volume:		local
Engine Version:		18.06.1-ce
TLS Info:
 TrustRoot:
-----BEGIN CERTIFICATE-----
MIIBajCCARCgAwIBAgIUEghWrNXYSbQg3wlLiG9t4hiGbR4wCgYIKoZIzj0EAwIw
EzERMA8GA1UEAxMIc3dhcm0tY2EwHhcNMTkxMDAxMTQ1MjAwWhcNMzkwOTI2MTQ1
MjAwWjATMREwDwYDVQQDEwhzd2FybS1jYTBZMBMGByqGSM49AgEGCCqGSM49AwEH
A0IABHNc8UCq2Q+ohxqtVHctz53DGeQLCUj7JtqirNJpgF719b1NIGun4iQBjqHQ
fL2SLuq8YcVgseImBbL7VPPj3/GjQjBAMA4GA1UdDwEB/wQEAwIBBjAPBgNVHRMB
Af8EBTADAQH/MB0GA1UdDgQWBBSGkPt63+zJQL+llsYksrSDlhWsuDAKBggqhkjO
PQQDAgNIADBFAiB8J8WvFcGikp+3k4KuhgQxAQ8kL9ggPYrG+mSgrf6BAgIhAPhT
AoFkqwvlmh82TxUt/a/60CCQcbO295/hCe7DZpTO
-----END CERTIFICATE-----

 Issuer Subject:	MBMxETAPBgNVBAMTCHN3YXJtLWNh
 Issuer Public Key:	MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEc1zxQKrZD6iHGq1Udy3PncMZ5AsJSPsm2qKs0mmAXvX1vU0ga6fiJAGOodB8vZIu6rxhxWCx4iYFsvtU8+Pf8Q==
--END--


    docker node inspect Edieldan --pretty

--START--
ID:			x57ocbwao9wjdh64rjg88ya2d
Hostname:              	Edieldan
Joined at:             	2019-10-01 14:58:46.674688375 +0000 utc
Status:
 State:			Ready
 Availability:         	Active
 Address:		172.16.9.2
Platform:
 Operating System:	linux
 Architecture:		x86_64
Resources:
 CPUs:			4
 Memory:		3.851GiB
Plugins:
 Log:		awslogs, fluentd, gcplogs, gelf, journald, json-file, logentries, splunk, syslog
 Network:		bridge, host, macvlan, null, overlay
 Volume:		local
Engine Version:		18.06.1-ce
TLS Info:
 TrustRoot:
-----BEGIN CERTIFICATE-----
MIIBajCCARCgAwIBAgIUEghWrNXYSbQg3wlLiG9t4hiGbR4wCgYIKoZIzj0EAwIw
EzERMA8GA1UEAxMIc3dhcm0tY2EwHhcNMTkxMDAxMTQ1MjAwWhcNMzkwOTI2MTQ1
MjAwWjATMREwDwYDVQQDEwhzd2FybS1jYTBZMBMGByqGSM49AgEGCCqGSM49AwEH
A0IABHNc8UCq2Q+ohxqtVHctz53DGeQLCUj7JtqirNJpgF719b1NIGun4iQBjqHQ
fL2SLuq8YcVgseImBbL7VPPj3/GjQjBAMA4GA1UdDwEB/wQEAwIBBjAPBgNVHRMB
Af8EBTADAQH/MB0GA1UdDgQWBBSGkPt63+zJQL+llsYksrSDlhWsuDAKBggqhkjO
PQQDAgNIADBFAiB8J8WvFcGikp+3k4KuhgQxAQ8kL9ggPYrG+mSgrf6BAgIhAPhT
AoFkqwvlmh82TxUt/a/60CCQcbO295/hCe7DZpTO
-----END CERTIFICATE-----

 Issuer Subject:	MBMxETAPBgNVBAMTCHN3YXJtLWNh
 Issuer Public Key:	MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEc1zxQKrZD6iHGq1Udy3PncMZ5AsJSPsm2qKs0mmAXvX1vU0ga6fiJAGOodB8vZIu6rxhxWCx4iYFsvtU8+Pf8Q==
--END--


# -----------------------------------------------------
# Try scaling to 6 nodes.
#[Stevedore@Cinna]

    docker service scale spark_spark-worker=6

--START--
spark_spark-worker scaled to 6
overall progress: 6 out of 6 tasks
1/6: running   [==================================================>]
2/6: running   [==================================================>]
3/6: invalid mount config for type "bind": bind mount source path does not exis…
3/6: running   [==================================================>]
4/6: running   [==================================================>]
5/6: running   [==================================================>]
6/6: running   [==================================================>]
verify: Service converged
--END--

    #
    # Errors appear momentatirly, but then get replaced by 'running'.
    #

    #
    # Looks like the compose file relies on a bind mount to a local directory containing the jar file.
    #

    #
    # This might explain
    # https://stackoverflow.com/questions/47756029/how-does-docker-swarm-implement-volume-sharing
    # and provide a solution
    # https://stackoverflow.com/a/52151840


    #
    # Using single VM for now ...
    #


# -----------------------------------------------------
# -----------------------------------------------------
# Create a SSH tunnel to port 8080 pn the master VM.
#[user@desktop]

    ssh -L '*:8084:Cinna:8080' Stevedore@Cinna


# -----------------------------------------------------
# Point browser at the local end of the tunnel.
#[user@desktop]

    firefox 'http://localhost:8084/' &


# -----------------------------------------------------
# Run a shell in the master container.
#[Stevedore@Cinna]

    docker exec \
        --tty \
        --interactive \
        'spark_spark-master.1.shimlk0jyom665413dsrtgwhw' \
            bash


# -----------------------------------------------------
# Submit our Spark application.
#[root@spark-master]

    /spark/bin/spark-submit /app/app.py


--START--
2019-10-01 15:52:52 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-01 15:52:53 INFO  SparkContext:54 - Running Spark version 2.4.0
2019-10-01 15:52:53 INFO  SparkContext:54 - Submitted application: spark-basic
2019-10-01 15:52:53 INFO  SecurityManager:54 - Changing view acls to: root
2019-10-01 15:52:53 INFO  SecurityManager:54 - Changing modify acls to: root
2019-10-01 15:52:53 INFO  SecurityManager:54 - Changing view acls groups to:
2019-10-01 15:52:53 INFO  SecurityManager:54 - Changing modify acls groups to:
2019-10-01 15:52:53 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2019-10-01 15:52:53 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 44131.
2019-10-01 15:52:53 INFO  SparkEnv:54 - Registering MapOutputTracker
2019-10-01 15:52:53 INFO  SparkEnv:54 - Registering BlockManagerMaster
2019-10-01 15:52:53 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-10-01 15:52:53 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2019-10-01 15:52:53 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-0267848c-9d2d-41ff-90fe-6dbb51666f1f
2019-10-01 15:52:53 INFO  MemoryStore:54 - MemoryStore started with capacity 140.1 MB
2019-10-01 15:52:53 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2019-10-01 15:52:53 INFO  log:192 - Logging initialized @2501ms
2019-10-01 15:52:53 INFO  Server:351 - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-10-01 15:52:53 INFO  Server:419 - Started @2602ms
2019-10-01 15:52:53 INFO  AbstractConnector:278 - Started ServerConnector@2955a7bc{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-10-01 15:52:53 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
--END--


--START--
2019-10-01 15:52:54 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor updated: app-20191001155254-0000/3 is now RUNNING
2019-10-01 15:52:54 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor updated: app-20191001155254-0000/4 is now RUNNING
2019-10-01 15:52:54 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor updated: app-20191001155254-0000/5 is now RUNNING
2019-10-01 15:52:54 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor updated: app-20191001155254-0000/0 is now RUNNING
2019-10-01 15:52:54 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor updated: app-20191001155254-0000/1 is now RUNNING
2019-10-01 15:52:54 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor updated: app-20191001155254-0000/2 is now RUNNING
--END--


--START--
2019-10-01 15:52:58 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/spark-warehouse').
2019-10-01 15:52:58 INFO  SharedState:54 - Warehouse path is 'file:/spark-warehouse'.
2019-10-01 15:52:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3de5d78c{/SQL,null,AVAILABLE,@Spark}
2019-10-01 15:52:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2fa71591{/SQL/json,null,AVAILABLE,@Spark}
2019-10-01 15:52:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@433fb122{/SQL/execution,null,AVAILABLE,@Spark}
2019-10-01 15:52:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@743358ad{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-10-01 15:52:58 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5f576515{/static/sql,null,AVAILABLE,@Spark}
--END--


--START--
2019-10-01 15:53:00 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint
Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.
2019-10-01 15:53:02 INFO  CoarseGrainedSchedulerBackend$DriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.0.29:53966) with ID 5
2019-10-01 15:53:02 INFO  CoarseGrainedSchedulerBackend$DriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.0.14:53472) with ID 2
2019-10-01 15:53:02 INFO  CoarseGrainedSchedulerBackend$DriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.0.12:58074) with ID 0
2019-10-01 15:53:02 INFO  CoarseGrainedSchedulerBackend$DriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.0.20:49814) with ID 3
2019-10-01 15:53:02 INFO  CoarseGrainedSchedulerBackend$DriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.0.21:38294) with ID 4
2019-10-01 15:53:02 INFO  CoarseGrainedSchedulerBackend$DriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.0.28:54954) with ID 1
--END--


--START--
2019-10-01 15:53:02 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.0.0.14:36261 with 129.6 MB RAM, BlockManagerId(2, 10.0.0.14, 36261, None)
2019-10-01 15:53:03 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.0.0.29:40287 with 129.6 MB RAM, BlockManagerId(5, 10.0.0.29, 40287, None)
2019-10-01 15:53:03 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.0.0.12:43029 with 129.6 MB RAM, BlockManagerId(0, 10.0.0.12, 43029, None)
2019-10-01 15:53:03 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.0.0.20:37205 with 129.6 MB RAM, BlockManagerId(3, 10.0.0.20, 37205, None)
2019-10-01 15:53:03 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.0.0.21:44149 with 129.6 MB RAM, BlockManagerId(4, 10.0.0.21, 44149, None)
2019-10-01 15:53:03 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.0.0.28:42259 with 129.6 MB RAM, BlockManagerId(1, 10.0.0.28, 42259, None)
--END--


--START--
2019-10-01 15:53:36 INFO  TaskSchedulerImpl:54 - Adding task set 43.0 with 1 tasks
2019-10-01 15:53:36 INFO  TaskSetManager:54 - Starting task 0.0 in stage 43.0 (TID 43, 10.0.0.29, executor 5, partition 0, PROCESS_LOCAL, 7694 bytes)
2019-10-01 15:53:36 INFO  BlockManagerInfo:54 - Added broadcast_43_piece0 in memory on 10.0.0.29:40287 (size: 8.9 KB, free: 129.6 MB)
2019-10-01 15:53:36 INFO  TaskSetManager:54 - Finished task 0.0 in stage 43.0 (TID 43) in 119 ms on 10.0.0.29 (executor 5) (1/1)
2019-10-01 15:53:36 INFO  TaskSchedulerImpl:54 - Removed TaskSet 43.0, whose tasks have all completed, from pool
2019-10-01 15:53:36 INFO  DAGScheduler:54 - ResultStage 43 (collect at /app/app.py:62) finished in 0.125 s
2019-10-01 15:53:36 INFO  DAGScheduler:54 - Job 43 finished: collect at /app/app.py:62, took 0.128713 s
--END--


--START--
Found 29 matches
[[{'arcsec': 0.400620074197715, 'cone_id': 76141, 'name': u'KUV 00311-1938', 'objectId': u'ZTF18abtftkf', 'wl_id': 35, 'ndethist': 46}], [{'arcsec': 0.33467606695185426, 'cone_id': 76142, 'name': u'1ES 0033+595', 'objectId': u'ZTF19abgzjrh', 'wl_id': 35, 'ndethist': 2}], [{'arcsec': 0.1662871796475325, 'cone_id': 76144, 'name': u'3C 66A', 'objectId': u'ZTF18aabezmq', 'wl_id': 35, 'ndethist': 87}], [{'arcsec': 0.22974095717614754, 'cone_id': 76145, 'name': u'1ES 0229+200', 'objectId': u'ZTF18acsykeu', 'wl_id': 35, 'ndethist': 16}], [{'arcsec': 0.23133075098705513, 'cone_id': 76146, 'name': u'PKS 0301-243', 'objectId': u'ZTF18acebmhq', 'wl_id': 35, 'ndethist': 14}], [{'arcsec': 0.11868233217580097, 'cone_id': 76147, 'name': u'RBS 0413', 'objectId': u'ZTF18acebdlq', 'wl_id': 35, 'ndethist': 25}], [{'arcsec': 0.0313723409963505, 'cone_id': 76148, 'name': u'1ES 0347-121', 'objectId': u'ZTF18abuxvfk', 'wl_id': 35, 'ndethist': 3}], [{'arcsec': 0.2577698553389672, 'cone_id': 76149, 'name': u'1ES 0414+009', 'objectId': u'ZTF18acrvucs', 'wl_id': 35, 'ndethist': 58}], [{'arcsec': 0.1838454189306695, 'cone_id': 76151, 'name': u'1ES 0502+675', 'objectId': u'ZTF18abvfkym', 'wl_id': 35, 'ndethist': 29}], [{'arcsec': 0.1084784106478084, 'cone_id': 76153, 'name': u'RX J0648.7+1516', 'objectId': u'ZTF18abvmpjc', 'wl_id': 35, 'ndethist': 163}], [{'arcsec': 0.135155254738842, 'cone_id': 76154, 'name': u'1ES 0647+250', 'objectId': u'ZTF17aadptpa', 'wl_id': 35, 'ndethist': 78}], [{'arcsec': 0.027836022440623862, 'cone_id': 76156, 'name': u'S5 0716+714', 'objectId': u'ZTF18abvtfpt', 'wl_id': 35, 'ndethist': 14}], [{'arcsec': 0.05853030776645684, 'cone_id': 76157, 'name': u'1ES 0806+524', 'objectId': u'ZTF18acbznzg', 'wl_id': 35, 'ndethist': 68}], [{'arcsec': 0.2498989208217886, 'cone_id': 76159, 'name': u'1ES 1011+496', 'objectId': u'ZTF18aajmxtj', 'wl_id': 35, 'ndethist': 21}, {'arcsec': 0.29858907506419163, 'cone_id': 76159, 'name': u'1ES 1011+496', 'objectId': u'ZTF18abalsiv', 'wl_id': 35, 'ndethist': 73}], [{'arcsec': 0.16165262983952913, 'cone_id': 76162, 'name': u'Markarian 180', 'objectId': u'ZTF18aakdfgo', 'wl_id': 35, 'ndethist': 72}], [{'arcsec': 0.12189657991231728, 'cone_id': 76163, 'name': u'1ES 1215+303', 'objectId': u'ZTF18aabxehk', 'wl_id': 35, 'ndethist': 158}, {'arcsec': 0.6759367324649533, 'cone_id': 76163, 'name': u'1ES 1215+303', 'objectId': u'ZTF18acurlrz', 'wl_id': 35, 'ndethist': 4}], [{'arcsec': 0.05058847788829838, 'cone_id': 76164, 'name': u'1ES 1218+304', 'objectId': u'ZTF18aacapwh', 'wl_id': 35, 'ndethist': 105}], [{'arcsec': 0.10280102420287003, 'cone_id': 76165, 'name': u'W Comae', 'objectId': u'ZTF17aaapqiz', 'wl_id': 35, 'ndethist': 117}], [{'arcsec': 0.12637603868918934, 'cone_id': 76167, 'name': u'PKS 1424+240', 'objectId': u'ZTF18aazhkrc', 'wl_id': 35, 'ndethist': 93}], [{'arcsec': 0.12186109292941122, 'cone_id': 76168, 'name': u'H 1426+428', 'objectId': u'ZTF18aaquoaj', 'wl_id': 35, 'ndethist': 6}], [{'arcsec': 0.605413894834733, 'cone_id': 76169, 'name': u'1ES 1440+122', 'objectId': u'ZTF18actubhl', 'wl_id': 35, 'ndethist': 89}], [{'arcsec': 0.07602221679392437, 'cone_id': 76170, 'name': u'AP Lib', 'objectId': u'ZTF19aardope', 'wl_id': 35, 'ndethist': 13}], [{'arcsec': 0.19165521406190683, 'cone_id': 76171, 'name': u'PG 1553+113', 'objectId': u'ZTF18aaylblx', 'wl_id': 35, 'ndethist': 69}], [{'arcsec': 0.14364945883145228, 'cone_id': 76172, 'name': u'Markarian 501', 'objectId': u'ZTF18aamndyk', 'wl_id': 35, 'ndethist': 76}, {'arcsec': 0.695898941160862, 'cone_id': 76172, 'name': u'Markarian 501', 'objectId': u'ZTF18aaltula', 'wl_id': 35, 'ndethist': 8}], [{'arcsec': 0.4485672599828992, 'cone_id': 76173, 'name': u'1ES 1741+196', 'objectId': u'ZTF18aaqzpsv', 'wl_id': 35, 'ndethist': 74}], [{'arcsec': 0.10732033925503805, 'cone_id': 76175, 'name': u'MAGIC J2001+435', 'objectId': u'ZTF18aaxdpoz', 'wl_id': 35, 'ndethist': 235}], [None, {'arcsec': 0.10362219625496334, 'cone_id': 76178, 'name': u'BL Lacertae', 'objectId': u'ZTF18abmjhvi', 'wl_id': 35, 'ndethist': 88}], [{'arcsec': 0.06463853132559297, 'cone_id': 76179, 'name': u'B3 2247+381', 'objectId': u'ZTF18abmodtn', 'wl_id': 35, 'ndethist': 124}], [{'arcsec': 0.11060623770906537, 'cone_id': 76180, 'name': u'1ES 2344+514', 'objectId': u'ZTF18abbuwwg', 'wl_id': 35, 'ndethist': 132}]]
Time taken: 35.9526631832 seconds
2019-10-01 15:53:36 INFO  SparkContext:54 - Invoking stop() from shutdown hook
2019-10-01 15:53:36 INFO  AbstractConnector:318 - Stopped Spark@2955a7bc{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-10-01 15:53:36 INFO  SparkUI:54 - Stopped Spark web UI at http://1db29c585497:4040
2019-10-01 15:53:36 INFO  StandaloneSchedulerBackend:54 - Shutting down all executors
2019-10-01 15:53:36 INFO  CoarseGrainedSchedulerBackend$DriverEndpoint:54 - Asking each executor to shut down
2019-10-01 15:53:36 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2019-10-01 15:53:36 INFO  MemoryStore:54 - MemoryStore cleared
2019-10-01 15:53:36 INFO  BlockManager:54 - BlockManager stopped
2019-10-01 15:53:36 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2019-10-01 15:53:36 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2019-10-01 15:53:36 INFO  SparkContext:54 - Successfully stopped SparkContext
2019-10-01 15:53:36 INFO  ShutdownHookManager:54 - Shutdown hook called
2019-10-01 15:53:36 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-5c38cfb7-9e32-41f6-8e51-e69a8fb8ccae
2019-10-01 15:53:36 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-5c38cfb7-9e32-41f6-8e51-e69a8fb8ccae/pyspark-09184163-6e60-45fe-9d87-1ec05d73aaec
2019-10-01 15:53:36 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-3d3a8c0d-38f0-4433-82e7-9ef33e5ae8eb
--END--

