#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    The reason for doing this is we need a Spark deployment with an up to date version of Hadoop.
    The binary releases of Spark include an old version of Hadoop, that have a very old version of the S3 jars.
    We need a more recent version of the S3 coponents to be able to access data in the OpenStack Swift system.

    Plan is to have separate installs of (Hadoop) and (Spark without Haddop), and then point Spark at the
    Hadoop libraies.

    https://stackoverflow.com/questions/28664834/which-cluster-type-should-i-choose-for-spark/34657719#34657719
    https://stackoverflow.com/questions/32022334/can-apache-spark-run-without-hadoop

    https://spark.apache.org/docs/latest/hadoop-provided.html
    https://spark.apache.org/docs/latest/

    There are some issues with S3 to be aware of (2015):
    https://arnon.me/2015/08/spark-parquet-s3/

    This also provides a step towards ditching YARN and using Kubernetes instead.
    https://spark.apache.org/docs/latest/running-on-kubernetes.html


    Plan

    Install Java
    Install Hadoop
    Install Spark without Hadoop
    Configure Spark to use YARN
    Configure Spark to use S3

    Note - at the moment our worker nodes have no local storage.


# -----------------------------------------------------
# Create our cloud config file.
#[user@desktop]

    #
    # See 20200114-03-podman-volume.txt
    #

# -----------------------------------------------------
# Allow podman container to use the SSH authentication socket on our desktop.
# https://osric.com/chris/accidental-developer/2017/11/selinux-audit2why-audit2allow-policy-files/
# https://stackoverflow.com/a/52326925
#[user@desktop]

    #
    # See 20200222-01-ansible-deploy.txt
    #

# -----------------------------------------------------
# Create a container to work with.
# https://podman.readthedocs.io/en/latest/markdown/podman-run.1.html
#[user@desktop]

    source "${HOME}/aglais.settings"

    podman run \
        --rm \
        --tty \
        --interactive \
        --hostname ansibler \
        --env SSH_AUTH_SOCK=/mnt/ssh_auth_sock \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --volume "${HOME}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/zrq/ansible:/mnt/ansible:ro,z" \
        atolmis/ansible-client:latest \
        bash


# -----------------------------------------------------
# Create our Ansible include vars file.
#[root@ansibler]

    cat > /tmp/ansible-vars.yml << EOF

buildtag:  'aglais-$(date '+%Y%m%d')'
cloudname: 'gaia-prod'

EOF


# -----------------------------------------------------
# Run our playbooks from the /mnt/ansible directory.
# Needed to pick up the 'ansible.cfg' config file.
# https://docs.ansible.com/ansible/latest/reference_appendices/config.html#the-configuration-file
#[root@ansibler]

    cd /mnt/ansible


# -----------------------------------------------------
# Create our gateway, master and worker instances.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-01.yml"

    >   ....
    >   ....
    >   PLAY RECAP ..
    >   gateway                    : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   localhost                  : ok=32   changed=25   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   master01                   : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   master02                   : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker01                   : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker02                   : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker03                   : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   worker04                   : ok=1    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0


# -----------------------------------------------------
# Install Java on the master and worker nodes.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "10-install-java.yml"

    >   ....
    >   TASK [Install the Java runtime] ..
    >   changed: [worker02]
    >   changed: [master02]
    >   changed: [master01]
    >   changed: [worker03]
    >   changed: [worker01]
    >   changed: [worker04]
    >   ....


# -----------------------------------------------------
# Check the Java on the master and worker nodes.
#[root@ansibler]

    masters=(
        master01
        master02
        )

    workers=(
        worker01
        worker02
        worker03
        worker04
        )

    for nodename in ${masters[@]} ${workers[@]}
    do
        echo "----"
        echo "Node [${nodename}]"
        ssh \
            -F "${HOME}/.ssh/ansible-config" \
            "${nodename}" \
                'hostname; date ; java --version'
    done

    >   ----
    >   Node [master01]
    >   aglais-20200124-master01.novalocal
    >   Fri 24 Jan 2020 08:52:56 PM UTC
    >   openjdk 13.0.1 2019-10-15
    >   OpenJDK Runtime Environment 19.9 (build 13.0.1+9)
    >   OpenJDK 64-Bit Server VM 19.9 (build 13.0.1+9, mixed mode, sharing)
    >   ....
    >   ....
    >   ----
    >   Node [worker04]
    >   aglais-20200124-worker04.novalocal
    >   Fri 24 Jan 2020 08:52:58 PM UTC
    >   openjdk 13.0.1 2019-10-15
    >   OpenJDK Runtime Environment 19.9 (build 13.0.1+9)
    >   OpenJDK 64-Bit Server VM 19.9 (build 13.0.1+9, mixed mode, sharing)


# -----------------------------------------------------
# Install Hadoop on the master and worker nodes.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "11-install-hadoop.yml"

    >   ....
    >   TASK [Download and unpack the Hadoop tar gzip file.] ..
    >   changed: [master01]
    >   
    >   TASK [Create a symbolic link] ..
    >   changed: [master01]
    >   
    >   TASK [Add hadoop/bin to the PATH.] ..
    >   changed: [master01]
    >   ....














# From new system
cat "/opt/hadoop/etc/hadoop/core-site.xml"

    <?xml version="1.0" encoding="UTF-8"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    ...
    <configuration>
    </configuration>

# From live system
cat "${HOME:?}/hadoop/etc/hadoop/core-site.xml"

    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>fs.default.name</name>
            <value>hdfs://stv-dev-master:9000</value>
        </property>
    </configuration>



# From new system
cat "/opt/hadoop/etc/hadoop/hdfs-site.xml"

    <?xml version="1.0" encoding="UTF-8"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    ....
    <!-- Put site-specific property overrides in this file. -->
    <configuration>
    </configuration>

# From live system
cat "${HOME:?}/hadoop/etc/hadoop/hdfs-site.xml"

    <?xml version="1.0" encoding="UTF-8"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    ....
    <!-- Put site-specific property overrides in this file. -->
    <configuration>
        <property>
                <name>dfs.namenode.name.dir</name>
                <value>/home/fedora/hadoop/data/nameNode</value>
        </property>

        <property>
                <name>dfs.datanode.data.dir</name>
                <value>/home/fedora/hadoop/data/dataNode</value>
        </property>

        <property>
                <name>dfs.replication</name>
                <value>2</value>
        </property>
    </configuration>



# From new system
cat "/opt/hadoop/etc/hadoop/yarn-site.xml"

    <?xml version="1.0"?>
    ....
    <configuration>
    <!-- Site specific YARN configuration properties -->
    </configuration>

# From live system
cat "${HOME:?}/hadoop/etc/hadoop/yarn-site.xml"

    <configuration>
        <property>
                <name>yarn.acl.enable</name>
                <value>0</value>
        </property>

        <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>stv-dev-master</value>
        </property>

        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
        </property>
    <property>
            <name>yarn.nodemanager.resource.memory-mb</name>
            <value>15000</value>
    </property>

    <property>
            <name>yarn.scheduler.maximum-allocation-mb</name>
            <value>15000</value>
    </property>

    <property>
            <name>yarn.scheduler.minimum-allocation-mb</name>
            <value>2000</value>
    </property>

    <property>
            <name>yarn.nodemanager.vmem-check-enabled</name>
            <value>false</value>
    </property>
    <property>
       <name>yarn.scheduler.capacity.root.support.user-limit-factor</name>
       <value>2</value>
    </property>
    <property>
       <name>yarn.nodemanager.disk-health-checker.min-healthy-disks</name>
       <value>0.0</value>
    </property>
    <property>
       <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
       <value>100.0</value>
    </property>

     <property>
        <name>yarn.resourcemanager.scheduler.address</name>
        <value>stv-dev-master:8030</value>
      </property>
      <property>
        <name>yarn.resourcemanager.address</name>
        <value>stv-dev-master:8032</value>
      </property>
      <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>stv-dev-master:8088</value>
      </property>
      <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>stv-dev-master:8031</value>
      </property>
      <property>
        <name>yarn.resourcemanager.admin.address</name>
        <value>stv-dev-master:8033</value>
      </property>

    </configuration>



# From new system
cat "/opt/hadoop/etc/hadoop/workers"

    localhost

# From live system
cat "${HOME:?}/hadoop/etc/hadoop/workers"

    stv-dev-worker-1
    stv-dev-worker-2
    stv-dev-worker-3
    stv-dev-worker-4
    stv-dev-worker-5
    stv-dev-worker-7
    stv-dev-worker-8



# From new system
cat "/opt/hadoop/etc/hadoop/mapred-site.xml"

    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    ....
    <!-- Put site-specific property overrides in this file. -->
    <configuration>
    </configuration>

# From live system
cat "${HOME:?}/hadoop/etc/hadoop/mapred-site.xml"

    <configuration>
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
        <property>
                <name>yarn.app.mapreduce.am.env</name>
                <value>HADOOP_MAPRED_HOME=/home/fedora/hadoop</value>
        </property>
        <property>
                <name>mapreduce.map.env</name>
                <value>HADOOP_MAPRED_HOME=/home/fedora/hadoop</value>
        </property>
        <property>
                <name>mapreduce.reduce.env</name>
                <value>HADOOP_MAPRED_HOME=/home/fedora/hadoop</value>
        </property>
    <property>
    <name>yarn.app.mapreduce.am.resource.mb</name>
            <value>15000</value>
    </property>

    <property>
    <name>mapreduce.map.memory.mb</name>
            <value>7000</value>
    </property>

    <property>
    <name>mapreduce.reduce.memory.mb</name>
            <value>7000</value>
    </property>
    </configuration>



