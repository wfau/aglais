#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#


    Unable to load an more than 1/4 of the dataset
    Needs more investigations to find out why

        Run the same Spark/Zep config on DigitalOcean to compare.
        Run the same Spark/Zep config with data in CephFS

        Suggestion from Nigel & Stelios - look at temp disc space for over-flow caching ?

    New directions

        Can we run Nigel's examples on the 1/8 datasets ?
        Can we make Zeppelin multi-user ?
        Can we make Zeppelin multi-user with the OAuth proxy ?
            What does that do to the Spark sessons ?

        Can we submit jobs to Zeppelin via REST.
            - yes
            Stelios has experimented with the REST API on a standard cluster, and
            Dave has a 'hello world' on the SparkInZeppelin system.

        * What are the pros/cons of running Spark-in-Zeppelin or as a separate service ?



    Openstack autoscale

        Openstack autoscale works.

            Stop the existing interpreter process.
            Doubled the size if the worker nodes set in the notebook, 1g 1core to 2g 2core.

            Kubernetes dashboard flagged errors in provisioning the Pods - not enough resources.
            Openstack triggered to provision more resources.
            Kubernetes found the new resources and allocated the Pods.
            Openstack Horizon showed 2 new virtual machines allocated.

            The process works - but is way too slow for an interactive system.



    Kubernetes Pod Autoscaler
    https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/

        Do we need a Pod Autoscaler ?
        At the moment we are not using a kubernetes auto scaler.
        The number of Spark workers is fixed using Zeppelin interpreter config properties.

            spark.driver.cores       4
            spark.driver.memory     16g
            spark.executor.cores     4
            spark.executor.memory   16g
            spark.executor.instances 4

        IF we did use a Pod Autoscaler, how would we tell Spark about the extra workers ?
        Probably not applicable in our case.


