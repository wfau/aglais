0#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    Follow onb from previous notes.
    20201001-01-manila-static.txt

    Helm chart works for a 5Gbyte rw deployment.
    Try to create a larger 5Tbyte rw deployment.

    ----

    Yay - looks like it worked :-)
    We have a 5000Gbyte public share with a ReadWrite mount into a Pod.
    So the previous problems with empty staging credentials ... due to typos in my notes ?
    Whatever, chase it later ...


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --hostname kubernator \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --env "clustername=${CLUSTER_NAME:?}" \
        --volume "${HOME}/clouds.yaml:/etc/openstack/clouds.yaml:z" \
        --volume "${AGLAIS_CODE}/experiments/zrq/helm:/helm:z" \
        --volume "${AGLAIS_CODE}/experiments/zrq/kubernetes:/kubernetes:z" \
        atolmis/openstack-client \
        bash


# -----------------------------------------------------
# Set the deployment params.
#[user@kubernator]

    sharename=cotopi-dr2
    sharesize=5000
    sharepublic=true


# -----------------------------------------------------
# Get the connection details for our cluster.
#[user@kubernator]

    mkdir -p "${HOME}/.kube"
    openstack \
        --os-cloud "${cloudname:?}" \
        coe cluster config \
            "${clustername:?}" \
                --force \
                --dir "${HOME}/.kube"


    kubectl \
        cluster-info

    >   Kubernetes master is running at https://....
    >   Heapster is running at https://....
    >   CoreDNS is running at https://....


# -----------------------------------------------------
# Set the Manila API version.
# https://stackoverflow.com/a/58806536
#[user@kubernator]

    export OS_SHARE_API_VERSION=2.51


# -----------------------------------------------------
# Create a new static share.
# https://docs.openstack.org/python-openstackclient/latest/cli/plugin-commands/manila.html#share-create
#[user@kubernator]

    openstack \
        --os-cloud "${cloudname:?}" \
        share create \
            --format json \
            --name   "${sharename:?}" \
            --public "${sharepublic:?}" \
            --share-type 'cephfsnativetype' \
            --availability-zone 'nova' \
            'CEPHFS' \
            "${sharesize:?}" \
    | tee "/tmp/${sharename:?}-share.json"


    >   ShareLimitExceeded: Maximum number of shares allowed (10) either per project/user or share type quota is exceeded. (HTTP 413) (Request-ID: req-aad074a3-890b-41d5-8139-6bdc1815fd21)

    # EEEEK
    # This will have serious repercussions for our plans.
    # We were planning on using Manila shares to provide the user data space for each user.
    # If we have several hundred registered users, then that means hundreds of 1Gbyte shares.

    # Logged a question the IRIS TWG Slack channel.
    # Discussion with John Garbutt and Paul Browne - they are happy to increase the quota to 100 sdhares per project.
    # :-)

    # Mean time, delete some unused shares to make space.
    # ....


# -----------------------------------------------------
# List our current shares.
#[user@kubernator]

    openstack \
        --os-cloud "${cloudname:?}" \
        share list

    >   +--------------------------------------+------------------------------------------+------+-------------+-----------+-----------+------------------+------+-------------------+
    >   | ID                                   | Name                                     | Size | Share Proto | Status    | Is Public | Share Type Name  | Host | Availability Zone |
    >   +--------------------------------------+------------------------------------------+------+-------------+-----------+-----------+------------------+------+-------------------+
    >   | f14a2454-6385-41f8-a89e-3fd8e51e2fb6 | dostinkie-share                          |    5 | CEPHFS      | available | False     | cephfsnativetype |      | nova              |
    >   | 7199f136-2d10-4afa-b21b-0c3631fc6dac | gaia-dr2                                 |    5 | CEPHFS      | available | False     | cephfsnativetype |      | nova              |
    >   | ad1d9ca2-5b1c-4064-8c74-695286de6098 | gaia-dr2-share                           | 4399 | CEPHFS      | available | True      | cephfsnativetype |      | nova              |
    >   | 8611ddef-087a-4032-b02f-b8d7b1e280a7 | pvc-16eaf7c5-57e1-403e-aecf-f2dbe4fdf0a6 |    1 | CEPHFS      | available | False     | cephfsnativetype |      | nova              |
    >   | 77587c48-3a34-4f21-9769-a3df932a1195 | pvc-1f1750b4-94c0-4b56-800d-50d7abf923a0 |    1 | CEPHFS      | available | False     | cephfsnativetype |      | nova              |
    >   | 0e1e1421-bb29-4e35-b21b-4e32b397f52f | pvc-2b42b52e-b0b3-4708-b85f-e8e36697a668 |    1 | CEPHFS      | available | False     | cephfsnativetype |      | nova              |
    >   | b9c3d40d-81db-460e-8779-60e7e2221c26 | pvc-67919d92-b56e-4306-b241-00a812676ea3 |    1 | CEPHFS      | available | False     | cephfsnativetype |      | nova              |
    >   | b0fdeefe-5886-4e0e-8231-187facb514b5 | pvc-70e6f042-2047-48f0-be91-0e8846a383ce |    1 | CEPHFS      | available | False     | cephfsnativetype |      | nova              |
    >   | f5ad90af-29c6-447d-8b8a-967d5ccc7e45 | pvc-e41dc65c-f3d9-4c27-8987-797016ebce36 |    1 | CEPHFS      | available | False     | cephfsnativetype |      | nova              |
    >   +--------------------------------------+------------------------------------------+------+-------------+-----------+-----------+------------------+------+-------------------+


# -----------------------------------------------------
# List our current PersistentVolumes.
#[user@kubernator]

    kubectl get \
        PersistentVolumes

    >   NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                           STORAGECLASS         REASON   AGE
    >   dostinkie-volume                           4Gi        RWX            Retain           Bound    default/dostinkie-claim                                       2d10h
    >   gaia-admin-volume                          4G         RWX            Retain           Bound    default/gaia-admin-claim                                      2d4h
    >   pvc-2b42b52e-b0b3-4708-b85f-e8e36697a668   1Gi        RWO            Retain           Bound    default/drupal-site-db-pvc      manila-trust-class            6d1h
    >   pvc-67919d92-b56e-4306-b241-00a812676ea3   1Gi        RWO            Retain           Bound    default/drupal-site-files-pvc   manila-trust-class            6d1h
    >   rutoria-volume                             5G         RWO            Retain           Bound    default/rutoria-claim                                         8h


    kubectl get \
        --output json \
        PersistentVolumes \
    | jq '.items[] | select( .metadata.name | startswith("pvc-")) | { name: .metadata.name, share: .spec.csi.volumeAttributes.shareID }'


    >   {
    >     "name": "pvc-2b42b52e-b0b3-4708-b85f-e8e36697a668",
    >     "share": "0e1e1421-bb29-4e35-b21b-4e32b397f52f"
    >   }
    >   {
    >     "name": "pvc-67919d92-b56e-4306-b241-00a812676ea3",
    >     "share": "b9c3d40d-81db-460e-8779-60e7e2221c26"
    >   }


# -----------------------------------------------------
# Delete the orphaned shares.
#[user@kubernator]

    orphans=(
        "8611ddef-087a-4032-b02f-b8d7b1e280a7"
        "77587c48-3a34-4f21-9769-a3df932a1195"
        "b0fdeefe-5886-4e0e-8231-187facb514b5"
        "f5ad90af-29c6-447d-8b8a-967d5ccc7e45"
        )

    for orphan in ${orphans[@]}
    do
        echo "Orphan [${orphan:?}]"
        openstack \
            --os-cloud "${cloudname:?}" \
            share delete \
                "${orphan:?}"
    done

    >   Orphan [8611ddef-087a-4032-b02f-b8d7b1e280a7]
    >   Orphan [77587c48-3a34-4f21-9769-a3df932a1195]
    >   Orphan [b0fdeefe-5886-4e0e-8231-187facb514b5]
    >   Orphan [f5ad90af-29c6-447d-8b8a-967d5ccc7e45]


# -----------------------------------------------------
# Create a new static share.
# https://docs.openstack.org/python-openstackclient/latest/cli/plugin-commands/manila.html#share-create
#[user@kubernator]

    openstack \
        --os-cloud "${cloudname:?}" \
        share create \
            --format json \
            --name   "${sharename:?}" \
            --public "${sharepublic:?}" \
            --share-type 'cephfsnativetype' \
            --availability-zone 'nova' \
            'CEPHFS' \
            "${sharesize:?}" \
    | tee "/tmp/${sharename:?}-share.json"

    shareid=$(
        jq -r '.id' "/tmp/${sharename:?}-share.json"
        )

    openstack \
        --os-cloud "${cloudname:?}" \
            share show \
                --format json \
                "${shareid:?}"

    >   {
    >     "access_rules_status": "active",
    >     "availability_zone": "nova",
    >     "create_share_from_snapshot_support": false,
    >     "created_at": "2020-10-01T14:00:28.000000",
    >     "description": null,
    >     "export_locations": "\npath = 10.206.1.5:6789,10.206.1.6:6789,10.206.1.7:6789:/volumes/_nogroup/80df10fd-5f69-4e11-b04b-ec1e284acb38\nid = 135e45e6-ff2d-4c4b-b60b-333e5a7c8cd7\npreferred = False",
    >     "has_replicas": false,
    >     "id": "16ea3057-2a06-4753-9f45-91fee826b1ea",
    >     "is_public": true,
    >     "mount_snapshot_support": false,
    >     "name": "cotopi-dr2",
    >     "project_id": "21b4ae3a2ea44bc5a9c14005ed2963af",
    >     "properties": {},
    >     "replication_type": null,
    >     "revert_to_snapshot_support": false,
    >     "share_group_id": null,
    >     "share_network_id": null,
    >     "share_proto": "CEPHFS",
    >     "share_type": "5d0f58c5-ed21-4e1f-91bb-fe1a49deb5d8",
    >     "share_type_name": "cephfsnativetype",
    >     "size": 5000,
    >     "snapshot_id": null,
    >     "snapshot_support": false,
    >     "source_share_group_snapshot_member_id": null,
    >     "status": "available",
    >     "task_state": null,
    >     "user_id": "98169f87de174ad4ac98c32e59646488",
    >     "volume_type": "cephfsnativetype"
    >   }


# -----------------------------------------------------
# Create a RW access rule for our share.
#[user@kubernator]

    openstack \
        --os-cloud "${cloudname:?}" \
        share access create \
            --format json \
            --access-level 'rw' \
            "${shareid:?}" \
            'cephx' \
            "${sharename:?}-rw" \
    | tee "/tmp/${sharename:?}-rw-access.json"

    rwaccess=$(
        jq -r '.id' "/tmp/${sharename:?}-rw-access.json"
        )

    openstack \
        --os-cloud "${cloudname:?}" \
            share access show \
                --format json \
                "${rwaccess:?}"

    >   {
    >     "id": "c1009d73-8850-44db-8429-690298e146b0",
    >     "share_id": "16ea3057-2a06-4753-9f45-91fee826b1ea",
    >     "access_level": "rw",
    >     "access_to": "cotopi-dr2-rw",
    >     "access_type": "cephx",
    >     "state": "active",
    >     "access_key": "AQDC4XVfnuOKLhAA2NCak8KoGlksGbK47TbAaA==",
    >     "created_at": "2020-10-01T14:03:46.000000",
    >     "updated_at": "2020-10-01T14:03:46.000000",
    >     "properties": ""
    >   }


# -----------------------------------------------------
# Create our Chart values.
#[user@kubernator]

    source "${HOME}/aglais.env"

cat > "/tmp/${sharename:?}-values.yaml" << EOF

aglais:
  dataset: "test-data"

share:
  name:   "${sharename:?}"
  size:   "${sharesize:?}"
  access: "ReadWriteOnce"

openstack:
  shareid:   "${shareid:?}"
  accessid:  "${rwaccess:?}"

EOF


# -----------------------------------------------------
# Install our Chart.
#[user@kubernator]

    helm install \
        "${sharename:?}" \
        "/helm/manila-static-share" \
        --values "/tmp/${sharename:?}-values.yaml"

    >   NAME: cotopi-dr2
    >   LAST DEPLOYED: Thu Oct  1 14:05:14 2020
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 1
    >   TEST SUITE: None
    >   NOTES:
    >   Use the testpod to check access to the mounted volume.


# -----------------------------------------------------
# Check all the components are there.
#[user@kubernator]

    kubectl describe \
        PersistentVolume \
            "${sharename:?}-volume"

    >   Name:            cotopi-dr2-volume
    >   Labels:          aglais.dataset=test-data
    >                    aglais.name=cotopi-dr2
    >                    app.kubernetes.io/component=test-data
    >                    app.kubernetes.io/instance=cotopi-dr2
    >                    app.kubernetes.io/managed-by=Helm
    >                    app.kubernetes.io/name=manila-static-share
    >                    app.kubernetes.io/version=0.0.1
    >                    helm.sh/chart=manila-static-share-0.0.1
    >   Annotations:     meta.helm.sh/release-name: cotopi-dr2
    >                    meta.helm.sh/release-namespace: default
    >                    pv.kubernetes.io/bound-by-controller: yes
    >   Finalizers:      [kubernetes.io/pv-protection]
    >   StorageClass:
    >   Status:          Bound
    >   Claim:           default/cotopi-dr2-claim
    >   Reclaim Policy:  Retain
    >   Access Modes:    RWO
    >   VolumeMode:      Filesystem
    >   Capacity:        5T
    >   Node Affinity:   <none>
    >   Message:
    >   Source:
    >       Type:              CSI (a Container Storage Interface (CSI) volume source)
    >       Driver:            cephfs.manila.csi.openstack.org
    >       VolumeHandle:      cotopi-dr2-handle
    >       ReadOnly:          false
    >       VolumeAttributes:      shareAccessID=c1009d73-8850-44db-8429-690298e146b0
    >                              shareID=16ea3057-2a06-4753-9f45-91fee826b1ea
    >   Events:                <none>


    kubectl describe \
        PersistentVolumeClaim \
            "${sharename:?}-claim"

    >   Name:          cotopi-dr2-claim
    >   Namespace:     default
    >   StorageClass:
    >   Status:        Bound
    >   Volume:        cotopi-dr2-volume
    >   Labels:        aglais.dataset=test-data
    >                  aglais.name=cotopi-dr2
    >                  app.kubernetes.io/component=test-data
    >                  app.kubernetes.io/instance=cotopi-dr2
    >                  app.kubernetes.io/managed-by=Helm
    >                  app.kubernetes.io/name=manila-static-share
    >                  app.kubernetes.io/version=0.0.1
    >                  helm.sh/chart=manila-static-share-0.0.1
    >   Annotations:   meta.helm.sh/release-name: cotopi-dr2
    >                  meta.helm.sh/release-namespace: default
    >                  pv.kubernetes.io/bind-completed: yes
    >                  pv.kubernetes.io/bound-by-controller: yes
    >   Finalizers:    [kubernetes.io/pvc-protection]
    >   Capacity:      5T
    >   Access Modes:  RWO
    >   VolumeMode:    Filesystem
    >   Mounted By:    cotopi-dr2-testpod
    >   Events:        <none>


    kubectl describe \
        Pod \
            "${sharename:?}-testpod"

    >   Name:         cotopi-dr2-testpod
    >   Namespace:    default
    >   Node:         tiberius-20200923-nqzekodqww64-node-3/10.0.0.41
    >   Start Time:   Thu, 01 Oct 2020 14:05:15 +0000
    >   Labels:       aglais.dataset=test-data
    >                 aglais.name=cotopi-dr2
    >                 app.kubernetes.io/component=test-data
    >                 app.kubernetes.io/instance=cotopi-dr2
    >                 app.kubernetes.io/managed-by=Helm
    >                 app.kubernetes.io/name=manila-static-share
    >                 app.kubernetes.io/version=0.0.1
    >                 helm.sh/chart=manila-static-share-0.0.1
    >   Annotations:  meta.helm.sh/release-name: cotopi-dr2
    >                 meta.helm.sh/release-namespace: default
    >   Status:       Running
    >   IP:           10.100.2.18
    >   Containers:
    >     cotopi-dr2-container:
    >       Container ID:  docker://efb371f5eaaa3b3ae6ce773f56b4fd29d0182b2c072ec116dd04d932fb8b0d51
    >       Image:         fedora:32
    >       Image ID:      docker-pullable://docker.io/fedora@sha256:d6a6d60fda1b22b6d5fe3c3b2abe2554b60432b7b215adc11a2b5fae16f50188
    >       Port:          <none>
    >       Host Port:     <none>
    >       Command:
    >         /bin/sh
    >       Args:
    >         -c
    >         while true; do date >> /local-data/${HOSTNAME}.log; sleep 1; done
    >       State:          Running
    >         Started:      Thu, 01 Oct 2020 14:05:18 +0000
    >       Ready:          True
    >       Restart Count:  0
    >       Environment:    <none>
    >       Mounts:
    >         /local-data from local-data (rw)
    >         /share-data from share-data (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from default-token-pxrzv (ro)
    >   Conditions:
    >     Type              Status
    >     Initialized       True
    >     Ready             True
    >     ContainersReady   True
    >     PodScheduled      True
    >   Volumes:
    >     share-data:
    >       Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    >       ClaimName:  cotopi-dr2-claim
    >       ReadOnly:   false
    >     local-data:
    >       Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    >       Medium:
    >       SizeLimit:  <unset>
    >     default-token-pxrzv:
    >       Type:        Secret (a volume populated by a Secret)
    >       SecretName:  default-token-pxrzv
    >       Optional:    false
    >   QoS Class:       BestEffort
    >   Node-Selectors:  <none>
    >   Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
    >                    node.kubernetes.io/unreachable:NoExecute for 300s
    >   Events:
    >     Type    Reason     Age        From                                            Message
    >     ----    ------     ----       ----                                            -------
    >     Normal  Scheduled  <unknown>  default-scheduler                               Successfully assigned default/cotopi-dr2-testpod to tiberius-20200923-nqzekodqww64-node-3
    >     Normal  Pulled     107s       kubelet, tiberius-20200923-nqzekodqww64-node-3  Container image "fedora:32" already present on machine
    >     Normal  Created    107s       kubelet, tiberius-20200923-nqzekodqww64-node-3  Created container cotopi-dr2-container
    >     Normal  Started    107s       kubelet, tiberius-20200923-nqzekodqww64-node-3  Started container cotopi-dr2-container


# -----------------------------------------------------
# Login to our test pod.
#[user@kubernator]

    kubectl exec \
        --tty \
        --stdin \
        "${sharename:?}-testpod" \
            -- \
                bash


# -----------------------------------------------------
# -----------------------------------------------------
# Check we can write to the local data.
#[root@rutoria]

    ls -al /local-data/

    >   drwxrwxrwx. 2 root root   36 Oct  1 14:05 .
    >   drwxr-xr-x. 1 root root   53 Oct  1 14:05 ..
    >   -rw-r--r--. 1 root root 3799 Oct  1 14:07 cotopi-dr2-testpod.log


    tail /local-data/${HOSTNAME:?}.log

    >   ....
    >   ....
    >   Thu Oct  1 14:07:39 UTC 2020
    >   Thu Oct  1 14:07:40 UTC 2020
    >   Thu Oct  1 14:07:41 UTC 2020
    >   Thu Oct  1 14:07:42 UTC 2020


# -----------------------------------------------------
# Check we can write to the shared data.
#[root@rutoria]

    ls -al /share-data/

    >   drwxrwxrwx. 2 root root  0 Oct  1 14:00 .
    >   drwxr-xr-x. 1 root root 53 Oct  1 14:05 ..


    for i in {0..8}
    do
        date >> /share-data/${HOSTNAME:?}.log
    done

    tail    /share-data/${HOSTNAME:?}.log

    >   ....
    >   ....
    >   Thu Oct  1 14:08:09 UTC 2020
    >   Thu Oct  1 14:08:09 UTC 2020
    >   Thu Oct  1 14:08:09 UTC 2020
    >   Thu Oct  1 14:08:09 UTC 2020



# -----------------------------------------------------
# Delete our Chart deployment (release).
#[user@kubernator]

    helm list

    >   NAME             	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART                     	APP VERSION
    >   augusta-20200923 	default  	1       	2020-09-23 14:00:28.990000054 +0000 UTC	deployed	ingress-nginx-3.3.0       	0.35.0
    >   cotopi-dr2       	default  	1       	2020-10-01 14:05:14.42160186 +0000 UTC 	deployed	manila-static-share-0.0.1
    >   csi-manila-cephfs	default  	1       	2020-09-24 14:33:11.255369512 +0000 UTC	deployed	openstack-manila-csi-0.1.2	latest
    >   rutoria          	default  	3       	2020-10-01 05:16:50.622116368 +0000 UTC	deployed	manila-static-share-0.0.1
    >   valeria-20200923 	default  	1       	2020-09-23 14:16:31.194359487 +0000 UTC	deployed	kubernetes-dashboard-2.7.1	2.0.4


    helm delete \
        "${sharename:?}"

    >   release "cotopi-dr2" uninstalled


    kubectl get \
        Pods

    >   NAME                                                         READY   STATUS              RESTARTS   AGE
    >   ....
    >   cotopi-dr2-testpod                                           1/1     Terminating         0          41m
    >   ....


# -----------------------------------------------------
# Delete our Manila share.
#[user@kubernator]

    openstack \
        --os-cloud "${cloudname:?}" \
        share delete \
            "${shareid:?}"

    >   -



