#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    # Plan
    # Create Kubernetes cluster
    # Create CephFS router
    # Create CephFS storage

    #
    # Created the Kubernetes cluster OK
    # Got stuck with conflicts creating the router.
    # Got stuck with bits of old deployments I couldn't delete.
    #

# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --hostname terraformer \
        --volume "${HOME}/clouds.yaml:/etc/openstack/clouds.yaml:z" \
        --volume "${AGLAIS_CODE}/experiments/zrq/terraform:/terraform:z" \
        atolmis/terraform-client \
        bash


# -----------------------------------------------------
# Set the cloud, credentials and cluster names.
#[user@openstacker]

    cloudname=gaia-prod
    clustername=Tiberius
    keypairname=zrq-gaia-keypair

    # Export them as TF_VAR variables so that Terraform will pick them up.

    export TF_VAR_zrq_cloud_name=${cloudname:?}-super
    export TF_VAR_zrq_cluster_name=${clustername:?}
    export TF_VAR_zrq_keypair_name=${keypairname}


# -----------------------------------------------------
# Run Terraform to deploy the example cluster.
#[user@openstacker]

    pushd "/terraform"

        terraform init

    >   Initializing modules...
    >   
    >   Initializing the backend...
    >   
    >   Initializing provider plugins...
    >   
    >   Terraform has been successfully initialized!

        terraform plan

    >   ....
    >     # module.cluster.openstack_compute_keypair_v2.zrq_keypair will be created
    >     + resource "openstack_compute_keypair_v2" "zrq_keypair" {
    >       ....
    >       }
    >   
    >     # module.cluster.openstack_containerinfra_cluster_v1.cluster will be created
    >     + resource "openstack_containerinfra_cluster_v1" "cluster" {
    >       ....
    >       }
    >   ....

        terraform apply

    >   ....
    >   ....
    >   module.cluster.openstack_containerinfra_cluster_v1.cluster: Creating...
    >   module.cluster.openstack_containerinfra_cluster_v1.cluster: Still creating... [10s elapsed]
    >   module.cluster.openstack_containerinfra_cluster_v1.cluster: Still creating... [20s elapsed]
    >   module.cluster.openstack_containerinfra_cluster_v1.cluster: Still creating... [30s elapsed]
    >   module.cluster.openstack_containerinfra_cluster_v1.cluster: Still creating... [40s elapsed]
    >   module.cluster.openstack_containerinfra_cluster_v1.cluster: Still creating... [50s elapsed]
    >   module.cluster.openstack_containerinfra_cluster_v1.cluster: Still creating... [1m0s elapsed]
    >   
    >   Error: Error waiting for openstack_containerinfra_cluster_v1 6b6078c4-bbb5-4ac8-a68f-5ee0609c2daf to become ready: json: cannot unmarshal object into Go struct field Cluster.health_status_reason of type string


    popd


# -----------------------------------------------------
# List our clusters.
#[user@openstacker]

    openstack \
        --os-cloud "${cloudname:?}" \
        coe cluster list

    >   +--------------------------------------+----------+------------------+------------+--------------+--------------------+---------------+
    >   | uuid                                 | name     | keypair          | node_count | master_count | status             | health_status |
    >   +--------------------------------------+----------+------------------+------------+--------------+--------------------+---------------+
    >   | 6b6078c4-bbb5-4ac8-a68f-5ee0609c2daf | Tiberius | Tiberius-keypair |          1 |            1 | CREATE_IN_PROGRESS | None          |
    >   +--------------------------------------+----------+------------------+------------+--------------+--------------------+---------------+

    >   +--------------------------------------+----------+------------------+------------+--------------+-----------------+---------------+
    >   | uuid                                 | name     | keypair          | node_count | master_count | status          | health_status |
    >   +--------------------------------------+----------+------------------+------------+--------------+-----------------+---------------+
    >   | 6b6078c4-bbb5-4ac8-a68f-5ee0609c2daf | Tiberius | Tiberius-keypair |          1 |            1 | CREATE_COMPLETE | HEALTHY       |
    >   +--------------------------------------+----------+------------------+------------+--------------+-----------------+---------------+


# -----------------------------------------------------
# Get the details for our new cluster.
#[user@openstacker]

    clusterid=$(
        openstack \
        --os-cloud "${cloudname:?}" \
            coe cluster list \
                --format json \
        | jq -r '.[] | select(.name | test("^'${clustername:?}'")) | .uuid'
        )

    openstack \
        --os-cloud "${cloudname:?}" \
        coe cluster show \
            "${clusterid}" \
                --format json \
    | jq '.'


    >   {
    >     "status": "CREATE_COMPLETE",
    >     "health_status": "HEALTHY",
    >     "cluster_template_id": "d54167d9-495f-437e-88fe-d182b2a230ea",
    >     "node_addresses": [
    >       "10.0.0.198"
    >     ],
    >     "uuid": "6b6078c4-bbb5-4ac8-a68f-5ee0609c2daf",
    >     "stack_id": "d5882512-52f2-458e-b089-ca97fabd2910",
    >     "status_reason": null,
    >     "created_at": "2020-06-26T01:42:33+00:00",
    >     "updated_at": "2020-06-26T01:48:43+00:00",
    >     "coe_version": "v1.15.9",
    >     "labels": {
    >       "auto_healing_controller": "magnum-auto-healer",
    >       "max_node_count": "2",
    >       "cloud_provider_tag": "v1.15.0",
    >       "etcd_tag": "3.3.17",
    >       "monitoring_enabled": "true",
    >       "tiller_enabled": "true",
    >       "autoscaler_tag": "v1.15.2",
    >       "master_lb_floating_ip_enabled": "true",
    >       "min_node_count": "1",
    >       "tiller_tag": "v2.16.1",
    >       "use_podman": "true",
    >       "auto_healing_enabled": "true",
    >       "heat_container_agent_tag": "train-stable-1",
    >       "kube_tag": "v1.15.9",
    >       "auto_scaling_enabled": "true"
    >     },
    >     "labels_overridden": "",
    >     "labels_skipped": "",
    >     "labels_added": "",
    >     "faults": "",
    >     "keypair": "Tiberius-keypair",
    >     "api_address": "https://128.232.227.126:6443",
    >     "master_addresses": [
    >       "10.0.0.30"
    >     ],
    >     "create_timeout": null,
    >     "node_count": 1,
    >     "discovery_url": "https://discovery.etcd.io/c16d9762d5b53942306af973a8cc1fd8",
    >     "master_count": 1,
    >     "container_version": "1.12.6",
    >     "name": "Tiberius",
    >     "master_flavor_id": "general.v1.tiny",
    >     "flavor_id": "general.v1.tiny",
    >     "health_status_reason": {
    >       "api": "ok",
    >       "tiberius-ten3ivhddijo-master-0.Ready": "True",
    >       "tiberius-ten3ivhddijo-node-0.Ready": "True"
    >     },
    >     "project_id": "21b4ae3a2ea44bc5a9c14005ed2963af"
    >   }




