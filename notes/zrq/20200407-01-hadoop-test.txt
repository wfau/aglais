#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    #
    # Lots of details about Hadoop and HDFS.
    # https://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html
    # https://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-common/CommandsManual.html
    # https://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-common/FileSystemShell.html


# -----------------------------------------------------
# Create a container to work with.
# https://podman.readthedocs.io/en/latest/markdown/podman-run.1.html
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --hostname ansibler \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --env "clouduser=${AGLAIS_USER:?}" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --volume "${HOME:?}/clouds.yaml:/tmp/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/zrq/ansible:/mnt/ansible:ro,z" \
        atolmis/ansible-client:latest \
        bash


# -----------------------------------------------------
# Create our Ansible include vars file.
#[root@ansibler]

    cat > /tmp/ansible-vars.yml << EOF

buildtag:  'aglais-$(date '+%Y%m%d')'
cloudname: '${cloudname}'
clouduser: '${clouduser}'
EOF


# -----------------------------------------------------
# Link our clouds.cfg file.
#[root@ansibler]

    if [ ! -e '/etc/openstack' ]
    then
        mkdir '/etc/openstack'
    fi
    pushd '/etc/openstack'
        ln -sf '/tmp/clouds.yaml' 'clouds.yaml'
    popd


# -----------------------------------------------------
# Run the scripts from the ansible directory.
#[root@ansibler]

    pushd '/mnt/ansible'

# -----------------------------------------------------
# Run the initial part of our deplyment.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-01.yml"

    >   ....
    >   ....


# -----------------------------------------------------
# Run the Hadoop HDFS part of our deplyment.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-02.yml"


    >   ....
    >   ....


    #
    # Accept ssh keys.
    #


# -----------------------------------------------------
# Link the Ansible generated ssh config.
#[root@ansibler]

    pushd "${HOME}/.ssh"
        ln -sf 'ansible-config' 'config'
    popd


# -----------------------------------------------------
# Delete the existing known_hosts file.
#[root@ansibler]

    rm -f ~/.ssh/known_hosts 


# -----------------------------------------------------
# -----------------------------------------------------
# Use podman exec to open a new terminal in the ansible-client.
#[user@desktop]

    podman ps

    >   CONTAINER ID  IMAGE                                      COMMAND  CREATED         STATUS             PORTS  NAMES
    >   79850d8b677a  localhost/atolmis/ansible-client:latest    bash     11 minutes ago  Up 11 minutes ago         condescending_ishizaka
    >   a158f968e276  localhost/atolmis/openstack-client:latest  bash     3 days ago      Up 3 days ago             naughty_wescoff

    podman exec -it 79850d8b677a bash

    >   [root@ansibler]


# -----------------------------------------------------
# Login to a master node ...
#[root@ansibler]

    ssh master01

    >   The authenticity of host 'master01 (<no hostip for proxy command>)' can't be established.
    >   ECDSA key fingerprint is SHA256:8crjTZBk2SZTbiKmc3s0Xv01ZTOR2A+qfOuGyQ9WJWs.
    >   Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
    >   Warning: Permanently added 'master01' (ECDSA) to the list of known hosts.
    >   Last login: Tue Apr  7 02:59:45 2020 from 10.10.0.12


# -----------------------------------------------------
# Format the HDFS filesystem
#[fedora@master01]

    hdfs namenode -format

    >   2020-04-07 03:01:37,965 INFO namenode.NameNode: STARTUP_MSG: 
    >   /..
    >   STARTUP_MSG: Starting NameNode
    >   STARTUP_MSG:   host = aglais-20200407-master01.novalocal/10.10.0.5
    >   STARTUP_MSG:   args = [-format]
    >   STARTUP_MSG:   version = 3.2.1
    >   STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:....
    >   STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z
    >   STARTUP_MSG:   java = 13.0.2
    >   ************************************************************/
    >   2020-04-07 03:01:37,974 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
    >   2020-04-07 03:01:38,072 INFO namenode.NameNode: createNameNode [-format]
    >   2020-04-07 03:01:38,550 INFO common.Util: Assuming 'file' scheme for path /var/local/hadoop/namenode/fsimage in configuration.
    >   2020-04-07 03:01:38,551 INFO common.Util: Assuming 'file' scheme for path /var/local/hadoop/namenode/fsimage in configuration.
    >   Formatting using clusterid: CID-a5d3362d-2a87-4ec2-92c2-fd42148fb84d
    >   2020-04-07 03:01:38,600 INFO namenode.FSEditLog: Edit logging is async:true
    >   2020-04-07 03:01:38,613 INFO namenode.FSNamesystem: KeyProvider: null
    >   2020-04-07 03:01:38,614 INFO namenode.FSNamesystem: fsLock is fair: true
    >   2020-04-07 03:01:38,615 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
    >   2020-04-07 03:01:38,645 INFO namenode.FSNamesystem: fsOwner             = fedora (auth:SIMPLE)
    >   2020-04-07 03:01:38,645 INFO namenode.FSNamesystem: supergroup          = supergroup
    >   2020-04-07 03:01:38,645 INFO namenode.FSNamesystem: isPermissionEnabled = true
    >   2020-04-07 03:01:38,645 INFO namenode.FSNamesystem: HA Enabled: false
    >   2020-04-07 03:01:38,691 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
    >   2020-04-07 03:01:38,701 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
    >   2020-04-07 03:01:38,701 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
    >   2020-04-07 03:01:38,705 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
    >   2020-04-07 03:01:38,706 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Apr 07 03:01:38
    >   2020-04-07 03:01:38,707 INFO util.GSet: Computing capacity for map BlocksMap
    >   2020-04-07 03:01:38,707 INFO util.GSet: VM type       = 64-bit
    >   2020-04-07 03:01:38,709 INFO util.GSet: 2.0% max memory 1.5 GB = 29.8 MB
    >   2020-04-07 03:01:38,709 INFO util.GSet: capacity      = 2^22 = 4194304 entries
    >   2020-04-07 03:01:38,732 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
    >   2020-04-07 03:01:38,732 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
    >   2020-04-07 03:01:38,738 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
    >   2020-04-07 03:01:38,738 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
    >   2020-04-07 03:01:38,738 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
    >   2020-04-07 03:01:38,738 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
    >   2020-04-07 03:01:38,739 INFO blockmanagement.BlockManager: defaultReplication         = 2
    >   2020-04-07 03:01:38,739 INFO blockmanagement.BlockManager: maxReplication             = 512
    >   2020-04-07 03:01:38,739 INFO blockmanagement.BlockManager: minReplication             = 1
    >   2020-04-07 03:01:38,739 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
    >   2020-04-07 03:01:38,739 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
    >   2020-04-07 03:01:38,739 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
    >   2020-04-07 03:01:38,739 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
    >   2020-04-07 03:01:38,758 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
    >   2020-04-07 03:01:38,758 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
    >   2020-04-07 03:01:38,759 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
    >   2020-04-07 03:01:38,759 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
    >   2020-04-07 03:01:38,768 INFO util.GSet: Computing capacity for map INodeMap
    >   2020-04-07 03:01:38,768 INFO util.GSet: VM type       = 64-bit
    >   2020-04-07 03:01:38,768 INFO util.GSet: 1.0% max memory 1.5 GB = 14.9 MB
    >   2020-04-07 03:01:38,768 INFO util.GSet: capacity      = 2^21 = 2097152 entries
    >   2020-04-07 03:01:38,774 INFO namenode.FSDirectory: ACLs enabled? false
    >   2020-04-07 03:01:38,774 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
    >   2020-04-07 03:01:38,774 INFO namenode.FSDirectory: XAttrs enabled? true
    >   2020-04-07 03:01:38,775 INFO namenode.NameNode: Caching file names occurring more than 10 times
    >   2020-04-07 03:01:38,778 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
    >   2020-04-07 03:01:38,780 INFO snapshot.SnapshotManager: SkipList is disabled
    >   2020-04-07 03:01:38,783 INFO util.GSet: Computing capacity for map cachedBlocks
    >   2020-04-07 03:01:38,783 INFO util.GSet: VM type       = 64-bit
    >   2020-04-07 03:01:38,784 INFO util.GSet: 0.25% max memory 1.5 GB = 3.7 MB
    >   2020-04-07 03:01:38,784 INFO util.GSet: capacity      = 2^19 = 524288 entries
    >   2020-04-07 03:01:38,791 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
    >   2020-04-07 03:01:38,791 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
    >   2020-04-07 03:01:38,791 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
    >   2020-04-07 03:01:38,794 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
    >   2020-04-07 03:01:38,794 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
    >   2020-04-07 03:01:38,796 INFO util.GSet: Computing capacity for map NameNodeRetryCache
    >   2020-04-07 03:01:38,796 INFO util.GSet: VM type       = 64-bit
    >   2020-04-07 03:01:38,796 INFO util.GSet: 0.029999999329447746% max memory 1.5 GB = 457.7 KB
    >   2020-04-07 03:01:38,796 INFO util.GSet: capacity      = 2^16 = 65536 entries
    >   2020-04-07 03:01:38,820 INFO namenode.FSImage: Allocated new BlockPoolId: BP-717462176-10.10.0.5-1586228498812
    >   2020-04-07 03:01:38,837 INFO common.Storage: Storage directory /var/local/hadoop/namenode/fsimage has been successfully formatted.
    >   2020-04-07 03:01:38,874 INFO namenode.FSImageFormatProtobuf: Saving image file /var/local/hadoop/namenode/fsimage/current/fsimage.ckpt_0000000000000000000 using no compression
    >   2020-04-07 03:01:38,975 INFO namenode.FSImageFormatProtobuf: Image file /var/local/hadoop/namenode/fsimage/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved in 0 seconds .
    >   2020-04-07 03:01:38,985 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
    >   2020-04-07 03:01:38,998 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
    >   2020-04-07 03:01:38,999 INFO namenode.NameNode: SHUTDOWN_MSG: 
    >   /..
    >   SHUTDOWN_MSG: Shutting down NameNode at aglais-20200407-master01.novalocal/10.10.0.5
    >   ************************************************************/


# -----------------------------------------------------
# Start the HDFS daemon.
#[fedora@master01]

    hdfs --daemon start namenode


# -----------------------------------------------------
# Tail the log file.
#[fedora@master01]

    pushd '/var/local/hadoop/logs/'

        tail -f hadoop-fedora-namenode-aglais-20200407-master01.novalocal.log

    >   ....
    >   2020-04-07 03:02:17,793 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
    >   2020-04-07 03:02:17,794 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
    >   2020-04-07 03:02:17,803 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master01/10.10.0.5:9000
    >   2020-04-07 03:02:17,809 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
    >   2020-04-07 03:02:17,809 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with 4 thread(s)
    >   2020-04-07 03:02:17,814 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in 5 milliseconds
    >   name space=1
    >   storage space=0
    >   storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
    >   2020-04-07 03:02:17,818 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
    >   2020-04-07 03:03:23,507 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.10.0.22:9866, datanodeUuid=4f7c4144-ea84-49fb-b603-f6737cbe9eed, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-a5d3362d-2a87-4ec2-92c2-fd42148fb84d;nsid=1701219293;c=1586228498812) storage 4f7c4144-ea84-49fb-b603-f6737cbe9eed
    >   2020-04-07 03:03:23,509 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.10.0.22:9866
    >   2020-04-07 03:03:23,509 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 4f7c4144-ea84-49fb-b603-f6737cbe9eed (10.10.0.22:9866).
    >   2020-04-07 03:03:23,558 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-8bdb6d81-13a7-4f04-a44c-ab75bdeecfc8 for DN 10.10.0.22:9866
    >   2020-04-07 03:03:23,600 INFO BlockStateChange: BLOCK* processReport 0xd6d9df6b05354a7: Processing first storage report for DS-8bdb6d81-13a7-4f04-a44c-ab75bdeecfc8 from datanode 4f7c4144-ea84-49fb-b603-f6737cbe9eed
    >   2020-04-07 03:03:23,601 INFO BlockStateChange: BLOCK* processReport 0xd6d9df6b05354a7: from storage DS-8bdb6d81-13a7-4f04-a44c-ab75bdeecfc8 node DatanodeRegistration(10.10.0.22:9866, datanodeUuid=4f7c4144-ea84-49fb-b603-f6737cbe9eed, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-a5d3362d-2a87-4ec2-92c2-fd42148fb84d;nsid=1701219293;c=1586228498812), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
    >   ....


# -----------------------------------------------------
# -----------------------------------------------------
# Use podman exec to open a new terminal in the ansible-client.
#[user@desktop]

    podman ps 


    podman exec -it 912ab6437b3c bash

    >   [root@ansibler]


# -----------------------------------------------------
# Login to a worker node ...
#[root@ansibler]

    ssh worker01
    ssh worker02

    >   The authenticity of host 'worker01 (<no hostip for proxy command>)' can't be established.
    >   ECDSA key fingerprint is SHA256:tYKL/wsJ4SUirAO9bOJf9cuoNaVBg/ieRFNnxfaONwA.
    >   Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
    >   Warning: Permanently added 'worker01' (ECDSA) to the list of known hosts.
    >   Last login: Tue Apr  7 02:59:50 2020 from 10.10.0.12


# -----------------------------------------------------
# Start the HDFS daemon.
#[fedora@worker01]
#[fedora@worker02]

    hdfs --daemon start datanode


# -----------------------------------------------------
# Tail the log file.
#[fedora@worker01]
#[fedora@worker02]

    pushd '/var/local/hadoop/logs/'

        tail -f hadoop-fedora-datanode-aglais-20200407-worker01.novalocal.log


    >   ....
    >   2020-04-07 03:03:23,430 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-717462176-10.10.0.5-1586228498812: 4ms
    >   2020-04-07 03:03:23,432 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-717462176-10.10.0.5-1586228498812 on volume /data-01/hdfs/data
    >   2020-04-07 03:03:23,433 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/data-01/hdfs/data, DS-8bdb6d81-13a7-4f04-a44c-ab75bdeecfc8): finished scanning block pool BP-717462176-10.10.0.5-1586228498812
    >   2020-04-07 03:03:23,466 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 4/7/20, 3:10 AM with interval of 21600000ms
    >   2020-04-07 03:03:23,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-717462176-10.10.0.5-1586228498812 (Datanode Uuid 4f7c4144-ea84-49fb-b603-f6737cbe9eed) service to master01/10.10.0.5:9000 beginning handshake with NN
    >   2020-04-07 03:03:23,481 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/data-01/hdfs/data, DS-8bdb6d81-13a7-4f04-a44c-ab75bdeecfc8): no suitable block pools found to scan.  Waiting 1814399951 ms.
    >   2020-04-07 03:03:23,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-717462176-10.10.0.5-1586228498812 (Datanode Uuid 4f7c4144-ea84-49fb-b603-f6737cbe9eed) service to master01/10.10.0.5:9000 successfully registered with NN
    >   2020-04-07 03:03:23,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode master01/10.10.0.5:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
    >   2020-04-07 03:03:23,622 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xd6d9df6b05354a7,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 2 msec to generate and 41 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
    >   2020-04-07 03:03:23,622 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-717462176-10.10.0.5-1586228498812
    >   ....

    >   ....
    >   2020-04-07 03:04:34,526 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-717462176-10.10.0.5-1586228498812: 5ms
    >   2020-04-07 03:04:34,529 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-717462176-10.10.0.5-1586228498812 on volume /data-01/hdfs/data
    >   2020-04-07 03:04:34,530 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/data-01/hdfs/data, DS-2594caaf-f3bb-484f-9baa-f8d5c3b2fd32): finished scanning block pool BP-717462176-10.10.0.5-1586228498812
    >   2020-04-07 03:04:34,552 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 4/7/20, 8:40 AM with interval of 21600000ms
    >   2020-04-07 03:04:34,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-717462176-10.10.0.5-1586228498812 (Datanode Uuid 7af65d06-af82-45ed-9ee2-7878e4149484) service to master01/10.10.0.5:9000 beginning handshake with NN
    >   2020-04-07 03:04:34,565 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/data-01/hdfs/data, DS-2594caaf-f3bb-484f-9baa-f8d5c3b2fd32): no suitable block pools found to scan.  Waiting 1814399961 ms.
    >   2020-04-07 03:04:34,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-717462176-10.10.0.5-1586228498812 (Datanode Uuid 7af65d06-af82-45ed-9ee2-7878e4149484) service to master01/10.10.0.5:9000 successfully registered with NN
    >   2020-04-07 03:04:34,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode master01/10.10.0.5:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
    >   2020-04-07 03:04:34,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xce0b39d81d520d5d,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 20 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
    >   2020-04-07 03:04:34,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-717462176-10.10.0.5-1586228498812
    >   ....



# -----------------------------------------------------
# -----------------------------------------------------
# Use podman exec to open a new terminal in the ansible-client.
#[user@desktop]

    podman ps | grep ansible-client

    >   79850d8b677a  localhost/atolmis/ansible-client:latest    bash     17 minutes ago  Up 17 minutes ago         condescending_ishizaka

    podman exec -it 79850d8b677a bash

    >   [root@ansibler]


# -----------------------------------------------------
# Login to the other master node ...
#[root@ansibler]

    ssh master02

# -----------------------------------------------------
# Check the HDFS status.
# (*) Needs the master->master:9000 rule in place.
#[fedora@master02]

    hdfs dfsadmin -report

    >   2020-04-07 03:10:18,572 INFO ipc.Client: Retrying connect to server: master01/10.10.0.5:9000. Already tried 0 time(s); maxRetries=45
    >   2020-04-07 03:10:38,595 INFO ipc.Client: Retrying connect to server: master01/10.10.0.5:9000. Already tried 1 time(s); maxRetries=45
    >   2020-04-07 03:10:58,606 INFO ipc.Client: Retrying connect to server: master01/10.10.0.5:9000. Already tried 2 time(s); maxRetries=45
    >   2020-04-07 03:11:18,628 INFO ipc.Client: Retrying connect to server: master01/10.10.0.5:9000. Already tried 3 time(s); maxRetries=45
    >   Configured Capacity: 2199023255552 (2 TB)
    >   Present Capacity: 2194692120576 (2.00 TB)
    >   DFS Remaining: 2194692112384 (2.00 TB)
    >   DFS Used: 8192 (8 KB)
    >   DFS Used%: 0.00%
    >   Replicated Blocks:
    >   	Under replicated blocks: 0
    >   	Blocks with corrupt replicas: 0
    >   	Missing blocks: 0
    >   	Missing blocks (with replication factor 1): 0
    >   	Low redundancy blocks with highest priority to recover: 0
    >   	Pending deletion blocks: 0
    >   Erasure Coded Block Groups: 
    >   	Low redundancy block groups: 0
    >   	Block groups with corrupt internal blocks: 0
    >   	Missing block groups: 0
    >   	Low redundancy blocks with highest priority to recover: 0
    >   	Pending deletion blocks: 0
    >   
    >   -------------------------------------------------
    >   Live datanodes (2):
    >   
    >   Name: 10.10.0.22:9866 (worker01)
    >   Hostname: worker01
    >   Decommission Status : Normal
    >   Configured Capacity: 1099511627776 (1 TB)
    >   DFS Used: 4096 (4 KB)
    >   Non DFS Used: 17297408 (16.50 MB)
    >   DFS Remaining: 1097346056192 (1021.98 GB)
    >   DFS Used%: 0.00%
    >   DFS Remaining%: 99.80%
    >   Configured Cache Capacity: 0 (0 B)
    >   Cache Used: 0 (0 B)
    >   Cache Remaining: 0 (0 B)
    >   Cache Used%: 100.00%
    >   Cache Remaining%: 0.00%
    >   Xceivers: 1
    >   Last contact: Tue Apr 07 03:11:20 UTC 2020
    >   Last Block Report: Tue Apr 07 03:03:23 UTC 2020
    >   Num of Blocks: 0
    >   
    >   
    >   Name: 10.10.0.7:9866 (worker02)
    >   Hostname: worker02
    >   Decommission Status : Normal
    >   Configured Capacity: 1099511627776 (1 TB)
    >   DFS Used: 4096 (4 KB)
    >   Non DFS Used: 17297408 (16.50 MB)
    >   DFS Remaining: 1097346056192 (1021.98 GB)
    >   DFS Used%: 0.00%
    >   DFS Remaining%: 99.80%
    >   Configured Cache Capacity: 0 (0 B)
    >   Cache Used: 0 (0 B)
    >   Cache Remaining: 0 (0 B)
    >   Cache Used%: 100.00%
    >   Cache Remaining%: 0.00%
    >   Xceivers: 1
    >   Last contact: Tue Apr 07 03:11:19 UTC 2020
    >   Last Block Report: Tue Apr 07 03:04:34 UTC 2020
    >   Num of Blocks: 0
    
    
# -----------------------------------------------------
# Download some test data.
#[fedora@master02]

    wget http://cdn.gea.esac.esa.int/Gaia/gdr2/gaia_source/csv/

    >   --2020-04-07 03:29:53--  http://cdn.gea.esac.esa.int/Gaia/gdr2/gaia_source/csv/
    >   Resolving cdn.gea.esac.esa.int (cdn.gea.esac.esa.int)... 185.59.221.24
    >   Connecting to cdn.gea.esac.esa.int (cdn.gea.esac.esa.int)|185.59.221.24|:80... connected.
    >   HTTP request sent, awaiting response... 200 OK
    >   Length: unspecified [text/html]
    >   Saving to: ‘index.html’
    >   
    >   index.html  [ <=> ]   9.63M  63.9MB/s    in 0.2s    
    >   
    >   2020-04-07 03:29:53 (63.9 MB/s) - ‘index.html’ saved [10093850]
    
    vi index.html

        <a href="GaiaSource_1000172165251650944_1000424567594791808.csv.gz">GaiaSource_1000172165251650944_1000424567594791..&gt;</a> 16-Apr-2018 07:32             5347523
        <a href="GaiaSource_1000424601954531200_1000677322125743488.csv.gz">GaiaSource_1000424601954531200_1000677322125743..&gt;</a> 16-Apr-2018 07:32             5024698
        <a href="GaiaSource_1000677386549270528_1000959999693425920.csv.gz">GaiaSource_1000677386549270528_1000959999693425..&gt;</a> 16-Apr-2018 07:32             5976430
        <a href="GaiaSource_1000960034052654336_1001215258190537216.csv.gz">GaiaSource_1000960034052654336_1001215258190537..&gt;</a> 16-Apr-2018 07:32             6102333
        <a href="GaiaSource_1001215288252921728_1001455428465395840.csv.gz">GaiaSource_1001215288252921728_1001455428465395..&gt;</a> 16-Apr-2018 07:32             6143061
        <a href="GaiaSource_1001455467121397632_1001731032222989696.csv.gz">GaiaSource_1001455467121397632_1001731032222989..&gt;</a> 16-Apr-2018 07:32             6517254

    cat > files.txt << EOF
GaiaSource_1000172165251650944_1000424567594791808.csv.gz
GaiaSource_1000424601954531200_1000677322125743488.csv.gz
GaiaSource_1000677386549270528_1000959999693425920.csv.gz
GaiaSource_1000960034052654336_1001215258190537216.csv.gz
GaiaSource_1001215288252921728_1001455428465395840.csv.gz
GaiaSource_1001455467121397632_1001731032222989696.csv.gz
EOF


    mkdir downloads
    pushd downloads

        for filename in $(cat ~/files.txt)
        do
            echo "File [${filename}]"
            wget "http://cdn.gea.esac.esa.int/Gaia/gdr2/gaia_source/csv/${filename}"
        done
    
    popd


    ls -alh downloads/

    >   total 34M
    >   drwxrwxr-x. 2 fedora fedora 4.0K Apr  7 03:34 .
    >   drwx------. 5 fedora fedora 4.0K Apr  7 03:34 ..
    >   -rw-rw-r--. 1 fedora fedora 5.1M Apr 16  2018 GaiaSource_1000172165251650944_1000424567594791808.csv.gz
    >   -rw-rw-r--. 1 fedora fedora 4.8M Apr 16  2018 GaiaSource_1000424601954531200_1000677322125743488.csv.gz
    >   -rw-rw-r--. 1 fedora fedora 5.7M Apr 16  2018 GaiaSource_1000677386549270528_1000959999693425920.csv.gz
    >   -rw-rw-r--. 1 fedora fedora 5.9M Apr 16  2018 GaiaSource_1000960034052654336_1001215258190537216.csv.gz
    >   -rw-rw-r--. 1 fedora fedora 5.9M Apr 16  2018 GaiaSource_1001215288252921728_1001455428465395840.csv.gz
    >   -rw-rw-r--. 1 fedora fedora 6.3M Apr 16  2018 GaiaSource_1001455467121397632_1001731032222989696.csv.gz


# -----------------------------------------------------
# Transfer the files into HDFS.
#[fedora@master02]


    hdfs dfs -mkdir /Gaia
    hdfs dfs -mkdir /Gaia/gdr2
    hdfs dfs -mkdir /Gaia/gdr2/gaia_source
    hdfs dfs -mkdir /Gaia/gdr2/gaia_source/csv
    
    for filename in $(cat ~/files.txt)
    do
        echo "File [${filename}]"
        hdfs dfs -put "downloads/${filename}" /Gaia/gdr2/gaia_source/csv/
    done


    >   ....
    >   2020-04-07 03:45:27,394 INFO hdfs.DataStreamer: Exception in createBlockOutputStream blk_1073741825_1001
    >   org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/10.10.0.22:9866]
    >   	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:536)
    >   	at org.apache.hadoop.hdfs.DataStreamer.createSocketForPipeline(DataStreamer.java:253)
    >   	at org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1725)
    >   	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1679)
    >   	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:716)
    >   2020-04-07 03:45:27,400 WARN hdfs.DataStreamer: Abandoning BP-717462176-10.10.0.5-1586228498812:blk_1073741825_1001
    >   2020-04-07 03:45:27,410 WARN hdfs.DataStreamer: Excluding datanode DatanodeInfoWithStorage[10.10.0.22:9866,DS-8bdb6d81-13a7-4f04-a44c-ab75bdeecfc8,DISK]
    >   ....

    #
    # Added master->worker:9866 and worker->worker:9866 security rules.
    #


    >   File [GaiaSource_1000172165251650944_1000424567594791808.csv.gz]
    >   2020-04-07 03:48:08,106 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
    >   File [GaiaSource_1000424601954531200_1000677322125743488.csv.gz]
    >   2020-04-07 03:48:09,936 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
    >   File [GaiaSource_1000677386549270528_1000959999693425920.csv.gz]
    >   2020-04-07 03:48:11,829 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
    >   File [GaiaSource_1000960034052654336_1001215258190537216.csv.gz]
    >   2020-04-07 03:48:13,697 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
    >   File [GaiaSource_1001215288252921728_1001455428465395840.csv.gz]
    >   2020-04-07 03:48:15,669 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
    >   File [GaiaSource_1001455467121397632_1001731032222989696.csv.gz]
    >   2020-04-07 03:48:17,590 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false


# -----------------------------------------------------
# List the files in HDFS.
#[fedora@master02]

    hdfs dfs -ls /Gaia/gdr2/gaia_source/csv/

    >   Found 6 items
    >   -rw-r--r--   3 fedora supergroup    5347523 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1000172165251650944_1000424567594791808.csv.gz
    >   -rw-r--r--   3 fedora supergroup    5024698 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1000424601954531200_1000677322125743488.csv.gz
    >   -rw-r--r--   3 fedora supergroup    5976430 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1000677386549270528_1000959999693425920.csv.gz
    >   -rw-r--r--   3 fedora supergroup    6102333 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1000960034052654336_1001215258190537216.csv.gz
    >   -rw-r--r--   3 fedora supergroup    6143061 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1001215288252921728_1001455428465395840.csv.gz
    >   -rw-r--r--   3 fedora supergroup    6517254 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1001455467121397632_1001731032222989696.csv.gz


# -----------------------------------------------------
# List the files in HDFS from the workers.
#[fedora@master02]

    ssh worker01 \
        '
        hdfs dfs -ls /Gaia/gdr2/gaia_source/csv/
        '

    >   Found 6 items
    >   -rw-r--r--   3 fedora supergroup    5347523 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1000172165251650944_1000424567594791808.csv.gz
    >   -rw-r--r--   3 fedora supergroup    5024698 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1000424601954531200_1000677322125743488.csv.gz
    >   -rw-r--r--   3 fedora supergroup    5976430 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1000677386549270528_1000959999693425920.csv.gz
    >   -rw-r--r--   3 fedora supergroup    6102333 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1000960034052654336_1001215258190537216.csv.gz
    >   -rw-r--r--   3 fedora supergroup    6143061 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1001215288252921728_1001455428465395840.csv.gz
    >   -rw-r--r--   3 fedora supergroup    6517254 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1001455467121397632_1001731032222989696.csv.gz


    ssh worker02 \
        '
        hdfs dfs -ls /Gaia/gdr2/gaia_source/csv/
        '

    >   Found 6 items
    >   -rw-r--r--   3 fedora supergroup    5347523 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1000172165251650944_1000424567594791808.csv.gz
    >   -rw-r--r--   3 fedora supergroup    5024698 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1000424601954531200_1000677322125743488.csv.gz
    >   -rw-r--r--   3 fedora supergroup    5976430 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1000677386549270528_1000959999693425920.csv.gz
    >   -rw-r--r--   3 fedora supergroup    6102333 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1000960034052654336_1001215258190537216.csv.gz
    >   -rw-r--r--   3 fedora supergroup    6143061 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1001215288252921728_1001455428465395840.csv.gz
    >   -rw-r--r--   3 fedora supergroup    6517254 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1001455467121397632_1001731032222989696.csv.gz


    ssh master01 \
        '
        hdfs dfs -ls /Gaia/gdr2/gaia_source/csv/
        '

    >   Found 6 items
    >   -rw-r--r--   3 fedora supergroup    5347523 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1000172165251650944_1000424567594791808.csv.gz
    >   -rw-r--r--   3 fedora supergroup    5024698 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1000424601954531200_1000677322125743488.csv.gz
    >   -rw-r--r--   3 fedora supergroup    5976430 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1000677386549270528_1000959999693425920.csv.gz
    >   -rw-r--r--   3 fedora supergroup    6102333 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1000960034052654336_1001215258190537216.csv.gz
    >   -rw-r--r--   3 fedora supergroup    6143061 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1001215288252921728_1001455428465395840.csv.gz
    >   -rw-r--r--   3 fedora supergroup    6517254 2020-04-07 03:48 /Gaia/gdr2/gaia_source/csv/GaiaSource_1001455467121397632_1001731032222989696.csv.gz


# -----------------------------------------------------
# Fetch a file from HDFS and checksum it on our workers.
#[fedora@master02]

    filename=GaiaSource_1000172165251650944_1000424567594791808.csv.gz

    ssh worker01 \
    ssh worker02 \
        "
        sudo dnf -y install \
            cksfv \
            coreutils
        "



    ssh worker01 \
    ssh worker02 \
        "
        mkdir /tmp/gaia
        hdfs dfs -get -crc /Gaia/gdr2/gaia_source/csv/${filename:?} /tmp/gaia
        md5sum /tmp/gaia/${filename:?}
        "

    >   614baf41facb6c07d0ec91e5fe8a9517  /tmp/gaia/GaiaSource_1000172165251650944_1000424567594791808.csv.gz

    >   614baf41facb6c07d0ec91e5fe8a9517  /tmp/gaia/GaiaSource_1000172165251650944_1000424567594791808.csv.gz





    #
    # Create users home directory ...
    # http://www.hadooplessons.info/2017/12/creating-home-directory-for-user-in-hdfs-hdpca.html
    # Requires user to have a Unix account on all the nodes.
    #
    
    #
    # HDFS Fuse mount
    # http://bigdata-tips.blogspot.com/2017/01/how-to-mount-hdfs-in-linux-fuse.html
    # http://www.idata.co.il/2018/10/how-to-connect-hdfs-to-local-filesystem/
    #
    
    #
    # HDFS HighAvailability
    # https://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html
    #

    
