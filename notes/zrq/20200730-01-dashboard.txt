#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    # In previous experiments, we ended up with two instances of dashboard.
    # We know we created one of them ... did the other come as part of the cluster ?

# -----------------------------------------------------

    # Deleted old cluster.
    # notes/zrq/20200718-03-openstack-delete.txt

    # Created new cluster.
    # notes/zrq/20200718-04-terraform-create.txt

# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --hostname kubernator \
        --publish 127.0.0.1:8443:8443 \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --env "clustername=${MAGNUM_CLUSTER:?}" \
        --volume "${HOME}/clouds.yaml:/etc/openstack/clouds.yaml:z" \
        --volume "${AGLAIS_CODE}/experiments/zrq/kubernetes:/kubernetes:z" \
        --volume "${ZEPPELIN_CODE}:/zeppelin:z" \
        atolmis/openstack-client \
        bash


# -----------------------------------------------------
# Get the connection details for our cluster.
#[user@kubernator]

    mkdir -p "${HOME}/.kube"
    openstack \
        --os-cloud "${cloudname:?}-super" \
        coe cluster config \
            "${clustername:?}" \
                --force \
                --dir "${HOME}/.kube"


# -----------------------------------------------------
# List all the pods in all the namespaces.
#[user@kubernator]

    kubectl get pod -A

    >   NAMESPACE       NAME                                                      READY   STATUS              RESTARTS   AGE
    >   kube-system     alertmanager-prometheus-alertmanager-0                    2/2     Running             0          16s
    >   kube-system     cluster-autoscaler-6cd7c5f567-28lbp                       1/1     Running             4          3m30s
    >   kube-system     coredns-ffc7449c-59x6n                                    1/1     Running             0          3m33s
    >   kube-system     coredns-ffc7449c-8fc5z                                    1/1     Running             0          3m33s
    >   kube-system     heapster-868bbf8578-lh2hh                                 1/1     Running             0          3m33s
    >   kube-system     k8s-keystone-auth-4h9kc                                   1/1     Running             0          3m32s
    >   kube-system     kube-dns-autoscaler-6d5d44bf86-2cqcm                      1/1     Running             0          3m33s
    >   kube-system     kube-flannel-ds-amd64-lhcbz                               1/1     Running             0          102s
    >   kube-system     kube-flannel-ds-amd64-pgtvh                               1/1     Running             0          110s
    >   kube-system     kube-flannel-ds-amd64-pw5dn                               1/1     Running             0          117s
    >   kube-system     kube-flannel-ds-amd64-qdc4j                               1/1     Running             0          106s
    >   kube-system     kube-flannel-ds-amd64-qwgxp                               1/1     Running             0          3m33s
    >   kube-system     kubernetes-dashboard-6bcf74b4cd-zkv9d                     1/1     Running             0          3m32s
    >   kube-system     magnum-auto-healer-lfjn8                                  1/1     Running             0          3m32s
    >   kube-system     metrics-server-77f4b87964-zc46h                           0/1     Running             0          26s
    >   kube-system     npd-6f4hs                                                 1/1     Running             0          86s
    >   kube-system     npd-gvhrf                                                 1/1     Running             0          90s
    >   kube-system     npd-rgvw7                                                 1/1     Running             0          82s
    >   kube-system     npd-zf7dl                                                 1/1     Running             0          97s
    >   kube-system     openstack-cloud-controller-manager-hbkfq                  1/1     Running             0          3m33s
    >   kube-system     prometheus-adapter-6dcc4c897-stdsp                        1/1     Running             0          69s
    >   kube-system     prometheus-admission-patch-snqjl                          0/1     Completed           0          13s
    >   kube-system     prometheus-operator-686b6964b6-pvnh6                      2/2     Running             0          29s
    >   kube-system     prometheus-operator-grafana-5f9894dd95-rzm8q              2/2     Running             0          29s
    >   kube-system     prometheus-operator-kube-state-metrics-6f7dff9ddc-r9t8z   1/1     Running             0          29s
    >   kube-system     prometheus-operator-prometheus-node-exporter-g5ttp        1/1     Running             0          29s
    >   kube-system     prometheus-operator-prometheus-node-exporter-lfhd6        1/1     Running             0          29s
    >   kube-system     prometheus-operator-prometheus-node-exporter-mhjc8        1/1     Running             0          29s
    >   kube-system     prometheus-operator-prometheus-node-exporter-srss8        1/1     Running             0          29s
    >   kube-system     prometheus-operator-prometheus-node-exporter-vkvzj        1/1     Running             0          29s
    >   kube-system     prometheus-prometheus-prometheus-0                        0/3     ContainerCreating   0          6s
    >   magnum-tiller   install-metrics-server-job-85hls                          0/1     Error               0          54s
    >   magnum-tiller   install-metrics-server-job-ftgs8                          0/1     Error               0          3m31s
    >   magnum-tiller   install-metrics-server-job-lfjwp                          0/1     Completed           0          34s
    >   magnum-tiller   install-metrics-server-job-v4sfv                          0/1     Error               0          64s
    >   magnum-tiller   install-prometheus-adapter-job-b6v9f                      0/1     Completed           0          3m31s
    >   magnum-tiller   install-prometheus-operator-job-2s5vz                     1/1     Running             0          3m30s
    >   magnum-tiller   tiller-deploy-657c556f44-kxct4                            1/1     Running             0          3m33s

    #
    # It looks like there is a dashboard already created as part of the cluster.
    #

    >   ....
    >   kube-system     kubernetes-dashboard-6bcf74b4cd-zkv9d                     1/1     Running             0          3m32s
    >   ....


# -----------------------------------------------------
# Get details of the kubernetes-dashboard pod.
#[user@kubernator]

    kubectl \
        --namespace kube-system \
        describe pod \
            kubernetes-dashboard-6bcf74b4cd-zkv9d

    >   Name:           kubernetes-dashboard-6bcf74b4cd-zkv9d
    >   Namespace:      kube-system
    >   Node:           tiberius-x6rxvofoub5w-node-0/10.0.0.185
    >   Start Time:     Fri, 31 Jul 2020 00:35:40 +0000
    >   Labels:         k8s-app=kubernetes-dashboard
    >                   pod-template-hash=6bcf74b4cd
    >   Annotations:    <none>
    >   Status:         Running
    >   IP:             10.100.2.4
    >   Controlled By:  ReplicaSet/kubernetes-dashboard-6bcf74b4cd
    >   Containers:
    >     kubernetes-dashboard:
    >       Container ID:  docker://1c1dc6ec5bc91b7bd4d1f1777993c33d2708dbc0c58d2732c88f8e613f7c584f
    >       Image:         gcr.io/google_containers/kubernetes-dashboard-amd64:v1.8.3
    >       Image ID:      docker-pullable://gcr.io/google_containers/kubernetes-dashboard-amd64@sha256:dc4026c1b595435ef5527ca598e1e9c4343076926d7d62b365c44831395adbd0
    >       Port:          8443/TCP
    >       Host Port:     0/TCP
    >       Args:
    >         --auto-generate-certificates
    >         --heapster-host=heapster:80
    >       State:          Running
    >         Started:      Fri, 31 Jul 2020 00:35:52 +0000
    >       Ready:          True
    >       Restart Count:  0
    >       Liveness:       http-get https://:8443/ delay=30s timeout=30s period=10s #success=1 #failure=3
    >       Environment:
    >         POD_NAME:       kubernetes-dashboard-6bcf74b4cd-zkv9d (v1:metadata.name)
    >         POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    >         POD_IP:          (v1:status.podIP)
    >       Mounts:
    >         /certs from kubernetes-dashboard-certs (rw)
    >         /tmp from tmp-volume (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from kubernetes-dashboard-token-x2mkq (ro)
    >   Conditions:
    >     Type              Status
    >     Initialized       True
    >     Ready             True
    >     ContainersReady   True
    >     PodScheduled      True
    >   Volumes:
    >     kubernetes-dashboard-certs:
    >       Type:        Secret (a volume populated by a Secret)
    >       SecretName:  kubernetes-dashboard-certs
    >       Optional:    false
    >     tmp-volume:
    >       Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    >       Medium:
    >       SizeLimit:  <unset>
    >     kubernetes-dashboard-token-x2mkq:
    >       Type:        Secret (a volume populated by a Secret)
    >       SecretName:  kubernetes-dashboard-token-x2mkq
    >       Optional:    false
    >   QoS Class:       BestEffort
    >   Node-Selectors:  <none>
    >   Tolerations:     node-role.kubernetes.io/master:NoSchedule
    >                    node.kubernetes.io/not-ready:NoExecute for 300s
    >                    node.kubernetes.io/unreachable:NoExecute for 300s
    >   Events:
    >     Type     Reason            Age                    From                                   Message
    >     ----     ------            ----                   ----                                   -------
    >     Warning  FailedScheduling  5m32s (x5 over 6m15s)  default-scheduler                      0/1 nodes are available: 1 node(s) had taints that the pod didn't tolerate.
    >     Warning  FailedScheduling  4m35s (x4 over 4m40s)  default-scheduler                      0/2 nodes are available: 2 node(s) had taints that the pod didn't tolerate.
    >     Warning  FailedScheduling  4m30s                  default-scheduler                      0/3 nodes are available: 3 node(s) had taints that the pod didn't tolerate.
    >     Warning  FailedScheduling  4m21s (x2 over 4m21s)  default-scheduler                      0/5 nodes are available: 5 node(s) had taints that the pod didn't tolerate.
    >     Normal   Scheduled         4m10s                  default-scheduler                      Successfully assigned kube-system/kubernetes-dashboard-6bcf74b4cd-zkv9d to tiberius-x6rxvofoub5w-node-0
    >     Normal   Pulling           4m7s                   kubelet, tiberius-x6rxvofoub5w-node-0  Pulling image "gcr.io/google_containers/kubernetes-dashboard-amd64:v1.8.3"
    >     Normal   Pulled            3m58s                  kubelet, tiberius-x6rxvofoub5w-node-0  Successfully pulled image "gcr.io/google_containers/kubernetes-dashboard-amd64:v1.8.3"
    >     Normal   Created           3m58s                  kubelet, tiberius-x6rxvofoub5w-node-0  Created container kubernetes-dashboard
    >     Normal   Started           3m58s                  kubelet, tiberius-x6rxvofoub5w-node-0  Started container kubernetes-dashboard


# -----------------------------------------------------
# Can we setup a port-forward to that pod?
#[user@kubernator]

    kubectl \
        --namespace kube-system \
        port-forward \
            kubernetes-dashboard-6bcf74b4cd-zkv9d \
            8443:8443


# -----------------------------------------------------
# -----------------------------------------------------
# Point firefox at the local port.
#[user@desktop]

    firefox "https://localhost:8443/#/login" &

    Yay - it works :-)

    If we skip the login, then we see the same
    access control limits as before.

        pods is forbidden: User "system:serviceaccount:kube-system:kubernetes-dashboard"
        cannot list resource "pods" in API group "" in the namespace "default"


# -----------------------------------------------------
# -----------------------------------------------------
# Use JQ to decode the dashboard secret token.
# https://github.com/stedolan/jq/issues/47#issuecomment-374179653
#[root@dashboard]

    dashtoken=$(
        kubectl \
            --output json \
            --namespace kube-system \
                get secret \
        | jq -r '.items[] | select(.metadata.annotations."kubernetes.io/service-account.name" == "kubernetes-dashboard") | .data.token | @base64d'
        )

    echo ${dashtoken:?}



# -----------------------------------------------------
# -----------------------------------------------------
# Point firefox at the local port, and login using the token.
#[user@desktop]

    firefox "https://localhost:8443/#/login" &

    # Login using the token.
    # Still get access control limits.

        pods is forbidden: User "system:serviceaccount:kube-system:kubernetes-dashboard"
        cannot list resource "pods" in API group "" in the namespace "default"





